name: databricks/databricks
resources:
    access_control_rule_set Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "access_control_rule_set Resource - terraform-provider-databricks"'
        name: access_control_rule_set Resource - terraform-provider-databricks
        title: access_control_rule_set Resource - terraform-provider-databricks
        argumentDocs:
            grant_rules: "- (Required) The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them."
            grant_rules.principals: "- (Required) a list of principals who are granted a role. The following format is supported:"
            grant_rules.role: "- (Required) Role to be granted. The supported roles are listed below. For more information about these roles, refer to service principal roles, group roles or marketplace roles."
            groups/{groupname}: (also exposed as acl_principal_id attribute of databricks_group resource).
            id: "- ID of the access control rule set - the same as name."
            name: "- (Required) Unique identifier of a rule set. The name determines the resource to which the rule set applies. Currently, only default rule sets are supported. The following rule set formats are supported:"
            roles/group.manager: "- Manager of a group."
            roles/marketplace.admin: "- Admin of marketplace."
            roles/servicePrincipal.manager: "- Manager of a service principal."
            roles/servicePrincipal.user: "- User of a service principal."
            servicePrincipals/{applicationId}: (also exposed as acl_principal_id attribute of databricks_service_principal resource).
            users/{username}: (also exposed as acl_principal_id attribute of databricks_user resource).
        importStatements: []
    alert Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "alert Resource - terraform-provider-databricks"'
        name: alert Resource - terraform-provider-databricks
        title: alert Resource - terraform-provider-databricks
        argumentDocs:
            bool_value: "- boolean value (true or false) to compare against boolean results."
            column: "- (Required, Block) Block describing the column from the query result to use for comparison in alert evaluation:"
            condition: "- (Required) Trigger conditions of the alert. Block consists of the following attributes:"
            create_time: "- The timestamp string indicating when the alert was created."
            custom_body: "- (Optional, String) Custom body of alert notification, if it exists. See Alerts API reference for custom templating instructions."
            custom_subject: "- (Optional, String) Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See Alerts API reference for custom templating instructions."
            databricks_permissions: .
            databricks_sql_alert: ", for example, by executing the terraform state show databricks_sql_alert.alert command."
            display_name: "- (Required, String) Name of the alert."
            double_value: "- double value to compare against integer and double results."
            empty_result_state: "- (Optional, String Enum) Alert state if the result is empty (UNKNOWN, OK, TRIGGERED)"
            id: "- unique ID of the Alert."
            lifecycle_state: "- The workspace state of the alert. Used for tracking trashed status. (Possible values are ACTIVE or TRASHED)."
            name: "- (Required, String) Name of the column."
            notify_on_ok: "- (Optional, Boolean) Whether to notify alert subscribers when alert returns back to normal."
            op: "- (Required, String Enum) Operator used for comparison in alert evaluation. (Enum: GREATER_THAN, GREATER_THAN_OR_EQUAL, LESS_THAN, LESS_THAN_OR_EQUAL, EQUAL, NOT_EQUAL, IS_NULL)"
            operand: "- (Required, Block) Name of the column from the query result to use for comparison in alert evaluation:"
            options: "block is converted into the condition block with the following changes:"
            owner_user_name: "- (Optional, String) Alert owner's username."
            parent: (if exists) is renamed to parent_path attribute and should be converted from folders/object_id to the actual path.
            parent_path: "- (Optional, String) The path to a workspace folder containing the alert. The default is the user's home folder.  If changed, the alert will be recreated."
            query_id: "- (Required, String) ID of the query evaluated by the alert."
            rearm: attribute is renamed to seconds_to_retrigger.
            seconds_to_retrigger: "- (Optional, Integer) Number of seconds an alert must wait after being triggered to rearm itself. After rearming, it can be triggered again. If 0 or not specified, the alert will not be triggered again."
            state: "- Current state of the alert's trigger status (UNKNOWN, OK, TRIGGERED). This field is set to UNKNOWN if the alert has not yet been evaluated or ran into an error during the last evaluation."
            string_value: "- string value to compare against string results."
            terraform import databricks_alert.alert <alert-id>: command.
            terraform import.databricks_permissions: .
            terraform import.import: "and removed blocks like this:"
            terraform import.terraform apply: command to apply changes.
            terraform import.terraform plan: command to check possible changes, such as value type change, etc.
            terraform plan: command to check possible changes, such as value type change, etc.
            terraform state rm databricks_sql_alert.alert: command.
            threshold: "- (Optional for IS_NULL operation, Block) Threshold value used for comparison in alert evaluation:"
            trigger_time: "- The timestamp string when the alert was last triggered if the alert has been triggered before."
            update_time: "- The timestamp string indicating when the alert was updated."
            value: "- (Required, Block) actual value used in comparison (one of the attributes is required):"
        importStatements: []
    artifact_allowlist Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "artifact_allowlist Resource - terraform-provider-databricks"'
        name: artifact_allowlist Resource - terraform-provider-databricks
        title: artifact_allowlist Resource - terraform-provider-databricks
        argumentDocs:
            artifact_matcher.artifact: "- The artifact path or maven coordinate."
            artifact_matcher.match_type: "- The pattern matching type of the artifact. Only PREFIX_MATCH is supported."
            artifact_type: "- The artifact type of the allowlist. Can be INIT_SCRIPT, LIBRARY_JAR or LIBRARY_MAVEN. Change forces creation of a new resource."
            created_at: "-  Time at which this artifact allowlist was set."
            created_by: "-  Identity that set the artifact allowlist."
            id: "- ID of the artifact allow list in form of metastore_id|artifact_type."
            metastore_id: "- ID of the parent metastore."
        importStatements: []
    automatic_cluster_update_setting Resource - terraform-provider-databricks:
        subCategory: Settings
        description: '""page_title: "automatic_cluster_update_setting Resource - terraform-provider-databricks"'
        name: automatic_cluster_update_setting Resource - terraform-provider-databricks
        title: automatic_cluster_update_setting Resource - terraform-provider-databricks
        argumentDocs:
            automatic_cluster_update_workspace: (Required) block with following attributes
            day_of_week: "- the day of the week in uppercase, e.g. MONDAY or SUNDAY"
            enabled: "- (Required) The configuration details."
            frequency: "- one of the FIRST_OF_MONTH, SECOND_OF_MONTH, THIRD_OF_MONTH, FOURTH_OF_MONTH, FIRST_AND_THIRD_OF_MONTH, SECOND_AND_FOURTH_OF_MONTH, EVERY_WEEK."
            hours: "- hour to perform update: 0-23"
            maintenance_window: block that defines the maintenance frequency with the following arguments
            minutes: "- minute to perform update: 0-59"
            restart_even_if_no_updates_available: "- (Optional) To force clusters and other compute resources to restart during the maintenance window regardless of the availability of a new update."
            week_day_based_schedule: block with the following arguments
            window_start_time: block that defines the time of your maintenance window. The default timezone is UTC and cannot be changed.
        importStatements: []
    budget Resource - terraform-provider-databricks:
        subCategory: FinOps
        description: '""page_title: "budget Resource - terraform-provider-databricks"'
        name: budget Resource - terraform-provider-databricks
        title: budget Resource - terraform-provider-databricks
        argumentDocs:
            account_id: "- The ID of the Databricks Account."
            action_configurations: "- (Required) List of action configurations to take when the budget alert is triggered. Consists of the following fields:"
            action_type: "- (Required, String Enum) The type of action to take when the budget alert is triggered. (Enum: EMAIL_NOTIFICATION)"
            budget_configuration_id: "- The ID of the budget configuration."
            display_name: "- (Required) Name of the budget in Databricks Account."
            key: "- (Required, String) The key of the tag."
            operator: "- (Required, String Enum) The operator to use for the filter. (Enum: IN)"
            quantity_threshold: "- (Required, String) The threshold for the budget alert to determine if it is in a triggered state. The number is evaluated based on quantity_type."
            quantity_type: "- (Required, String Enum) The way to calculate cost for this budget alert. This is what quantity_threshold is measured in. (Enum: LIST_PRICE_DOLLARS_USD)"
            tags: "- (Optional) List of tags to filter by. Consists of the following fields:"
            target: "- (Required, String) The target of the action. For EMAIL_NOTIFICATION, this is the email address to send the notification to."
            time_period: "- (Required, String Enum) The time window of usage data for the budget. (Enum: MONTH)"
            trigger_type: "- (Required, String Enum) The evaluation method to determine when this budget alert is in a triggered state. (Enum: CUMULATIVE_SPENDING_EXCEEDED)"
            value: "- (Required) Consists of the following fields:"
            values: "- (Required, List of numbers) The values to filter by."
            workspace_id: "- (Optional) Filter by workspace ID (if empty, include usage all usage for this account). Consists of the following fields:"
        importStatements: []
    catalog Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "catalog Resource - terraform-provider-databricks"'
        name: catalog Resource - terraform-provider-databricks
        title: catalog Resource - terraform-provider-databricks
        argumentDocs:
            comment: "- (Optional) User-supplied free-form text."
            connection_name: "- (Optional) For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource."
            enable_predictive_optimization: "- (Optional) Whether predictive optimization should be enabled for this object and objects under it. Can be ENABLE, DISABLE or INHERIT"
            force_destroy: "- (Optional) Delete catalog regardless of its contents."
            id: "- ID of this catalog - same as the name."
            isolation_mode: "- (Optional) Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be ISOLATED or OPEN. Setting the catalog to ISOLATED will automatically allow access from the current workspace."
            metastore_id: "- ID of the parent metastore."
            name: "- Name of Catalog relative to parent metastore."
            options: "- (Optional) For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server."
            owner: "- (Optional) Username/groupname/sp application_id of the catalog owner."
            properties: "- (Optional) Extensible Catalog properties."
            provider_name: "- (Optional) For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource."
            share_name: "- (Optional) For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource."
            storage_root: "- (Optional if storage_root is specified for the metastore) Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource."
        importStatements: []
    catalog_workspace_binding Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "catalog_workspace_binding Resource - terraform-provider-databricks"'
        name: catalog_workspace_binding Resource - terraform-provider-databricks
        title: catalog_workspace_binding Resource - terraform-provider-databricks
        argumentDocs:
            binding_type: "- Binding mode. Default to BINDING_TYPE_READ_WRITE. Possible values are BINDING_TYPE_READ_ONLY, BINDING_TYPE_READ_WRITE"
            securable_name: "- Name of securable. Change forces creation of a new resource."
            securable_type: "- Type of securable. Default to catalog. Change forces creation of a new resource."
            workspace_id: "- ID of the workspace. Change forces creation of a new resource."
        importStatements: []
    cluster Resource - terraform-provider-databricks:
        subCategory: Compute
        description: '""page_title: "cluster Resource - terraform-provider-databricks"'
        name: cluster Resource - terraform-provider-databricks
        title: cluster Resource - terraform-provider-databricks
        argumentDocs:
            AUTO: ": Databricks picks an availability zone to schedule the cluster on."
            HA: "(default): High availability, spread nodes across availability zones for a Databricks deployment region."
            allow_cluster_create: argument set would still be able to create clusters, but within the boundary of the policy.
            apply_policy_default_values: "- (Optional) Whether to use policy default values for missing cluster attributes."
            autoscale.max_workers: "- (Optional) The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers."
            autoscale.min_workers: "- (Optional) The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation."
            autoscale.spark.databricks.cluster.profile: must have value singleNode
            autoscale.spark.master: must have prefix local, like local[*]
            autotermination_minutes: "- (Optional) Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to 60.  We highly recommend having this setting present for Interactive/BI clusters."
            aws_attributes.availability: "- (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT, SPOT_WITH_FALLBACK and ON_DEMAND. Note: If first_on_demand is zero, this availability type will be used for the entire cluster. Backend default value is SPOT_WITH_FALLBACK and could change in the future"
            aws_attributes.ebs_volume_count: "- (Optional) The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden."
            aws_attributes.ebs_volume_size: "- (Optional) The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized)."
            aws_attributes.ebs_volume_type: "- (Optional) The type of EBS volumes that will be launched with this cluster. Valid values are GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD. Use this option only if you're not picking Delta Optimized  node types."
            aws_attributes.first_on_demand: "- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. Backend default value is 1 and could change in the future"
            aws_attributes.instance_profile_arn: "- (Optional) Nodes for this cluster will only be placed on AWS instances with this instance profile. Please see databricks_instance_profile resource documentation for extended examples on adding a valid instance profile using Terraform."
            aws_attributes.spot_bid_price_percent: "- (Optional) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000."
            aws_attributes.zone_id: '- (Required) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-west-2a. The provided availability zone must be in the same region as the Databricks deployment. For example, us-west-2a is not a valid zone ID if the Databricks deployment resides in the us-east-1 region. Enable automatic availability zone selection ("Auto-AZ"), by setting the value auto. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.'
            azure_attributes.availability: "- (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT_AZURE, SPOT_WITH_FALLBACK_AZURE, and ON_DEMAND_AZURE. Note: If first_on_demand is zero, this availability type will be used for the entire cluster."
            azure_attributes.first_on_demand: "- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster."
            azure_attributes.spot_bid_max_price: "- (Optional) The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance."
            canned_acl: "- (Optional) Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs."
            cluster_mount_info.local_mount_dir_path: "- (Required) path inside the Spark container."
            cluster_mount_info.network_filesystem_info: "- block specifying connection. It consists of:"
            cluster_mount_info.remote_mount_dir_path: "- (Optional) string specifying path to mount on the remote service."
            cluster_name: "- (Optional) Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string."
            custom_tags: "- (Optional) Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to default_tags. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an x_ when it is propagated."
            data_security_mode: "- (Optional) Select the security features of the cluster. Unity Catalog requires SINGLE_USER or USER_ISOLATION mode. LEGACY_PASSTHROUGH for passthrough cluster and LEGACY_TABLE_ACL for Table ACL cluster. If omitted, default security features are enabled. To disable security features use NONE or legacy mode NO_ISOLATION. In the Databricks UI, this has been recently been renamed Access Mode and USER_ISOLATION has been renamed Shared, but use these terms here."
            dbfs:/mnt/name: .
            default_tags: "- (map) Tags that are added by Databricks by default, regardless of any custom_tags that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: , and any workspace and pool tags."
            destination: "- S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys."
            docker_image.basic_auth: "- (Optional) basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password."
            docker_image.url: "- URL for the Docker image"
            driver_instance_pool_id: (Optional) - similar to instance_pool_id, but for driver node. If omitted, and instance_pool_id is specified, then the driver will be allocated from that pool.
            driver_node_type_id: "- (Optional) The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as node_type_id defined above."
            enable_elastic_disk: "- (Optional) If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have autotermination_minutes and autoscale attributes set. More documentation available at cluster configuration page."
            enable_encryption: "- (Optional) Enable server-side encryption, false by default."
            enable_local_disk_encryption: "- (Optional) Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access."
            encryption_type: "- (Optional) The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3."
            endpoint: "- (Optional) S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used."
            gcp_attributes.availability: ", and will be removed soon."
            gcp_attributes.boot_disk_size: (optional, int) Boot disk size in GB
            gcp_attributes.google_service_account: "- (Optional, string) Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources."
            gcp_attributes.local_ssd_count: (optional, int) Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
            gcp_attributes.use_preemptible_executors: "- (Optional, bool) if we should use preemptible executors (GCP documentation). Warning: this field is deprecated in favor of"
            gcp_attributes.zone_id: "(optional)  Identifier for the availability zone in which the cluster resides. This can be one of the following:"
            id: "- Canonical unique identifier for the cluster."
            idempotency_token: "- (Optional) An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters."
            instance_pool_id: (Optional - required if node_type_id is not given) - To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to TERMINATED, the instances it used are returned to the pool and reused by a different cluster.
            instance_profile_arn: (AWS only) can control which data a given cluster can access through cloud-native controls.
            is_pinned: "- (Optional) boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is limited to 100, so apply may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number)."
            kms_key: "- (Optional) KMS key used if encryption is enabled and encryption type is set to sse-kms."
            mount_options: "- (Optional) string that will be passed as options passed to the mount command."
            no_wait: "- (Optional) If true, the provider will not wait for the cluster to reach RUNNING state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed)."
            node_type_id: "- (Required - optional if instance_pool_id is given) Any supported databricks_node_type id. If instance_pool_id is specified, this field is not needed."
            num_workers: "- (Optional) Number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes."
            policy_id: "- (Optional) Identifier of Cluster Policy to validate cluster and preset certain defaults. The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters. For example, when you specify policy_id of external metastore policy, you still have to fill in relevant keys for spark_conf.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Terraform will try to apply the detected changes."
            region: "- (Optional) S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used."
            runtime_engine: "- (Optional) The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: PHOTON, STANDARD."
            server_address: "- (Required) host name."
            single_user_name: "- (Optional) The optional user name of the user to assign to an interactive cluster. This field is required when using data_security_mode set to SINGLE_USER or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters)."
            spark.databricks.cluster.profile: set to serverless
            spark.databricks.repl.allowedLanguages: "set to a list of supported languages, for example: python,sql, or python,sql,r.  Scala is not supported!"
            spark_conf: "- (Optional) Map with key-value pairs to fine-tune Spark clusters, where you can provide custom Spark configuration properties in a cluster configuration."
            spark_env_vars: "- (Optional) Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers."
            spark_version: "- (Required) Runtime version of the cluster. Any supported databricks_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control."
            ssh_public_keys: "- (Optional) SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys."
            state: "- (string) State of the cluster."
            workload_type.jobs: "- (Optional) boolean flag defining if it's possible to run Databricks Jobs on this cluster. Default: true."
            workload_type.notebooks: "- (Optional) boolean flag defining if it's possible to run notebooks on this cluster. Default: true."
        importStatements: []
    cluster_policy Resource - terraform-provider-databricks:
        subCategory: Compute
        description: '""page_title: "cluster_policy Resource - terraform-provider-databricks"'
        name: cluster_policy Resource - terraform-provider-databricks
        title: cluster_policy Resource - terraform-provider-databricks
        argumentDocs:
            Free form: policy and create fully-configurable clusters.
            definition: "- Policy definition: JSON document expressed in Databricks Policy Definition Language. Cannot be used with policy_family_id"
            description: "- (Optional) Additional human-readable description of the cluster policy."
            id: "- Canonical unique identifier for the cluster policy. This is equal to policy_id."
            max_clusters_per_user: "- (Optional, integer) Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero."
            name: "- the name of the built-in cluster policy."
            policy_family_definition_overrides: "- settings to override in the built-in cluster policy."
            policy_family_id: "- the ID of the cluster policy family used for built-in cluster policy."
            policy_id: "- Canonical unique identifier for the cluster policy."
            spark_version: parameter in databricks_cluster and other resources.
        importStatements: []
    ? compliance_security_profile_setting Resource - terraform-provider-databricks
    : subCategory: Settings
      description: '""page_title: "compliance_security_profile_setting Resource - terraform-provider-databricks"'
      name: compliance_security_profile_setting Resource - terraform-provider-databricks
      title: compliance_security_profile_setting Resource - terraform-provider-databricks
      argumentDocs:
          compliance_security_profile_workspace: "block with following attributes:"
          compliance_standards: "- (Required) Enable one or more compliance standards on the workspace, e.g. HIPAA, PCI_DSS, FEDRAMP_MODERATE"
          is_enabled: "- (Required) Enable the Compliance Security Profile on the workspace"
      importStatements: []
    connection Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "connection Resource - terraform-provider-databricks"'
        name: connection Resource - terraform-provider-databricks
        title: connection Resource - terraform-provider-databricks
        argumentDocs:
            comment: "- (Optional) Free-form text."
            connection_type: "- Connection type. BIGQUERY MYSQL POSTGRESQL SNOWFLAKE REDSHIFT SQLDW SQLSERVER, SALESFORCE or DATABRICKS are supported. Up-to-date list of connection type supported"
            id: "- ID of this connection in form of <metastore_id>|<name>."
            name: "- Name of the Connection."
            options: "- The key value of options required by the connection, e.g. host, port, user, password or GoogleServiceAccountKeyJson. Please consult the documentation for the required option."
            owner: "- (Optional) Name of the connection owner."
            properties: "-  (Optional) Free-form connection properties."
        importStatements: []
    custom_app_integration Resource - terraform-provider-databricks:
        subCategory: Apps
        description: '""page_title: "custom_app_integration Resource - terraform-provider-databricks"'
        name: custom_app_integration Resource - terraform-provider-databricks
        title: custom_app_integration Resource - terraform-provider-databricks
        argumentDocs:
            access_token_ttl_in_minutes: "- access token time to live (TTL) in minutes."
            client_id: "- OAuth client-id generated by Databricks"
            client_secret: "- OAuth client-secret generated by the Databricks if this is a confidential OAuth app."
            confidential: "- Indicates whether an OAuth client secret is required to authenticate this client. Default to false. Change requires a new resource."
            integration_id: "- Unique integration id for the custom OAuth app."
            name: "- (Required) Name of the custom OAuth app. Change requires a new resource."
            redirect_urls: "- List of OAuth redirect urls."
            refresh_token_ttl_in_minutes: "- refresh token TTL in minutes. The TTL of refresh token cannot be lower than TTL of access token."
            scopes: "- OAuth scopes granted to the application. Supported scopes: all-apis, sql, offline_access, openid, profile, email."
        importStatements: []
    dashboard Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "dashboard Resource - terraform-provider-databricks"'
        name: dashboard Resource - terraform-provider-databricks
        title: dashboard Resource - terraform-provider-databricks
        argumentDocs:
            display_name: "- (Required) The display name of the dashboard."
            embed_credentials: "- (Optional) Whether to embed credentials in the dashboard. Default is true."
            file_path: "- (Optional) The path to the dashboard JSON file. Conflicts with serialized_dashboard."
            id: "- The unique ID of the dashboard."
            parent_path: "- (Required) The workspace path of the folder containing the dashboard. Includes leading slash and no trailing slash.  If folder doesn't exist, it will be created."
            serialized_dashboard: "- (Optional) The contents of the dashboard in serialized string form. Conflicts with file_path."
            warehouse_id: "- (Required) The warehouse ID used to run the dashboard."
        importStatements: []
    dbfs_file Resource - terraform-provider-databricks:
        subCategory: Storage
        description: '""page_title: "dbfs_file Resource - terraform-provider-databricks"'
        name: dbfs_file Resource - terraform-provider-databricks
        title: dbfs_file Resource - terraform-provider-databricks
        argumentDocs:
            content_base64: "- Encoded file contents. Conflicts with source. Use of content_base64 is discouraged, as it's increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a data pipeline configuration file."
            dbfs:/mnt/name: .
            dbfs_path: "- Path, but with dbfs: prefix."
            file_size: "- The file size of the file that is being tracked by this resource in bytes."
            id: "- Same as path."
            path: "- (Required) The path of the file in which you wish to save."
            source: "- The full absolute path to the file. Conflicts with content_base64."
        importStatements: []
    default_namespace_setting Resource - terraform-provider-databricks:
        subCategory: Settings
        description: '""page_title: "default_namespace_setting Resource - terraform-provider-databricks"'
        name: default_namespace_setting Resource - terraform-provider-databricks
        title: default_namespace_setting Resource - terraform-provider-databricks
        argumentDocs:
            namespace: "- (Required) The configuration details."
            value: "- (Required) The value for the setting."
        importStatements: []
    directory Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "directory Resource - terraform-provider-databricks"'
        name: directory Resource - terraform-provider-databricks
        title: directory Resource - terraform-provider-databricks
        argumentDocs:
            delete_recursive: "- Whether or not to trigger a recursive delete of this directory and its resources when deleting this on Terraform. Defaults to false"
            id: "- Path of directory on workspace"
            object_id: "- Unique identifier for a DIRECTORY"
            path: '- (Required) The absolute path of the directory, beginning with "/", e.g. "/Demo".'
            spark_version: parameter in databricks_cluster and other resources.
            workspace_path: "- path on Workspace File System (WSFS) in form of /Workspace + path"
        importStatements: []
    ? enhanced_security_monitoring_setting Resource - terraform-provider-databricks
    : subCategory: Settings
      description: '""page_title: "enhanced_security_monitoring_setting Resource - terraform-provider-databricks"'
      name: enhanced_security_monitoring_setting Resource - terraform-provider-databricks
      title: enhanced_security_monitoring_setting Resource - terraform-provider-databricks
      argumentDocs:
          enhanced_security_monitoring_workspace: "block with following attributes:"
          is_enabled: "- (Required) Enable the Enhanced Security Monitoring on the workspace"
      importStatements: []
    entitlements Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "entitlements Resource - terraform-provider-databricks"'
        name: entitlements Resource - terraform-provider-databricks
        title: entitlements Resource - terraform-provider-databricks
        argumentDocs:
            allow_cluster_create: "-  (Optional) Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy."
            allow_instance_pool_create: "-  (Optional) Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument."
            databricks_sql_access: "- (Optional) This is a field to allow the principal to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint."
            group/group_id: "- group group_id."
            group_id: "- Canonical unique identifier for the group."
            service_principal_id: "- Canonical unique identifier for the service principal."
            spn/spn_id: "- service principal spn_id."
            user/user_id: "- user user_id."
            user_id: "-  Canonical unique identifier for the user."
            workspace_access: "- (Optional) This is a field to allow the principal to have access to Databricks Workspace."
        importStatements: []
    external_location Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "external_location Resource - terraform-provider-databricks"'
        name: external_location Resource - terraform-provider-databricks
        title: external_location Resource - terraform-provider-databricks
        argumentDocs:
            access_point: "- (Optional) The ARN of the s3 access point to use with the external location (AWS)."
            comment: "- (Optional) User-supplied free-form text."
            credential_name: "- Name of the databricks_storage_credential to use with this external location."
            databricks_external_location: are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.
            encryption_details: "- (Optional) The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS)."
            force_destroy: "- (Optional) Destroy external location regardless of its dependents."
            force_update: "- (Optional) Update external location regardless of its dependents."
            id: "- ID of this external location - same as name."
            isolation_mode: "- (Optional) Whether the external location is accessible from all workspaces or a specific set of workspaces. Can be ISOLATION_MODE_ISOLATED or ISOLATION_MODE_OPEN. Setting the external location to ISOLATION_MODE_ISOLATED will automatically allow access from the current workspace."
            name: "- Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource."
            owner: "- (Optional) Username/groupname/sp application_id of the external location owner."
            read_only: "- (Optional) Indicates whether the external location is read-only."
            skip_validation: "- (Optional) Suppress validation errors if any & force save the external location"
            url: "- Path URL in cloud storage, of the form: s3://[bucket-host]/[bucket-dir] (AWS), abfss://[user]@[host]/[path] (Azure), gs://[bucket-host]/[bucket-dir] (GCP)."
        importStatements: []
    file Resource - terraform-provider-databricks:
        subCategory: Storage
        description: '""page_title: "file Resource - terraform-provider-databricks"'
        name: file Resource - terraform-provider-databricks
        title: file Resource - terraform-provider-databricks
        argumentDocs:
            content_base64: "- Contents in base 64 format. Conflicts with source."
            file_size: "- The file size of the file that is being tracked by this resource in bytes."
            id: "- Same as path."
            modification_time: "- The last time stamp when the file was modified"
            path: "- The path of the file in which you wish to save. For example, /Volumes/main/default/volume1/file.txt."
            source: "- The full absolute path to the file. Conflicts with content_base64."
        importStatements: []
    git_credential Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "git_credential Resource - terraform-provider-databricks"'
        name: git_credential Resource - terraform-provider-databricks
        title: git_credential Resource - terraform-provider-databricks
        argumentDocs:
            force: "- (Optional) specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail."
            git_provider: "-  (Required) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult Git Credentials API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition, awsCodeCommit."
            git_username: "- (Required) user name at Git provider."
            id: "- identifier of specific Git credential"
            personal_access_token: "- (Required) The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it's sourced from the first environment variable of GITHUB_TOKEN, GITLAB_TOKEN, or AZDO_PERSONAL_ACCESS_TOKEN, that has a non-empty value."
        importStatements: []
    global_init_script Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "global_init_script Resource - terraform-provider-databricks"'
        name: global_init_script Resource - terraform-provider-databricks
        title: global_init_script Resource - terraform-provider-databricks
        argumentDocs:
            content_base64: "- The base64-encoded source code global init script. Conflicts with source. Use of content_base64 is discouraged, as it's increasing memory footprint of Terraform state and should only be used in exceptional circumstances"
            dbfs:/mnt/name: .
            enabled: "(bool, optional default: false) specifies if the script is enabled for execution, or not"
            id: "- ID assigned to a global init script by API"
            name: (string, required) - the name of the script.  It should be unique
            position: "(integer, optional default: null) - the position of a global init script, where 0 represents the first global init script to run, 1 is the second global init script to run, and so on. When omitted, the script gets the last position."
            source: "- Path to script's source code on local filesystem. Conflicts with content_base64"
        importStatements: []
    grant Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "grant Resource - terraform-provider-databricks"'
        name: grant Resource - terraform-provider-databricks
        title: grant Resource - terraform-provider-databricks
        argumentDocs:
            principal: "- User name, group name or service principal application ID."
            privileges: "- One or more privileges that are specific to a securable type."
        importStatements: []
    grants Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "grants Resource - terraform-provider-databricks"'
        name: grants Resource - terraform-provider-databricks
        title: grants Resource - terraform-provider-databricks
        argumentDocs:
            databricks_grants.principal: "- User name, group name or service principal application ID."
            databricks_grants.privileges: "- One or more privileges that are specific to a securable type."
        importStatements: []
    group Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "group Resource - terraform-provider-databricks"'
        name: group Resource - terraform-provider-databricks
        title: group Resource - terraform-provider-databricks
        argumentDocs:
            acl_principal_id: "- identifier for use in databricks_access_control_rule_set, e.g. groups/Some Group."
            allow_cluster_create: "-  (Optional) This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy."
            allow_instance_pool_create: "-  (Optional) This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument."
            databricks_sql_access: "- (Optional) This is a field to allow the group to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint."
            display_name: "-  (Required) This is the display name for the given group."
            external_id: "- (Optional) ID of the group in an external identity provider."
            force: "- (Optional) Ignore cannot create group: Group with name X already exists. errors and implicitly import the specific group into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation."
            id: "-  The id for the group object."
            workspace_access: "- (Optional) This is a field to allow the group to have access to Databricks Workspace."
        importStatements: []
    group_instance_profile Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "group_instance_profile Resource - terraform-provider-databricks"'
        name: group_instance_profile Resource - terraform-provider-databricks
        title: group_instance_profile Resource - terraform-provider-databricks
        argumentDocs:
            group_id: "- (Required) This is the id of the group resource."
            id: "- The id in the format <group_id>|<instance_profile_id>."
            instance_profile_id: "-  (Required) This is the id of the instance profile resource."
        importStatements: []
    group_member Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "group_member Resource - terraform-provider-databricks"'
        name: group_member Resource - terraform-provider-databricks
        title: group_member Resource - terraform-provider-databricks
        argumentDocs:
            group_id: "- (Required) This is the id of the group resource."
            id: "- The id for the databricks_group_member object which is in the format <group_id>|<member_id>."
            member_id: "- (Required) This is the id of the group, service principal, or user."
        importStatements: []
    group_role Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "group_role Resource - terraform-provider-databricks"'
        name: group_role Resource - terraform-provider-databricks
        title: group_role Resource - terraform-provider-databricks
        argumentDocs:
            group_id: "- (Required) This is the id of the group resource."
            id: "- The id for the databricks_group_role object which is in the format <group_id>|<role>."
            role: "- (Required) Either a role name or the ARN/ID of the instance profile resource."
        importStatements: []
    instance_pool Resource - terraform-provider-databricks:
        subCategory: Compute
        description: '""page_title: "instance_pool Resource - terraform-provider-databricks"'
        name: instance_pool Resource - terraform-provider-databricks
        title: instance_pool Resource - terraform-provider-databricks
        argumentDocs:
            1 - 1023: GiB
            1- 1023: GiB
            100 - 4096: GiB
            500 - 4096: GiB
            availability: "- (Optional) (String) Availability type used for all instances in the pool. Only ON_DEMAND and SPOT are supported."
            azure_attributes.availability: "- (Optional) Availability type used for all nodes. Valid values are SPOT_AZURE and ON_DEMAND_AZURE."
            azure_attributes.spot_bid_max_price: "- (Optional) The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance."
            custom_tags: "- (Optional) (Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS & Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the official documentation). Attempting to set the same tags in both cluster and instance pool will raise an error. Databricks allows at most 43 custom tags."
            disk_count: "- (Optional) (Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified."
            disk_size: "- (Optional) (Integer) The size of each disk (in GiB) to attach."
            enable_elastic_disk: "- (Optional) (Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space."
            gcp_attributes.gcp_availability: "- (Optional) Availability type used for all nodes. Valid values are PREEMPTIBLE_GCP, PREEMPTIBLE_WITH_FALLBACK_GCP and ON_DEMAND_GCP, default: ON_DEMAND_GCP."
            gcp_attributes.local_ssd_count: (Optional, Int) Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
            gcp_attributes.zone_id: "- (Optional) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-central1-a. The provided availability zone must be in the same region as the Databricks workspace."
            id: "- Canonical unique identifier for the instance pool."
            idle_instance_autotermination_minutes: "- (Required) (Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible."
            instance_pool_name: "- (Required) (String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters."
            max_capacity: "- (Optional) (Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a best practice, this should be set based on anticipated usage."
            min_idle_instances: "- (Optional) (Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters."
            node_type_id: "- (Required) (String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the List Node Types API call."
            preloaded_docker_image.basic_auth: "- (Optional) basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password."
            preloaded_docker_image.url: "- URL for the Docker image"
            preloaded_spark_versions: "- (Optional) (List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks_spark_version data source or via  Runtime Versions API call."
            spot_bid_price_percent: "- (Optional) (Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. For safety, this field cannot be greater than 10000."
            zone_id: '- (Optional) (String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like "us-west-2a". The provided availability zone must be in the same region as the Databricks deployment. For example, "us-west-2a" is not a valid zone ID if the Databricks deployment resides in the "us-east-1" region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the List Zones API.'
        importStatements: []
    instance_profile Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "instance_profile Resource - terraform-provider-databricks"'
        name: instance_profile Resource - terraform-provider-databricks
        title: instance_profile Resource - terraform-provider-databricks
        argumentDocs:
            iam_role_arn: "- (Optional) The AWS IAM role ARN of the role associated with the instance profile. It must have the form arn:aws:iam::<account-id>:role/<name>. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless."
            id: "- ARN for EC2 Instance Profile, that is registered with Databricks."
            instance_profile_arn: "- (Required) ARN attribute of aws_iam_instance_profile output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation."
            is_meta_instance_profile: "- (Optional) Whether the instance profile is a meta instance profile. Used only in IAM credential passthrough."
            skip_validation: '- (Optional) For advanced usage only. If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. "Your requested instance type is not supported in your requested availability zone"), you can pass this flag to skip the validation and forcibly add the instance profile.'
        importStatements: []
    ip_access_list Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "ip_access_list Resource - terraform-provider-databricks"'
        name: ip_access_list Resource - terraform-provider-databricks
        title: ip_access_list Resource - terraform-provider-databricks
        argumentDocs:
            enabled: "- (Optional) Boolean true or false indicating whether this list should be active.  Defaults to true"
            id: "- Canonical unique identifier for the IP Access List, same as list_id."
            ip_addresses: "- A string list of IP addresses and CIDR ranges."
            label: "-  This is the display name for the given IP ACL List."
            list_id: "- Canonical unique identifier for the IP Access List."
            list_type: '-  Can only be "ALLOW" or "BLOCK".'
        importStatements: []
    job Resource - terraform-provider-databricks:
        subCategory: Compute
        description: '""page_title: "job Resource - terraform-provider-databricks"'
        name: job Resource - terraform-provider-databricks
        title: job Resource - terraform-provider-databricks
        argumentDocs:
            "*_task": "- (Required) one of the specific task blocks described below:"
            alert: "- (Optional) block consisting of following fields:"
            alert_id: "- (Required) (String) identifier of the Databricks Alert (databricks_alert)."
            alert_on_last_attempt: "- (Optional) (Bool) do not send notifications to recipients specified in on_start for the retried runs and do not send notifications to recipients specified in on_failure until the last retry of the run."
            always_running: "- (Optional, Deprecated) (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with parameters specified in spark_jar_task or spark_submit_task or spark_python_task or notebook_task blocks."
            autotermination_minutes: ", is_pinned, workload_type aren't supported!"
            base_parameters: "- (Optional) (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using dbutils.widgets.get."
            branch: "- name of the Git branch to use. Conflicts with tag and commit."
            budget_policy_id: "- (Optional) The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job."
            catalog: "- (Optional) The name of the catalog to use inside Unity Catalog."
            client: "- (Required, string) client version used by the environment."
            commands: '- (Required) (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".'
            commit: "- hash of Git commit to use. Conflicts with branch and tag."
            concurrency: "- (Optional) Controls the number of active iteration task runs. Default is 20, maximum allowed is 100."
            continuous: "- (Optional) Configuration block to configure pause status. See continuous Configuration Block."
            continuous.pause_status: "- (Optional) Indicate whether this continuous job is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted in the block, the server will default to using UNPAUSED as a value for pause_status."
            control_run_state: "- (Optional) (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the pause_status by stopping the current active run. This flag cannot be set for non-continuous jobs."
            custom_subject: "- (Optional) string specifying a custom subject of email sent."
            dashboard: "- (Optional) block consisting of following fields:"
            dashboard_id: "- (Required) (String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard."
            default: "- (Required) Default value of the parameter."
            dependencies: "- (list of strings) List of pip dependencies, as supported by the version of pip in this environment. Each dependency is a pip requirement file line.  See API docs for more information."
            depends_on: "- (Optional) block specifying dependency(-ies) for a given task."
            description: "- (Optional) An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding."
            disable_auto_optimization: "- (Optional) A flag to disable auto optimization in serverless tasks."
            email_notifications: "- (Optional) (List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below."
            enabled: "- (Required) If true, enable queueing for the job."
            entry_point: "- (Optional) Python function as entry point for the task"
            environment_key: "- (Optional) identifier of an environment block that is used to specify libraries.  Required for some tasks (spark_python_task, python_wheel_task, ...) running on serverless compute."
            existing_cluster_id: "- (Optional) Identifier of the interactive cluster to run job on.  Note: running tasks on interactive clusters may lead to increased costs!"
            file: "- (Optional) block consisting of single string fields:"
            full_refresh: "- (Optional) (Bool) Specifies if there should be full refresh of the pipeline."
            git_source: "- (Optional) Specifices the a Git repository for task source code. See git_source Configuration Block below."
            health: "- (Optional) An optional block that specifies the health conditions for the job documented below."
            id: "- ID of the system notification that is notified when an event defined in webhook_notifications is triggered."
            inputs: "- (Required) (String) Array for task to iterate on. This can be a JSON string or a reference to an array parameter."
            interval: "- (Required) Specifies the interval at which the job should run. This value is required."
            is_pinned: "- isn't supported"
            job_cluster: "- (Optional) A list of job databricks_cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. Multi-task syntax"
            job_cluster.job_cluster_key: "- (Required) Identifier that can be referenced in task block, so that cluster is shared between tasks"
            job_cluster.new_cluster: "- Block with almost the same set of parameters as for databricks_cluster resource, except following (check the REST API documentation for full list of supported parameters):"
            job_cluster_key: "- (Optional) Identifier of the Job cluster specified in the job_cluster block."
            job_id: "- (Required)(String) ID of the job"
            job_parameters: "- (Optional)(Map) Job parameters for the task"
            left: "- The left operand of the condition task. It could be a string value, job state, or a parameter reference."
            library: "- (Optional) (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below."
            main_class_name: "- (Optional) The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use SparkContext.getOrCreate to obtain a Spark context; otherwise, runs of the job will fail."
            max_concurrent_runs: "- (Optional) (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to 1."
            max_retries: "- (Optional) (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED or INTERNAL_ERROR lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED or INTERNAL_ERROR."
            metric: "- (Required) string specifying the metric to check.  The only supported metric is RUN_DURATION_SECONDS (check Jobs REST API documentation for the latest information)."
            min_retry_interval_millis: "- (Optional) (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried."
            min_time_between_triggers_seconds: "- (Optional) If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds."
            name: "- (Optional) An optional name for the job. The default value is Untitled."
            name.existing_cluster_id: "- (Optional) If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We strongly suggest to use new_cluster for greater reliability."
            name.new_cluster: "- (Optional) Same set of parameters as for databricks_cluster resource."
            named_parameters: "- (Optional) Named parameters for the task"
            new_cluster: "- (Optional) Task will run on a dedicated cluster.  See databricks_cluster documentation for specification. Some parameters, such as"
            no_alert_for_canceled_runs: "- (Optional) (Bool) don't send alert for cancelled runs."
            no_alert_for_skipped_runs: "- (Optional) (Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the notification_settings configuration block)."
            notebook_path: "- (Required) The path of the databricks_notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required."
            notification_settings: "- (Optional) An optional block controlling the notification settings on the job level documented below."
            on_duration_warning_threshold_exceeded: "- (Optional) (List) list of emails to notify when the duration of a run exceeds the threshold specified by the RUN_DURATION_SECONDS metric in the health block."
            on_failure: "- (Optional) (List) list of emails to notify when the run fails."
            on_start: "- (Optional) (List) list of emails to notify when the run starts."
            on_success: "- (Optional) (List) list of emails to notify when the run completes successfully."
            op: "- The string specifying the operation used to compare operands.  Currently, following operators are supported: EQUAL_TO, GREATER_THAN, GREATER_THAN_OR_EQUAL, LESS_THAN, LESS_THAN_OR_EQUAL, NOT_EQUAL. (Check the API docs for the latest information)."
            outcome: '- (Optional, string) Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are "true" or "false".'
            package_name: "- (Optional) Name of Python package"
            parameter: "- (Optional) Specifices job parameter for the job. See parameter Configuration Block"
            parameters: "- (Optional) Parameters for the task"
            path: "- If source is GIT: Relative path to the file in the repository specified in the git_source block with SQL commands to execute. If source is WORKSPACE: Absolute path to the file in the workspace with SQL commands to execute."
            pause_subscriptions: "- (Optional) flag that specifies if subscriptions are paused or not."
            pipeline_id: "- (Required) The pipeline's unique ID."
            profiles_directory: "- (Optional) The relative path to the directory in the repository specified by git_source where dbt should look in for the profiles.yml file. If not specified, defaults to the repository's root directory. Equivalent to passing --profile-dir to a dbt command."
            project_directory: "- (Required when source is WORKSPACE) The path where dbt should look for dbt_project.yml. Equivalent to passing --project-dir to the dbt CLI."
            provider: "- (Optional, if it's possible to detect Git provider by host name) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult Repos API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition."
            python_file: "- (Required) The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. s3:/, abfss:/, gs:/), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with /Repos. For files stored in a remote repository, the path must be relative. This field is required."
            query: "- (Optional) block consisting of single string field: query_id - identifier of the Databricks Query (databricks_query)."
            queue: "- (Optional) The queue status for the job. See queue Configuration Block below."
            retry_on_timeout: "- (Optional) (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout."
            right: "- The right operand of the condition task. It could be a string value, job state, or parameter reference."
            rules: "- (List) list of rules that are represented as objects with the following attributes:"
            run_as: "- (Optional) The user or the service prinicipal the job runs as. See run_as Configuration Block below."
            run_as.service_principal_name: "- (Optional) The application ID of an active service principal. Setting this field requires the servicePrincipal/user role."
            run_as.user_name: "- (Optional) The email of an active workspace user. Non-admin users can only set this field to their own email."
            run_if: "- (Optional) An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. One of ALL_SUCCESS, AT_LEAST_ONE_SUCCESS, NONE_FAILED, ALL_DONE, AT_LEAST_ONE_FAILED or ALL_FAILED. When omitted, defaults to ALL_SUCCESS."
            schedule: "- (Optional) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below."
            schedule.pause_status: "- (Optional) Indicate whether this schedule is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted and a schedule is provided, the server will default to using UNPAUSED as a value for pause_status."
            schedule.quartz_cron_expression: "- (Required) A Cron expression using Quartz syntax that describes the schedule for a job. This field is required."
            schedule.timezone_id: "- (Required) A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required."
            schema: "- (Optional) The name of the schema dbt should run in. Defaults to default."
            source: "- (Optional) The source of the project. Possible values are WORKSPACE and GIT.  Defaults to GIT if a git_source block is present in the job definition."
            spark_version: parameter in databricks_cluster and other resources.
            spec: "- block describing the Environment. Consists of following attributes:"
            subscriptions: "- (Optional) a list of subscription blocks consisting out of one of the required fields: user_name for user emails or destination_id - for Alert destination's identifier."
            tag: "- name of the Git branch to use. Conflicts with branch and commit."
            tags: "- (Optional) An optional map of the tags associated with the job. See tags Configuration Map"
            task: "- (Optional) A list of task specification that the job will execute. See task Configuration Block below."
            task_key: "- (Required) string specifying an unique key for a given task."
            timeout_seconds: "- (Optional) (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout."
            trigger: "- (Optional) The conditions that triggers the job to start. See trigger Configuration Block below."
            trigger.file_arrival: "- (Optional) configuration block to define a trigger for File Arrival events consisting of following attributes:"
            trigger.pause_status: "- (Optional) Indicate whether this trigger is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted in the block, the server will default to using UNPAUSED as a value for pause_status."
            trigger.periodic: "- (Optional) configuration block to define a trigger for Periodic Triggers consisting of the following attributes:"
            unit: '- (Required) Options are {"DAYS", "HOURS", "WEEKS"}.'
            url: "- (Required) URL to be monitored for file arrivals. The path must point to the root or a subpath of the external location. Please note that the URL must have a trailing slash character (/)."
            value: "- (Required) integer value used to compare to the given metric."
            wait_after_last_change_seconds: "- (Optional) If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds."
            warehouse_id: "- (Optional) The ID of the SQL warehouse that dbt should execute against."
            webhook_notification.on_duration_warning_threshold_exceeded: "- (Optional) (List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the RUN_DURATION_SECONDS metric in the health block."
            webhook_notification.on_failure: "- (Optional) (List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified."
            webhook_notification.on_start: "- (Optional) (List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified."
            webhook_notification.on_success: "- (Optional) (List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified."
            webhook_notifications: "- (Optional) (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below."
            workload_type: "- isn't supported"
        importStatements: []
    lakehouse_monitor Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "lakehouse_monitor Resource - terraform-provider-databricks"'
        name: lakehouse_monitor Resource - terraform-provider-databricks
        title: lakehouse_monitor Resource - terraform-provider-databricks
        argumentDocs:
            assets_dir: "- (Required) - The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)"
            baseline_table_name: |-
                - Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
                table.
            custom_metrics: "- Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows)."
            dashboard_id: "- The ID of the generated dashboard."
            data_classification_config: "- The data classification config for the monitor"
            definition: "- create metric definition"
            drift_metrics_table_name: "- The full name of the drift metrics table. Format: catalog_name.schema_name.table_name."
            granularities: "-  List of granularities to use when aggregating data into time windows based on their timestamp."
            id: "-  ID of this monitor is the same as the full table name of the format {catalog}.{schema_name}.{table_name}"
            inference_log: "- Configuration for the inference log monitor"
            input_columns: "- Columns on the monitored table to apply the custom metrics to."
            label_col: "- Column of the model label"
            model_id_col: "- Column of the model id or version"
            monitor_version: "- The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted"
            name: "- Name of the custom metric."
            notifications: "- The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name email_addresses containing a list of emails to notify:"
            on_failure: "- who to send notifications to on monitor failure."
            on_new_classification_tag_detected: "- Who to send notifications to when new data classification tags are detected."
            output_data_type: "- The output type of the custom metric."
            output_schema_name: "- (Required) - Schema where output metric tables are created"
            pause_status: "- optional string field that indicates whether a schedule is paused (PAUSED) or not (UNPAUSED)."
            prediction_col: "- Column of the model prediction"
            prediction_proba_col: "- Column of the model prediction probabilities"
            problem_type: "- Problem type the model aims to solve. Either PROBLEM_TYPE_CLASSIFICATION or PROBLEM_TYPE_REGRESSION"
            profile_metrics_table_name: "- The full name of the profile metrics table. Format: catalog_name.schema_name.table_name."
            quartz_cron_expression: "- string expression that determines when to run the monitor. See Quartz documentation for examples."
            schedule: "- The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:"
            skip_builtin_dashboard: "- Whether to skip creating a default dashboard summarizing data quality metrics."
            slicing_exprs: "- List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices."
            snapshot: "- Configuration for monitoring snapshot tables."
            status: "- Status of the Monitor"
            table_name: "- (Required) - The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}"
            time_series: "- Configuration for monitoring timeseries tables."
            timestamp_col: "- Column of the timestamp of predictions"
            timezone_id: "- string with timezone id (e.g., PST) in which to evaluate the Quartz expression."
            type: "- The type of the custom metric."
            warehouse_id: "- Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used."
        importStatements: []
    library Resource - terraform-provider-databricks:
        subCategory: Compute
        description: '""page_title: "library Resource - terraform-provider-databricks"'
        name: library Resource - terraform-provider-databricks
        title: library Resource - terraform-provider-databricks
        argumentDocs:
            dbfs:/mnt/name: .
        importStatements: []
    metastore Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "metastore Resource - terraform-provider-databricks"'
        name: metastore Resource - terraform-provider-databricks
        title: metastore Resource - terraform-provider-databricks
        argumentDocs:
            delta_sharing_organization_name: "- (Optional) The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource."
            delta_sharing_recipient_token_lifetime_in_seconds: "- (Optional) Required along with delta_sharing_scope. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration."
            delta_sharing_scope: "- (Optional) Required along with delta_sharing_recipient_token_lifetime_in_seconds. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing."
            force_destroy: "- (Optional) Destroy metastore regardless of its contents."
            id: "- system-generated ID of this Unity Catalog Metastore."
            name: "- Name of metastore."
            owner: "- (Optional) Username/groupname/sp application_id of the metastore owner."
            region: "- (Mandatory for account-level) The region of the metastore"
            storage_root: "- (Optional) Path on cloud storage account, where managed databricks_table are stored. Change forces creation of a new resource. If no storage_root is defined for the metastore, each catalog must have a storage_root defined."
        importStatements: []
    metastore_assignment Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "metastore_assignment Resource - terraform-provider-databricks"'
        name: metastore_assignment Resource - terraform-provider-databricks
        title: metastore_assignment Resource - terraform-provider-databricks
        argumentDocs:
            default_catalog_name: "- (Deprecated) Default catalog used for this assignment. Please use databricks_default_namespace_setting instead."
            id: "- ID of this metastore assignment in form of <workspace_id>|<metastore_id>."
            metastore_id: "- Unique identifier of the parent Metastore"
            workspace_id: "- id of the workspace for the assignment"
        importStatements: []
    metastore_data_access Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "metastore_data_access Resource - terraform-provider-databricks"'
        name: metastore_data_access Resource - terraform-provider-databricks
        title: metastore_data_access Resource - terraform-provider-databricks
        argumentDocs:
            id: "- ID of this data access configuration in form of <metastore_id>|<name>."
            is_default: "-  whether to set this credential as the default for the metastore. In practice, this should always be true."
        importStatements: []
    mlflow_experiment Resource - terraform-provider-databricks:
        subCategory: MLflow
        description: '""page_title: "mlflow_experiment Resource - terraform-provider-databricks"'
        name: mlflow_experiment Resource - terraform-provider-databricks
        title: mlflow_experiment Resource - terraform-provider-databricks
        argumentDocs:
            artifact_location: "- Path to dbfs:/ or s3:// artifact location of the MLflow experiment."
            description: "- The description of the MLflow experiment."
            id: "- ID of the MLflow experiment."
            name: "- (Required) Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. /Users/<some-username>/my-experiment. For more information about changes to experiment naming conventions, see mlflow docs."
        importStatements: []
    mlflow_model Resource - terraform-provider-databricks:
        subCategory: MLflow
        description: '""page_title: "mlflow_model Resource - terraform-provider-databricks"'
        name: mlflow_model Resource - terraform-provider-databricks
        title: mlflow_model Resource - terraform-provider-databricks
        argumentDocs:
            description: "- The description of the MLflow model."
            id: "- ID of the MLflow model, the same as name."
            name: "- (Required) Name of MLflow model. Change of name triggers new resource."
            tags: "- Tags for the MLflow model."
        importStatements: []
    mlflow_webhook Resource - terraform-provider-databricks:
        subCategory: MLflow
        description: '""page_title: "mlflow_webhook Resource - terraform-provider-databricks"'
        name: mlflow_webhook Resource - terraform-provider-databricks
        title: mlflow_webhook Resource - terraform-provider-databricks
        argumentDocs:
            access_token: "- (Required, Sensitive) The personal access token used to authorize webhook's job runs."
            authorization: "- (Optional) Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form <auth type> <credentials>, e.g. Bearer <access_token>. If set to an empty string, no authorization header will be included in the request."
            description: "- Optional description of the MLflow webhook."
            enable_ssl_verification: "- (Optional) Enable/disable SSL certificate validation. Default is true. For self-signed certificates, this field must be false AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host."
            events: "- (Required) The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, MODEL_VERSION_CREATED, MODEL_VERSION_TRANSITIONED_STAGE, TRANSITION_REQUEST_CREATED, etc.  Refer to the Webhooks API documentation for a full list of supported events."
            id: "- Unique ID of the MLflow Webhook."
            job_id: "- (Required) ID of the Databricks job that the webhook runs."
            model_name: "- (Optional) Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models."
            secret: "- (Optional, Sensitive) Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as X-Databricks-Signature: encoded_payload."
            status: "- Optional status of webhook. Possible values are ACTIVE, TEST_MODE, DISABLED. Default is ACTIVE."
            url: "- (Required) External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to documentation for more details."
            workspace_url: "- (Optional) URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created."
        importStatements: []
    model_serving Resource - terraform-provider-databricks:
        subCategory: Serving
        description: '""page_title: "model_serving Resource - terraform-provider-databricks"'
        name: model_serving Resource - terraform-provider-databricks
        title: model_serving Resource - terraform-provider-databricks
        argumentDocs:
            _plaintext: suffix) or in plain text (parameters with _plaintext suffix)!
            ai_gateway: "- (Optional) A block with AI Gateway configuration for the serving endpoint. Note: only external model endpoints are supported as of now."
            ai_gateway.guardrails: "- (Optional) Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:"
            ai_gateway.inference_table_config: "- (Optional) Block describing the configuration of usage tracking. Consists of the following attributes:"
            ai_gateway.rate_limits: "- (Optional) Block describing rate limits for AI gateway. For details see the description of rate_limits block above."
            ai_gateway.usage_tracking_config: "- (Optional) Block with configuration for payload logging using inference tables. For details see the description of auto_capture_config block above."
            ai21labs_api_key: "- The Databricks secret key reference for an AI21Labs API key."
            ai21labs_api_key_plaintext: "- An AI21 Labs API key provided as a plaintext string."
            ai21labs_config: "- AI21Labs Config"
            amazon_bedrock_config: "- Amazon Bedrock Config"
            anthropic_api_key: "- The Databricks secret key reference for an Anthropic API key."
            anthropic_api_key_plaintext: "- The Anthropic API key provided as a plaintext string."
            anthropic_config: "- Anthropic Config"
            auto_capture_config: "- Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog."
            auto_capture_config.catalog_name: "- The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set."
            auto_capture_config.enabled: "- If inference tables are enabled or not. NOTE: If you have already disabled payload logging once, you cannot enable it again."
            auto_capture_config.schema_name: "- The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set."
            auto_capture_config.table_name_prefix: "- The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set."
            aws_access_key_id: "- The Databricks secret key reference for an AWS Access Key ID with permissions to interact with Bedrock services."
            aws_access_key_id_plaintext: "- An AWS access key ID with permissions to interact with Bedrock services provided as a plaintext string."
            aws_region: "- The AWS region to use. Bedrock has to be enabled there."
            aws_secret_access_key: "- The Databricks secret key reference for an AWS Secret Access Key paired with the access key ID, with permissions to interact with Bedrock services."
            aws_secret_access_key_plaintext: "-  An AWS secret access key paired with the access key ID, with permissions to interact with Bedrock services provided as a plaintext string."
            bedrock_provider: "- The underlying provider in Amazon Bedrock. Supported values (case insensitive) include: Anthropic, Cohere, AI21Labs, Amazon."
            behavior: "- a string that describes the behavior for PII filter. Currently only BLOCK value is supported."
            cohere_api_key: "- The Databricks secret key reference for a Cohere API key."
            cohere_api_key_plaintext: "- The Cohere API key provided as a plaintext string."
            cohere_config: "- Cohere Config"
            config: "- (Required) The model serving endpoint configuration."
            databricks_api_token: "- The Databricks secret key reference for a Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model."
            databricks_api_token_plaintext: "- The Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model provided as a plaintext string."
            databricks_model_serving_config: "- Databricks Model Serving Config"
            databricks_workspace_url: "- The URL of the Databricks workspace containing the model serving endpoint pointed to by this external model."
            enabled: "- boolean flag specifying if usage tracking is enabled."
            google_cloud_vertex_ai_config: "- Google Cloud Vertex AI Config."
            id: "- Equal to the name argument and used to identify the serving endpoint."
            input: "- A block with configuration for input guardrail filters:"
            invalid_keywords: "- List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content."
            microsoft_entra_client_id: "- This field is only required for Azure AD OpenAI and is the Microsoft Entra Client ID."
            microsoft_entra_client_secret: "- The Databricks secret key reference for a client secret used for Microsoft Entra ID authentication."
            microsoft_entra_client_secret_plaintext: "- The client secret used for Microsoft Entra ID authentication provided as a plaintext string."
            microsoft_entra_tenant_id: "- This field is only required for Azure AD OpenAI and is the Microsoft Entra Tenant ID."
            name: "- (Required) The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name."
            openai_api_base: '- This is the base URL for the OpenAI API (default: "https://api.openai.com/v1"). For Azure OpenAI, this field is required and is the base URL for the Azure OpenAI API service provided by Azure.'
            openai_api_key: "- The Databricks secret key reference for an OpenAI or Azure OpenAI API key."
            openai_api_key_plaintext: "- The OpenAI API key using the OpenAI or Azure service provided as a plaintext string."
            openai_api_type: "- This is an optional field to specify the type of OpenAI API to use. For Azure OpenAI, this field is required, and this parameter represents the preferred security access validation protocol. For access token validation, use azure. For authentication using Azure Active Directory (Azure AD) use, azuread."
            openai_api_version: "- This is an optional field to specify the OpenAI API version. For Azure OpenAI, this field is required and is the version of the Azure OpenAI service to utilize, specified by a date."
            openai_config: "- OpenAI Config"
            openai_deployment_name: "- This field is only required for Azure OpenAI and is the name of the deployment resource for the Azure OpenAI service."
            openai_organization: "- This is an optional field to specify the organization in OpenAI or Azure OpenAI."
            output: "- A block with configuration for output guardrail filters.  Has the same structure as input block."
            palm_api_key: "- The Databricks secret key reference for a PaLM API key."
            palm_api_key_plaintext: "- The PaLM API key provided as a plaintext string."
            palm_config: "- PaLM Config"
            pii: "- Block with configuration for guardrail PII filter:"
            private_key: "- The Databricks secret key reference for a private key for the service account that has access to the Google Cloud Vertex AI Service."
            private_key_plaintext: "- The private key for the service account that has access to the Google Cloud Vertex AI Service is provided as a plaintext secret."
            project_id: "- This is the Google Cloud project id that the service account is associated with."
            provider: "- (Required) The name of the provider for the external model. Currently, the supported providers are ai21labs, anthropic, amazon-bedrock, cohere, databricks-model-serving, google-cloud-vertex-ai, openai, and palm."
            rate_limits: "- A list of rate limit blocks to be applied to the serving endpoint. Note: only external and foundation model endpoints are supported as of now."
            rate_limits.calls: "- (Required) Used to specify how many calls are allowed for a key within the renewal_period."
            rate_limits.key: "- (Optional) Key field for a serving endpoint rate limit. Currently, only user and endpoint are supported, with endpoint being the default if not specified."
            rate_limits.renewal_period: "- (Required) Renewal period field for a serving endpoint rate limit. Currently, only minute is supported."
            region: "- This is the region for the Google Cloud Vertex AI Service."
            route_optimized: "- (Optional) A boolean enabling route optimization for the endpoint. Note: only available for custom models."
            safety: "- the boolean flag that indicates whether the safety filter is enabled."
            served_entities: "- A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities."
            served_entities.entity_name: "- The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type FEATURE_SPEC in the UC. If it is a UC object, the full name of the object should be given in the form of catalog_name.schema_name.model_name."
            served_entities.entity_version: "- The version of the model in Databricks Model Registry to be served or empty if the entity is a FEATURE_SPEC."
            served_entities.environment_vars: '- An object containing a set of optional, user-specified environment variable key-value pairs used for serving this entity. Note: this is an experimental feature and is subject to change. Example entity environment variables that refer to Databricks secrets: {"OPENAI_API_KEY": "{{secrets/my_scope/my_key}}", "DATABRICKS_TOKEN": "{{secrets/my_scope2/my_key2}}"}'
            served_entities.external_model: "- The external model to be served. NOTE: Only one of external_model and (entity_name, entity_version, workload_size, workload_type, and scale_to_zero_enabled) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an external_model is present, the served entities list can only have one served_entity object. An existing endpoint with external_model can not be updated to an endpoint without external_model. If the endpoint is created without external_model, users cannot update it to add external_model later."
            served_entities.instance_profile_arn: "- ARN of the instance profile that the served entity uses to access AWS resources."
            served_entities.max_provisioned_throughput: "-  The maximum tokens per second that the endpoint can scale up to."
            served_entities.min_provisioned_throughput: "- The minimum tokens per second that the endpoint can scale down to."
            served_entities.name: "- The name of a served entity. It must be unique across an endpoint. A served entity name can consist of alphanumeric characters, dashes, and underscores. If not specified for an external model, this field defaults to external_model.name, with '.' and ':' replaced with '-', and if not specified for other entities, it defaults to -."
            served_entities.scale_to_zero_enabled: "- Whether the compute resources for the served entity should scale down to zero."
            served_entities.workload_size: "- The workload size of the served entity. The workload size corresponds to a range of provisioned concurrency that the compute autoscales between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency). If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size is 0."
            served_entities.workload_type: "- The workload type of the served entity. The workload type selects which type of compute to use in the endpoint. The default value for this parameter is CPU. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the available GPU types."
            served_entity_name: "- (Required) The name of the served entity this route configures traffic for. This needs to match the name of a served_entity block."
            served_models: "- (Deprecated, use served_entities instead) Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models."
            served_models.environment_vars: "- (Optional) a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}."
            served_models.instance_profile_arn: "- (Optional) ARN of the instance profile that the served model will use to access AWS resources."
            served_models.model_name: "- (Required) The name of the model in Databricks Model Registry to be served."
            served_models.model_version: "- (Required) The version of the model in Databricks Model Registry to be served."
            served_models.name: "- The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores."
            served_models.scale_to_zero_enabled: "- Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true."
            served_models.workload_size: "- (Required) The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency)."
            served_models.workload_type: "- The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU."
            serving_endpoint_id: "- Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations."
            tags: "- Tags to be attached to the serving endpoint and automatically propagated to billing logs."
            tags.key: "- The key field for a tag."
            tags.value: "- The value field for a tag."
            task: "- The task type of the external model."
            traffic_config: "- A single block represents the traffic split configuration amongst the served models."
            traffic_config.routes: "- (Required) Each block represents a route that defines traffic to each served entity. Each served_entity block needs to have a corresponding routes block."
            traffic_percentage: "- (Required) The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive."
            valid_topics: "- The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics."
        importStatements: []
    mount Resource - terraform-provider-databricks:
        subCategory: Storage
        description: '""page_title: "mount Resource - terraform-provider-databricks"'
        name: mount Resource - terraform-provider-databricks
        title: mount Resource - terraform-provider-databricks
        argumentDocs:
            abfs: "- to mount ADLS Gen2 using Azure Blob Filesystem (ABFS) driver"
            abfs.client_id: "- (Required) (String) This is the client_id (Application Object ID) for the enterprise application for the service principal."
            abfs.client_secret_key: "- (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored."
            abfs.client_secret_scope: "- (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored."
            abfs.container_name: "- (Required) (String) ADLS gen2 container name. (Could be omitted if resource_id is provided)"
            abfs.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            abfs.initialize_file_system: "- (Required) (Bool) either or not initialize FS for the first use"
            abfs.storage_account_name: "- (Required) (String) The name of the storage resource in which the data is. (Could be omitted if resource_id is provided)"
            abfs.tenant_id: "- (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract tenant_id from it)."
            adl: "- to mount ADLS Gen1 using Azure Data Lake (ADL) driver"
            adl.client_id: "- (Required) (String) This is the client_id for the enterprise application for the service principal."
            adl.client_secret_key: "- (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored."
            adl.client_secret_scope: "- (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored."
            adl.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            adl.spark_conf_prefix: "- (Optional) (String) This is the spark configuration prefix for adls gen 1 mount. The options are fs.adl, dfs.adls. Use fs.adl for runtime 6.0 and above for the clusters. Otherwise use dfs.adls. The default value is: fs.adl."
            adl.storage_resource_name: "- (Required) (String) The name of the storage resource in which the data is for ADLS gen 1. This is what you are trying to mount. (Could be omitted if resource_id is provided)"
            adl.tenant_id: "- (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract tenant_id from it)"
            bucket_name: for AWS S3 and Google Cloud Storage
            cluster_id: "- (Optional, String) Cluster to use for mounting. If no cluster is specified, a new cluster will be created and will mount the bucket for all of the clusters in this workspace. If the cluster is not running - it's going to be started, so be aware to set auto-termination rules on it."
            container_name: for ADLS Gen2 and Azure Blob Storage
            encryption_type: "- (Optional, String) encryption type. Currently used only for AWS S3 mounts"
            extra_configs: "- (Optional, String map) configuration parameters that are necessary for mounting of specific storage"
            gs: "- to mount Google Cloud Storage"
            gs.bucket_name: "- (Required) (String) GCS bucket name to be mounted."
            gs.service_account: "- (Optional) (String) email of registered Google Service Account for data access.  If it's not specified, then the cluster_id should be provided, and the cluster should have a Google service account attached to it."
            id: "- mount name"
            mount_name: to name
            name: "- (Optional, String) Name, under which mount will be accessible in dbfs:/mnt/<MOUNT_NAME>. If not specified, provider will try to infer it from depending on the resource type:"
            resource_id: "- (Optional, String) resource ID for a given storage account. Could be used to fill defaults, such as storage account & container names on Azure."
            s3: "- to mount AWS S3"
            s3.bucket_name: "- (Required) (String) S3 bucket name to be mounted."
            s3.instance_profile: "- (Optional) (String) ARN of registered instance profile for data access.  If it's not specified, then the cluster_id should be provided, and the cluster should have an instance profile attached to it. If both cluster_id & instance_profile are specified, then cluster_id takes precedence."
            s3_bucket_name: to bucket_name
            source: "- (String) HDFS-compatible url"
            storage_resource_name: for ADLS Gen1
            uri: "- (Optional, String) the URI for accessing specific storage (s3a://...., abfss://...., gs://...., etc.)"
            wasb: "- to mount Azure Blob Storage using Windows Azure Storage Blob (WASB) driver"
            wasb.auth_type: "- (Required) (String) This is the auth type for blob storage. This can either be SAS tokens (SAS) or account access keys (ACCESS_KEY)."
            wasb.container_name: "- (Required) (String) The container in which the data is. This is what you are trying to mount. (Could be omitted if resource_id is provided)"
            wasb.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            wasb.storage_account_name: "- (Required) (String) The name of the storage resource in which the data is. (Could be omitted if resource_id is provided)"
            wasb.token_secret_key: "- (Required) (String) This is the secret key in which your auth type token is stored."
            wasb.token_secret_scope: "- (Required) (String) This is the secret scope in which your auth type token is stored."
        importStatements: []
    mws_credentials Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_credentials Resource - terraform-provider-databricks"'
        name: mws_credentials Resource - terraform-provider-databricks
        title: mws_credentials Resource - terraform-provider-databricks
        argumentDocs:
            account_id: '- (Deprecated) Maintained for backwards compatibility and will be removed in a later version. It should now be specified under a provider instance where host = "https://accounts.cloud.databricks.com"'
            creation_time: "- (Integer) time of credentials registration"
            credentials_id: "- (String) identifier of credentials"
            credentials_name: "- (Required) name of credentials to register"
            id: "- Canonical unique identifier for the mws credentials."
            role_arn: "- (Required) ARN of cross-account role"
        importStatements: []
    mws_customer_managed_keys Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_customer_managed_keys Resource - terraform-provider-databricks"'
        name: mws_customer_managed_keys Resource - terraform-provider-databricks
        title: mws_customer_managed_keys Resource - terraform-provider-databricks
        argumentDocs:
            MANAGED_SERVICES: "- for encryption of the workspace objects (notebooks, secrets) that are stored in the control plane"
            STORAGE: "- for encryption of the DBFS Storage & Cluster EBS Volumes"
            account_id: "- Account Id that could be found in the top right corner of Accounts Console"
            aws_key_info: "- This field is a block and is documented below. This conflicts with gcp_key_info"
            aws_key_info.key_alias: "- (Optional) The AWS KMS key alias."
            aws_key_info.key_arn: "- The AWS KMS key's Amazon Resource Name (ARN)."
            aws_key_info.key_region: "- (Optional) (Computed) The AWS region in which KMS key is deployed to. This is not required."
            creation_time: "- (Integer) Time in epoch milliseconds when the customer key was created."
            customer_managed_key_id: "- (String) ID of the encryption key configuration object."
            gcp_key_info: "- This field is a block and is documented below. This conflicts with aws_key_info"
            gcp_key_info.kms_key_id: "- The GCP KMS key's resource name."
            id: "- Canonical unique identifier for the mws customer managed keys."
            use_cases: "- (since v0.3.4) List of use cases for which this key will be used. If you've used the resource before, please add  Possible values are:"
            use_cases = ["MANAGED_SERVICES"]: to keep the previous behaviour.
        importStatements: []
    mws_log_delivery Resource - terraform-provider-databricks:
        subCategory: Log Delivery
        description: '""page_title: "mws_log_delivery Resource - terraform-provider-databricks"'
        name: mws_log_delivery Resource - terraform-provider-databricks
        title: mws_log_delivery Resource - terraform-provider-databricks
        argumentDocs:
            account_id: "- Account Id that could be found in the top right corner of Accounts Console."
            config_id: "- Databricks log delivery configuration ID."
            config_name: "- The optional human-readable name of the log delivery configuration. Defaults to empty."
            credentials_id: "- The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page."
            delivery_path_prefix: "- (Optional) Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character."
            delivery_start_time: "- (Optional) The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03."
            id: "- the ID of log delivery configuration in form of account_id|config_id."
            log_type: "- The type of log delivery. BILLABLE_USAGE and AUDIT_LOGS are supported."
            output_format: "- The file type of log delivery. Currently CSV (for BILLABLE_USAGE) and JSON (for AUDIT_LOGS) are supported."
            status: "- Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update."
            storage_configuration_id: "- The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page."
            workspace_ids_filter: "- (Optional) By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the multitenant version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces."
        importStatements: []
    mws_ncc_binding Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_ncc_binding Resource - terraform-provider-databricks"'
        name: mws_ncc_binding Resource - terraform-provider-databricks
        title: mws_ncc_binding Resource - terraform-provider-databricks
        argumentDocs:
            network_connectivity_config_id: "- Canonical unique identifier of Network Connectivity Config in Databricks Account."
            workspace_id: "- Identifier of the workspace to attach the NCC to. Change forces creation of a new resource."
        importStatements: []
    mws_ncc_private_endpoint_rule Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_ncc_private_endpoint_rule Resource - terraform-provider-databricks"'
        name: mws_ncc_private_endpoint_rule Resource - terraform-provider-databricks
        title: mws_ncc_private_endpoint_rule Resource - terraform-provider-databricks
        argumentDocs:
            DISCONNECTED: ": Connection was removed by the private link resource owner, the private endpoint becomes informative and should be deleted for clean-up."
            ESTABLISHED: ": The endpoint has been approved and is ready to be used in your serverless compute resources."
            PENDING: ": The endpoint has been created and pending approval."
            REJECTED: ": Connection was rejected by the private link resource owner."
            connection_state: |-
                - The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.
                The possible values are:
            creation_time: "- Time in epoch milliseconds when this object was created."
            deactivated: "- Whether this private endpoint is deactivated."
            deactivated_at: "- Time in epoch milliseconds when this object was deactivated."
            endpoint_name: '- The name of the Azure private endpoint resource, e.g. "databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234"'
            group_id: "- The sub-resource type (group ID) of the target resource. Must be one of blob, dfs, sqlServer or mysqlServer. Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for blob and one for dfs. Change forces creation of a new resource."
            network_connectivity_config_id: "- Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource."
            resource_id: "- The Azure resource ID of the target resource. Change forces creation of a new resource."
            rule_id: "- the ID of a private endpoint rule."
            updated_time: "- Time in epoch milliseconds when this object was updated."
        importStatements: []
    mws_network_connectivity_config Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_network_connectivity_config Resource - terraform-provider-databricks"'
        name: mws_network_connectivity_config Resource - terraform-provider-databricks
        title: mws_network_connectivity_config Resource - terraform-provider-databricks
        argumentDocs:
            aws_stable_ip_rule: "(AWS only) - block with information about stable AWS IP CIDR blocks. You can use these to configure the firewall of your resources to allow traffic from your Databricks workspace.  Consists of the following fields:"
            azure_private_endpoint_rules: (Azure only) - list containing information about configure Azure Private Endpoints.
            azure_service_endpoint_rule: "(Azure only) - block with information about stable Azure service endpoints. You can configure the firewall of your Azure resources to allow traffic from your Databricks serverless compute resources.  Consists of the following fields:"
            cidr_blocks: "- list of IP CIDR blocks."
            default_rules: "- block describing network connectivity rules that are applied by default without resource specific configurations.  Consists of the following fields:"
            egress_conf: "- block containing information about network connectivity rules that apply to network traffic from your serverless compute resources. Consists of the following fields:"
            id: "- combination of account_id and network_connectivity_config_id separated by / character"
            name: "- Name of Network Connectivity Config in Databricks Account. Change forces creation of a new resource."
            network_connectivity_config_id: "- Canonical unique identifier of Network Connectivity Config in Databricks Account"
            region: "- Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource."
            subnets: "- list of subnets from which Databricks network traffic originates when accessing your Azure resources."
            target_region: "- the Azure region in which this service endpoint rule applies."
            target_rules: "- block describing network connectivity rules that configured for each destinations. These rules override default rules.  Consists of the following fields:"
            target_services: "- the Azure services to which this service endpoint rule applies to."
        importStatements: []
    mws_networks Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_networks Resource - terraform-provider-databricks"'
        name: mws_networks Resource - terraform-provider-databricks
        title: mws_networks Resource - terraform-provider-databricks
        argumentDocs:
            account_id: "- Account Id that could be found in the top right corner of Accounts Console"
            gcp_network_info: "- (GCP only) a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:"
            id: "- Canonical unique identifier for the mws networks."
            network_id: "- (String) id of network to be used for databricks_mws_workspaces resource."
            network_name: "- name under which this network is registered"
            network_project_id: "- The Google Cloud project ID of the VPC network."
            pod_ip_range_name: "- The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can only be used by one workspace."
            security_group_ids: "- (AWS only) ids of aws_security_group"
            service_ip_range_name: "- The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can only be used by one workspace."
            subnet_id: "- The ID of the subnet associated with this network."
            subnet_ids: "- (AWS only) ids of aws_subnet"
            subnet_region: "- The Google Cloud region of the workspace data plane. For example, us-east4."
            vpc_endpoints: "- (Optional) mapping of databricks_mws_vpc_endpoint for PrivateLink or Private Service Connect connections"
            vpc_id: "- (AWS only) aws_vpc id"
            vpc_status: "- (String) VPC attachment status"
            workspace_id: "- (Integer) id of associated workspace"
        importStatements: []
    mws_permission_assignment Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "mws_permission_assignment Resource - terraform-provider-databricks"'
        name: mws_permission_assignment Resource - terraform-provider-databricks
        title: mws_permission_assignment Resource - terraform-provider-databricks
        argumentDocs:
            '"ADMIN"': "- Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more."
            '"USER"': "- Can access the workspace with basic privileges."
            id: "- ID of the permission assignment in form of workspace_id|principal_id."
            permissions: "- The list of workspace permissions to assign to the principal:"
            principal_id: "- Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks_service_principal or databricks_group data sources."
            workspace_id: "- Databricks workspace ID."
        importStatements: []
    mws_private_access_settings Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_private_access_settings Resource - terraform-provider-databricks"'
        name: mws_private_access_settings Resource - terraform-provider-databricks
        title: mws_private_access_settings Resource - terraform-provider-databricks
        argumentDocs:
            allowed_vpc_endpoint_ids: "- (Optional) An array of databricks_mws_vpc_endpoint vpc_endpoint_id (not id). Only used when private_access_level is set to ENDPOINT. This is an allow list of databricks_mws_vpc_endpoint that in your account that can connect to your databricks_mws_workspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting public_access_enabled to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list."
            id: "- the ID of the Private Access Settings in form of account_id/private_access_settings_id."
            private_access_level: "- (Optional) The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. ACCOUNT level access (default) lets only databricks_mws_vpc_endpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. ENDPOINT level access lets only specified databricks_mws_vpc_endpoint connect to your workspace. Please see the allowed_vpc_endpoint_ids documentation for more details."
            private_access_settings_id: "- Canonical unique identifier of Private Access Settings in Databricks Account"
            private_access_settings_name: "- Name of Private Access Settings in Databricks Account"
            public_access_enabled: (Boolean, Optional, false by default on AWS, true by default on GCP) - If true, the databricks_mws_workspaces can be accessed over the databricks_mws_vpc_endpoint as well as over the public network. In such a case, you could also configure an databricks_ip_access_list for the workspace, to restrict the source networks that could be used to access it over the public network. If false, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.
            region: "- Region of AWS VPC or the Google Cloud VPC network"
            status: "- (AWS only) Status of Private Access Settings"
        importStatements: []
    mws_storage_configurations Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_storage_configurations Resource - terraform-provider-databricks"'
        name: mws_storage_configurations Resource - terraform-provider-databricks
        title: mws_storage_configurations Resource - terraform-provider-databricks
        argumentDocs:
            account_id: "- Account Id that could be found in the top right corner of Accounts Console"
            bucket_name: "- name of AWS S3 bucket"
            id: "- Canonical unique identifier for the mws storage configurations."
            storage_configuration_id: "- (String) id of storage config to be used for databricks_mws_workspace resource."
            storage_configuration_name: "- name under which this storage configuration is stored"
        importStatements: []
    mws_vpc_endpoint Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_vpc_endpoint Resource - terraform-provider-databricks"'
        name: mws_vpc_endpoint Resource - terraform-provider-databricks
        title: mws_vpc_endpoint Resource - terraform-provider-databricks
        argumentDocs:
            account_id: "- Account Id that could be found in the Accounts Console for AWS or GCP"
            aws_endpoint_service_id: "- (AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the Databricks PrivateLink documentation"
            aws_vpc_endpoint_id: "- (AWS only) ID of configured aws_vpc_endpoint"
            endpoint_region: "- Region of the PSC endpoint."
            gcp_vpc_endpoint_info: "- (GCP only) a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:"
            id: "- the ID of VPC Endpoint in form of account_id/vpc_endpoint_id"
            project_id: "- The Google Cloud project ID of the VPC network where the PSC connection resides."
            psc_connection_id: "- The unique ID of this PSC connection."
            psc_endpoint_name: "- The name of the PSC endpoint in the Google Cloud project."
            region: "- (AWS only) Region of AWS VPC"
            service_attachment_id: "- The service attachment this PSC connection connects to."
            state: "- (AWS Only) State of VPC Endpoint"
            vpc_endpoint_id: "- Canonical unique identifier of VPC Endpoint in Databricks Account"
            vpc_endpoint_name: "- Name of VPC Endpoint in Databricks Account"
        importStatements: []
    mws_workspaces Resource - terraform-provider-databricks:
        subCategory: Deployment
        description: '""page_title: "mws_workspaces Resource - terraform-provider-databricks"'
        name: mws_workspaces Resource - terraform-provider-databricks
        title: mws_workspaces Resource - terraform-provider-databricks
        argumentDocs:
            account_id: "- Account Id that could be found in the top right corner of Accounts Console."
            aws_region: "- (AWS only) region of VPC."
            cloud_resource_container: "- (GCP only) A block that specifies GCP workspace configurations, consisting of following blocks:"
            connectivity_type: ": Specifies the network connectivity types for the GKE nodes and the GKE master network. Possible values are: PRIVATE_NODE_PUBLIC_MASTER, PUBLIC_NODE_PUBLIC_MASTER."
            creation_time: "- (Integer) time when workspace was created"
            custom_tags: "- (Optional / AWS only) - The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any default_tags or custom_tags on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed."
            deployment_name: "- (Optional) part of URL as in https://<prefix>-<deployment-name>.cloud.databricks.com. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created."
            gcp: "- A block that consists of the following field:"
            gcp_workspace_sa: "- (String, GCP only) identifier of a service account created for the workspace in form of db-<workspace-id>@prod-gcp-<region>.iam.gserviceaccount.com"
            gke_config: "- (GCP only) A block that specifies GKE configuration for the Databricks workspace:"
            id: "- (String) Canonical unique identifier for the workspace, of the format <account-id>/<workspace-id>"
            location: "- (GCP only) region of the subnet."
            managed_services_customer_managed_key_id: "- (Optional) customer_managed_key_id from customer managed keys with use_cases set to MANAGED_SERVICES. This is used to encrypt the workspace's notebook and secret data in the control plane."
            master_ip_range: ": The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled. It must be exactly as big as /28."
            network_id: "- (Optional) network_id from networks."
            pricing_tier: "- (Optional) - The pricing tier of the workspace."
            private_access_settings_id: "- (Optional) Canonical unique identifier of databricks_mws_private_access_settings in Databricks Account."
            project_id: "- The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace."
            storage_configuration_id: "- (AWS only)storage_configuration_id from storage configuration."
            storage_customer_managed_key_id: "- (Optional) customer_managed_key_id from customer managed keys with use_cases set to STORAGE. This is used to encrypt the DBFS Storage & Cluster Volumes."
            token {}.comment: '- (Optional) Comment, that will appear in "User Settings / Access Tokens" page on Workspace UI. By default it''s "Terraform PAT".'
            token {}.lifetime_seconds: "- (Optional) Token expiry lifetime. By default its 2592000 (30 days)."
            workspace_id: "- (String) workspace id"
            workspace_name: "- name of the workspace, will appear on UI."
            workspace_status: "- (String) workspace status"
            workspace_status_message: "- (String) updates on workspace status"
            workspace_url: "- (String) URL of the workspace"
        importStatements: []
    notebook Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "notebook Resource - terraform-provider-databricks"'
        name: notebook Resource - terraform-provider-databricks
        title: notebook Resource - terraform-provider-databricks
        argumentDocs:
            content_base64: "- The base64-encoded notebook source code. Conflicts with source. Use of content_base64 is discouraged, as it's increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a notebook with configuration properties for a data pipeline."
            id: "-  Path of notebook on workspace"
            language: "-  (required with content_base64) One of SCALA, PYTHON, SQL, R."
            object_id: "-  Unique identifier for a NOTEBOOK"
            path: '-  (Required) The absolute path of the notebook or directory, beginning with "/", e.g. "/Demo".'
            source: "- Path to notebook in source code format on local filesystem. Conflicts with content_base64."
            url: "- Routable URL of the notebook"
            workspace_path: "- path on Workspace File System (WSFS) in form of /Workspace + path"
        importStatements: []
    notification_destination Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "notification_destination Resource - terraform-provider-databricks"'
        name: notification_destination Resource - terraform-provider-databricks
        title: notification_destination Resource - terraform-provider-databricks
        argumentDocs:
            addresses: "- (Required) The list of email addresses to send notifications to."
            config: "- (Required) The configuration of the Notification Destination. It must contain exactly one of the following blocks:"
            destination_type: "- the type of Notification Destination."
            display_name: "- (Required) The display name of the Notification Destination."
            email: "- The email configuration of the Notification Destination. It must contain the following:"
            generic_webhook: "- The Generic Webhook configuration of the Notification Destination. It must contain the following:"
            id: "- The unique ID of the Notification Destination."
            integration_key: "- (Required) The PagerDuty integration key."
            microsoft_teams: "- The Microsoft Teams configuration of the Notification Destination. It must contain the following:"
            pagerduty: "- The PagerDuty configuration of the Notification Destination. It must contain the following:"
            password: "- (Optional) The password for basic authentication."
            slack: "- The Slack configuration of the Notification Destination. It must contain the following:"
            url: "- (Required) The Slack webhook URL."
            username: "- (Optional) The username for basic authentication."
        importStatements: []
    obo_token Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "obo_token Resource - terraform-provider-databricks"'
        name: obo_token Resource - terraform-provider-databricks
        title: obo_token Resource - terraform-provider-databricks
        argumentDocs:
            application_id: "- Application ID of databricks_service_principal to create a PAT token for."
            comment: "- (String, Optional) Comment that describes the purpose of the token."
            id: "- Canonical unique identifier for the token."
            lifetime_seconds: "- (Integer, Optional) The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely."
            token_value: "- Sensitive value of the newly-created token."
        importStatements: []
    online_table Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "online_table Resource - terraform-provider-databricks"'
        name: online_table Resource - terraform-provider-databricks
        title: online_table Resource - terraform-provider-databricks
        argumentDocs:
            detailed_state: "- The state of the online table."
            id: "- The same as the name of the online table."
            message: "- A text description of the current state of the online table."
            name: "- (Required) 3-level name of the Online Table to create."
            perform_full_copy: '- (Optional) Whether to create a full-copy pipeline -- a pipeline that stops after creates a full copy of the source table upon initialization and does not process any change data feeds (CDFs) afterwards. The pipeline can still be manually triggered afterwards, but it always perform a full copy of the source table and there are no incremental updates. This mode is useful for syncing views or tables without CDFs to online tables. Note that the full-copy pipeline only supports "triggered" scheduling policy.'
            pipeline_id: "- ID of the associated Delta Live Table pipeline."
            primary_key_columns: "- (Required) list of the columns comprising the primary key."
            run_continuously: "- empty block that specifies that pipeline runs continuously after generating the initial data.  Conflicts with run_triggered."
            run_triggered: "- empty block that specifies that pipeline stops after generating the initial data and can be triggered later (manually, through a cron job or through data triggers)."
            source_table_full_name: "- (Required) full name of the source table."
            spec: "- (Required) object containing specification of the online table:"
            status: "- object describing status of the online table:"
            table_serving_url: "- Data serving REST API URL for this table."
            timeseries_key: "- (Optional) Time series key to deduplicate (tie-break) rows with the same primary key."
            unity_catalog_provisioning_state: '- The provisioning state of the online table entity in Unity Catalog. This is distinct from the state of the data synchronization pipeline (i.e. the table may be in "ACTIVE" but the pipeline may be in "PROVISIONING" as it runs asynchronously).'
        importStatements: []
    permission_assignment Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "permission_assignment Resource - terraform-provider-databricks"'
        name: permission_assignment Resource - terraform-provider-databricks
        title: permission_assignment Resource - terraform-provider-databricks
        argumentDocs:
            '"ADMIN"': "- Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more."
            '"USER"': "- Can access the workspace with basic privileges."
            id: "- ID of the permission assignment - same as principal_id."
            permissions: "- The list of workspace permissions to assign to the principal:"
            principal_id: "- Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the account-level SCIM API, or using databricks_user, databricks_service_principal or databricks_group data sources with account API (and has to be an account admin). A more sensible approach is to retrieve the list of principal_id as outputs from another Terraform stack."
        importStatements: []
    permissions Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "permissions Resource - terraform-provider-databricks"'
        name: permissions Resource - terraform-provider-databricks
        title: permissions Resource - terraform-provider-databricks
        argumentDocs:
            CAN_MANAGE: permission for items in the Workspace > Shared Icon Shared folder. You can grant CAN_MANAGE permission to notebooks and folders by moving them to the Shared Icon Shared folder.
            IS_OWNER: permission. Destroying databricks_permissions resource for a job would revert ownership to the creator.
            admins.group_name: "- (Optional) name of the group. We recommend setting permissions on groups."
            admins.permission_level: "- (Required) permission level according to specific resource. See examples above for the reference."
            admins.service_principal_name: "- (Optional) Application ID of the service_principal."
            admins.user_name: "- (Optional) name of the user."
            authorization: "- either tokens or passwords."
            cluster_id: "- cluster id"
            cluster_policy_id: "- cluster policy id"
            directory_id: "- directory id"
            directory_path: "- path of directory"
            experiment_id: "- MLflow experiment id"
            id: "- Canonical unique identifier for the permissions in form of /<object type>/<object id>."
            instance_pool_id: "- instance pool id"
            job_id: "- job id"
            notebook_id: "- ID of notebook within workspace"
            notebook_path: "- path of notebook"
            object_type: "- type of permissions."
            pipeline_id: "- pipeline id"
            registered_model_id: "- MLflow registered model id"
            repo_id: "- repo id"
            repo_path: "- path of databricks repo directory(/Repos/<username>/...)"
            serving_endpoint_id: "- Model Serving endpoint id."
            sql_alert_id: "- SQL alert id"
            sql_dashboard_id: "- SQL dashboard id"
            sql_endpoint_id: "- SQL warehouse id"
            sql_query_id: "- SQL query id"
        importStatements: []
    pipeline Resource - terraform-provider-databricks:
        subCategory: Compute
        description: '""page_title: "pipeline Resource - terraform-provider-databricks"'
        name: pipeline Resource - terraform-provider-databricks
        title: pipeline Resource - terraform-provider-databricks
        argumentDocs:
            alerts: (Required) non-empty list of alert types. Right now following alert types are supported, consult documentation for actual list
            allow_duplicate_names: "- Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is false."
            budget_policy_id: "- optional string specifying ID of the budget policy for this DLT pipeline."
            catalog: "- The name of catalog in Unity Catalog. Change of this parameter forces recreation of the pipeline. (Conflicts with storage)."
            channel: "- optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: CURRENT (default) and PREVIEW."
            cluster: blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. Please note that DLT pipeline clusters are supporting only subset of attributes as described in   Also, note that autoscale block is extended with the mode parameter that controls the autoscaling algorithm (possible values are ENHANCED for new, enhanced autoscaling algorithm, or LEGACY for old algorithm).
            configuration: "- An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs."
            connection_id: "- Immutable. The Unity Catalog connection this gateway pipeline uses to communicate with the source."
            continuous: "- A flag indicating whether to run the pipeline continuously. The default value is false."
            deployment: "- Deployment type of this pipeline. Supports following attributes:"
            development: "- A flag indicating whether to run the pipeline in development mode. The default value is false."
            edition: "- optional name of the product edition. Supported values are: CORE, PRO, ADVANCED (default).  Not required when serverless is set to true."
            email_recipients: (Required) non-empty list of emails to notify.
            exclude: "- Paths to exclude."
            filters: "- Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:"
            gateway_definition: "- The definition of a gateway pipeline to support CDC. Consists of following attributes:"
            gateway_storage_catalog: "- Required, Immutable. The name of the catalog for the gateway pipeline's storage location."
            gateway_storage_name: "- Required. The Unity Catalog-compatible naming for the gateway storage location. This is the destination to use for the data that is extracted by the gateway. Delta Live Tables system will automatically create the storage location under the catalog and schema."
            gateway_storage_schema: "- Required, Immutable. The name of the schema for the gateway pipelines's storage location."
            id: "- Canonical unique identifier of the DLT pipeline."
            include: "- Paths to include."
            kind: "- The deployment method that manages the pipeline."
            library: blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special notebook & file library types that should have the path attribute. Right now only the
            library.connection_name: "- Immutable. The Unity Catalog connection this ingestion pipeline uses to communicate with the source. Specify either ingestion_gateway_id or connection_name."
            library.ingestion_gateway_id: "- Immutable. Identifier for the ingestion gateway used by this ingestion pipeline to communicate with the source. Specify either ingestion_gateway_id or connection_name."
            library.objects: "- Required. Settings specifying tables to replicate and the destination for the replicated tables."
            library.table_configuration: "- Configuration settings to control the ingestion of tables. These settings are applied to all tables in the pipeline."
            metadata_file_path: "- The path to the file containing metadata about the deployment."
            name: "- A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI."
            notebook: "& file types are supported."
            on-flow-failure: "- a single data flow fails."
            on-update-failure: "- a pipeline update fails with a retryable error."
            on-update-fatal-failure: "- a pipeline update fails with a non-retryable (fatal) error."
            on-update-success: "- a pipeline update completes successfully."
            photon: "- A flag indicating whether to use Photon engine. The default value is false."
            schema: "- (Optional, String, Conflicts with target) The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode."
            serverless: "- An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires catalog to be set, as it could be used only with Unity Catalog."
            storage: "- A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. Change of this parameter forces recreation of the pipeline. (Conflicts with catalog)."
            target: "- (Optional, String, Conflicts with schema) The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI."
            url: "- URL of the DLT pipeline on the given workspace."
        importStatements: []
    provider Resource - terraform-provider-databricks:
        subCategory: Delta Sharing
        description: '""page_title: "provider Resource - terraform-provider-databricks"'
        name: provider Resource - terraform-provider-databricks
        title: provider Resource - terraform-provider-databricks
        argumentDocs:
            authentication_type: "- (Optional) The delta sharing authentication type. Valid values are TOKEN."
            comment: "- (Optional) Description about the provider."
            id: "- ID of this provider - same as the name."
            name: "- Name of provider. Change forces creation of a new resource."
            recipient_profile_str: "- (Optional) This is the json file that is created from a recipient url."
        importStatements: []
    quality_monitor Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "quality_monitor Resource - terraform-provider-databricks"'
        name: quality_monitor Resource - terraform-provider-databricks
        title: quality_monitor Resource - terraform-provider-databricks
        argumentDocs:
            assets_dir: "- (Required) - The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)"
            baseline_table_name: |-
                - Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
                table.
            custom_metrics: "- Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows)."
            dashboard_id: "- The ID of the generated dashboard."
            data_classification_config: "- The data classification config for the monitor"
            definition: "- create metric definition"
            drift_metrics_table_name: "- The full name of the drift metrics table. Format: catalog_name.schema_name.table_name."
            granularities: "-  List of granularities to use when aggregating data into time windows based on their timestamp."
            id: "-  ID of this monitor is the same as the full table name of the format {catalog}.{schema_name}.{table_name}"
            inference_log: "- Configuration for the inference log monitor"
            input_columns: "- Columns on the monitored table to apply the custom metrics to."
            label_col: "- Column of the model label"
            model_id_col: "- Column of the model id or version"
            monitor_version: "- The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted"
            name: "- Name of the custom metric."
            notifications: "- The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name email_addresses containing a list of emails to notify:"
            on_failure: "- who to send notifications to on monitor failure."
            on_new_classification_tag_detected: "- Who to send notifications to when new data classification tags are detected."
            output_data_type: "- The output type of the custom metric."
            output_schema_name: "- (Required) - Schema where output metric tables are created"
            prediction_col: "- Column of the model prediction"
            prediction_proba_col: "- Column of the model prediction probabilities"
            problem_type: "- Problem type the model aims to solve. Either PROBLEM_TYPE_CLASSIFICATION or PROBLEM_TYPE_REGRESSION"
            profile_metrics_table_name: "- The full name of the profile metrics table. Format: catalog_name.schema_name.table_name."
            quartz_cron_expression: "- string expression that determines when to run the monitor. See Quartz documentation for examples."
            schedule: "- The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:"
            skip_builtin_dashboard: "- Whether to skip creating a default dashboard summarizing data quality metrics."
            slicing_exprs: "- List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices."
            snapshot: "- Configuration for monitoring snapshot tables."
            status: "- Status of the Monitor"
            table_name: "- (Required) - The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}"
            time_series: "- Configuration for monitoring timeseries tables."
            timestamp_col: "- Column of the timestamp of predictions"
            timezone_id: "- string with timezone id (e.g., PST) in which to evaluate the Quartz expression."
            type: "- The type of the custom metric."
            warehouse_id: "- Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used."
        importStatements: []
    query Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "query Resource - terraform-provider-databricks"'
        name: query Resource - terraform-provider-databricks
        title: query Resource - terraform-provider-databricks
        argumentDocs:
            apply_auto_limit: "- (Optional, Boolean) Whether to apply a 1000 row limit to the query result."
            catalog: "- (Optional, String) Name of the catalog where this query will be executed."
            create_time: "- The timestamp string indicating when the query was created."
            databricks_permissions: .
            databricks_sql_query: ", for example, by executing the terraform state show databricks_sql_query.query command."
            date_range_value: "- (Block) Date-range query parameter value. Consists of following attributes (Can only specify one of dynamic_date_range_value or date_range_value):"
            date_value: "- (Block) Date query parameter value. Consists of following attributes (Can only specify one of dynamic_date_value or date_value):"
            description: "- (Optional, String) General description that conveys additional information about this query such as usage notes."
            display_name: "- (Required, String) Name of the query."
            dynamic_date_range_value: "- (String) Dynamic date-time range value based on current date-time.  Possible values are TODAY, YESTERDAY, THIS_WEEK, THIS_MONTH, THIS_YEAR, LAST_WEEK, LAST_MONTH, LAST_YEAR, LAST_HOUR, LAST_8_HOURS, LAST_24_HOURS, LAST_7_DAYS, LAST_14_DAYS, LAST_30_DAYS, LAST_60_DAYS, LAST_90_DAYS, LAST_12_MONTHS."
            dynamic_date_value: "- (String) Dynamic date-time value based on current date-time.  Possible values are NOW, YESTERDAY."
            end: (Required, String) - end of the date range.
            enum_options: "- (String) List of valid query parameter values, newline delimited."
            enum_value: "- (Block) Dropdown parameter value. Consists of following attributes:"
            id: "- unique ID of the created Query."
            last_modifier_user_name: "- Username of the user who last saved changes to this query."
            lifecycle_state: "- The workspace state of the query. Used for tracking trashed status. (Possible values are ACTIVE or TRASHED)."
            multi_values_options: "- (Optional, Block) If specified, allows multiple values to be selected for this parameter. Consists of following attributes:"
            name: "- (Required, String) Literal parameter marker that appears between double curly braces in the query text."
            numeric_value: "-  (Block) Numeric parameter value. Consists of following attributes:"
            owner_user_name: "- (Optional, String) Query owner's username."
            parameter: "- (Optional, Block) Query parameter definition.  Consists of following attributes (one of *_value is required):"
            parent: (if exists) is renamed to parent_path attribute, and should be converted from folders/object_id to the actual path.
            parent_path: "- (Optional, String) The path to a workspace folder containing the query. The default is the user's home folder.  If changed, the query will be recreated."
            precision: "- (Optional, String) Date-time precision to format the value into when the query is run.  Possible values are DAY_PRECISION, MINUTE_PRECISION, SECOND_PRECISION.  Defaults to DAY_PRECISION (YYYY-MM-DD)."
            prefix: "- (Optional, String) Character that prefixes each selected parameter value."
            query_backed_value: "- (Block) Query-based dropdown parameter value. Consists of following attributes:"
            query_id: "- (Required, String) ID of the query that provides the parameter values."
            query_text: "- (Required, String) Text of SQL query."
            run_as_mode: '- (Optional, String) Sets the "Run as" role for the object.'
            schema: "- (Optional, String) Name of the schema where this query will be executed."
            separator: "- (Optional, String) Character that separates each selected parameter value. Defaults to a comma."
            start: (Required, String) - begin of the date range.
            start_day_of_week: "- (Optional, Int) Specify what day that starts the week."
            suffix: "- (Optional, String) Character that suffixes each selected parameter value."
            tags: "- (Optional, List of strings) Tags that will be added to the query."
            terraform import databricks_query.query <query-id>: command.
            terraform import.databricks_permissions: .
            terraform import.import: "and removed blocks like this:"
            terraform import.terraform apply: command to apply changes.
            terraform import.terraform plan: command to check possible changes, such as value type change, etc.
            terraform plan: command to check possible changes, such as value type change, etc.
            terraform state rm databricks_sql_query.query: command.
            text_value: "- (Block) Text parameter value. Consists of following attributes:"
            title: "- (Optional, String) Text displayed in the user-facing parameter widget in the UI."
            update_time: "- The timestamp string indicating when the query was updated."
            value: "- (Required, String) - actual text value."
            values: "- (Array of strings) List of selected query parameter values."
            warehouse_id: "- (Required, String) ID of a SQL warehouse which will be used to execute this query."
        importStatements: []
    recipient Resource - terraform-provider-databricks:
        subCategory: Delta Sharing
        description: '""page_title: "recipient Resource - terraform-provider-databricks"'
        name: recipient Resource - terraform-provider-databricks
        title: recipient Resource - terraform-provider-databricks
        argumentDocs:
            activation_url: "- Full activation URL to retrieve the access token. It will be empty if the token is already retrieved."
            authentication_type: "- (Optional) The delta sharing authentication type. Valid values are TOKEN and DATABRICKS."
            cloud: "- Cloud vendor of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is DATABRICKS."
            comment: "- (Optional) Description about the recipient."
            created_at: "- Time at which this recipient Token was created, in epoch milliseconds."
            created_by: "- Username of recipient token creator."
            data_recipient_global_metastore_id: "- Required when authentication_type is DATABRICKS."
            expiration_time: "- Expiration timestamp of the token in epoch milliseconds."
            id: "- the ID of the recipient - the same as the name."
            ip_access_list: "- (Optional) Recipient IP access list."
            ip_access_list.allowed_ip_addresses: "- Allowed IP Addresses in CIDR notation. Limit of 100."
            metastore_id: "- Unique identifier of recipient's Unity Catalog metastore. This field is only present when the authentication_type is DATABRICKS."
            name: "- Name of recipient. Change forces creation of a new resource."
            owner: "- (Optional) Username/groupname/sp application_id of the recipient owner."
            properties: (Required) a map of string key-value pairs with recipient's properties.  Properties with name starting with databricks. are reserved.
            properties_kvpairs: "- (Optional) Recipient properties - object consisting of following fields:"
            region: "- Cloud region of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is DATABRICKS."
            sharing_code: "- (Optional) The one-time sharing code provided by the data recipient."
            tokens: "- List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:"
            updated_at: "- Time at which this recipient Token was updated, in epoch milliseconds."
            updated_by: "- Username of recipient Token updater."
        importStatements: []
    registered_model Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "registered_model Resource - terraform-provider-databricks"'
        name: registered_model Resource - terraform-provider-databricks
        title: registered_model Resource - terraform-provider-databricks
        argumentDocs:
            ALL_PRIVILEGES: ", APPLY_TAG, and EXECUTE privileges."
            catalog_name: "- (Required) The name of the catalog where the schema and the registered model reside. Change of this parameter forces recreation of the resource."
            comment: "- (Optional) The comment attached to the registered model."
            id: "- Equal to the full name of the model (catalog_name.schema_name.name) and used to identify the model uniquely across the metastore."
            name: "- (Required) The name of the registered model.  Change of this parameter forces recreation of the resource."
            owner: "- (Optional) Name of the registered model owner."
            schema_name: "- (Required) The name of the schema where the registered model resides. Change of this parameter forces recreation of the resource."
            storage_location: "- (Optional) The storage location under which model version data files are stored. Change of this parameter forces recreation of the resource."
        importStatements: []
    repo Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "repo Resource - terraform-provider-databricks"'
        name: repo Resource - terraform-provider-databricks
        title: repo Resource - terraform-provider-databricks
        argumentDocs:
            branch: "- (Optional) name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with tag.  If branch is removed, and tag isn't specified, then the repository will stay at the previously checked out state."
            commit_hash: "- Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API"
            git_provider: "- (Optional, if it's possible to detect Git provider by host name) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult Repos API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition, awsCodeCommit."
            id: "-  Git folder identifier"
            path: "- (Optional) path to put the checked out Git folder. If not specified, , then the Git folder will be created in the default location.  If the value changes, Git folder is re-created."
            sparse_checkout.patterns: "- array of paths (directories) that will be used for sparse checkout.  List of patterns could be updated in-place."
            tag: "- (Optional) name of the tag for initial checkout.  Conflicts with branch."
            url: "-  (Required) The URL of the Git Repository to clone from. If the value changes, Git folder is re-created."
            workspace_path: "- path on Workspace File System (WSFS) in form of /Workspace + path"
        importStatements: []
    restrict_workspace_admins_setting Resource - terraform-provider-databricks:
        subCategory: Settings
        description: '""page_title: "restrict_workspace_admins_setting Resource - terraform-provider-databricks"'
        name: restrict_workspace_admins_setting Resource - terraform-provider-databricks
        title: restrict_workspace_admins_setting Resource - terraform-provider-databricks
        argumentDocs:
            restrict_workspace_admins: "- (Required) The configuration details."
            status: "- (Required) The restrict workspace admins status for the workspace."
        importStatements: []
    schema Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "schema Resource - terraform-provider-databricks"'
        name: schema Resource - terraform-provider-databricks
        title: schema Resource - terraform-provider-databricks
        argumentDocs:
            catalog_name: "- Name of parent catalog. Change forces creation of a new resource."
            comment: "- (Optional) User-supplied free-form text."
            enable_predictive_optimization: "- (Optional) Whether predictive optimization should be enabled for this object and objects under it. Can be ENABLE, DISABLE or INHERIT"
            force_destroy: "- (Optional) Delete schema regardless of its contents."
            id: "- ID of this schema in form of <catalog_name>.<name>."
            name: "- Name of Schema relative to parent catalog. Change forces creation of a new resource."
            owner: "- (Optional) Username/groupname/sp application_id of the schema owner."
            properties: "- (Optional) Extensible Schema properties."
            storage_root: "- (Optional) Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource."
        importStatements: []
    secret Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "secret Resource - terraform-provider-databricks"'
        name: secret Resource - terraform-provider-databricks
        title: secret Resource - terraform-provider-databricks
        argumentDocs:
            config_reference: "- (String) value to use as a secret reference in Spark configuration and environment variables: {{secrets/scope/key}}."
            id: "- Canonical unique identifier for the secret."
            key: "- (Required) (String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters."
            last_updated_timestamp: "- (Integer) time secret was updated"
            scope: "- (Required) (String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters."
            string_value: "- (Required) (String) super secret sensitive value."
        importStatements: []
    secret_acl Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "secret_acl Resource - terraform-provider-databricks"'
        name: secret_acl Resource - terraform-provider-databricks
        title: secret_acl Resource - terraform-provider-databricks
        argumentDocs:
            application_id: attribute of databricks_service_principal.
            display_name: attribute of databricks_group.  Use users to allow access for all workspace users.
            permission: "- (Required) READ, WRITE or MANAGE."
            principal: "- (Required) principal's identifier. It can be:"
            scope: "- (Required) name of the scope"
            user_name: attribute of databricks_user.
        importStatements: []
    secret_scope Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "secret_scope Resource - terraform-provider-databricks"'
        name: secret_scope Resource - terraform-provider-databricks
        title: secret_scope Resource - terraform-provider-databricks
        argumentDocs:
            backend_type: "- Either DATABRICKS or AZURE_KEYVAULT"
            id: "- The id for the secret scope object."
            initial_manage_principal: "- (Optional) The principal with the only possible value users that is initially granted MANAGE permission to the created scope.  If it's omitted, then the databricks_secret_acl with MANAGE permission applied to the scope is assigned to the API request issuer's user identity (see documentation). This part of the state cannot be imported."
            name: "- (Required) Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters."
        importStatements: []
    service_principal Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "service_principal Resource - terraform-provider-databricks"'
        name: service_principal Resource - terraform-provider-databricks
        title: service_principal Resource - terraform-provider-databricks
        argumentDocs:
            acl_principal_id: "- identifier for use in databricks_access_control_rule_set, e.g. servicePrincipals/00000000-0000-0000-0000-000000000000."
            active: "- (Optional) Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets."
            allow_cluster_create: "- (Optional) Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy."
            allow_instance_pool_create: "- (Optional) Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument."
            application_id: This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.
            databricks_sql_access: "- (Optional) This is a field to allow the group to have access to Databricks SQL feature through databricks_sql_endpoint."
            disable_as_user_deletion: "- (Optional) Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to true when the provider is configured at the account-level and false when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags."
            display_name: "- (Required for Databricks-managed service principals) This is an alias for the service principal and can be the full name of the service principal."
            external_id: "- (Optional) ID of the service principal in an external identity provider."
            force: "- (Optional) Ignore cannot create service principal: Service principal with application ID X already exists errors and implicitly import the specified service principal into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation."
            force_delete_home_dir: "- (Optional) This flag determines whether the service principal's home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default."
            force_delete_repos: "- (Optional) This flag determines whether the service principal's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default."
            home: "- Home folder of the service principal, e.g. /Users/00000000-0000-0000-0000-000000000000."
            id: "- Canonical unique identifier for the service principal."
            repos: "- Personal Repos location of the service principal, e.g. /Repos/00000000-0000-0000-0000-000000000000."
            workspace_access: "- (Optional) This is a field to allow the group to have access to Databricks Workspace."
        importStatements: []
    service_principal_role Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "service_principal_role Resource - terraform-provider-databricks"'
        name: service_principal_role Resource - terraform-provider-databricks
        title: service_principal_role Resource - terraform-provider-databricks
        argumentDocs:
            id: "- The id in the format <service_principal_id>|<role>."
            role: "-  (Required) This is the id of the role or instance profile resource."
            service_principal_id: "- (Required) This is the id of the service principal resource."
        importStatements: []
    service_principal_secret Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "service_principal_secret Resource - terraform-provider-databricks"'
        name: service_principal_secret Resource - terraform-provider-databricks
        title: service_principal_secret Resource - terraform-provider-databricks
        argumentDocs:
            id: "- ID of the secret"
            secret: "- Generated secret for the service principal"
            service_principal_id: "- ID of the databricks_service_principal (not application ID)."
        importStatements: []
    share Resource - terraform-provider-databricks:
        subCategory: Delta Sharing
        description: '""page_title: "share Resource - terraform-provider-databricks"'
        name: share Resource - terraform-provider-databricks
        title: share Resource - terraform-provider-databricks
        argumentDocs:
            cdf_enabled: (Optional) - Whether to enable Change Data Feed (cdf) on the shared object. When this field is set, field history_data_sharing_status can not be set.
            comment: (Optional) -  Description about the object.
            created_at: "- Time when the share was created."
            created_by: "- The principal that created the share."
            data_object_type: (Required) - Type of the data object, currently TABLE, SCHEMA, VOLUME, and MODEL are supported.
            history_data_sharing_status: "(Optional) - Whether to enable history sharing, one of: ENABLED, DISABLED. When a table has history sharing enabled, recipients can query table data by version, starting from the current table version. If not specified, clients can only query starting from the version of the object at the time it was added to the share. NOTE: The start_version should be less than or equal the current version of the object. When this field is set, field cdf_enabled can not be set."
            id: "- the ID of the share, the same as name."
            name: (Required) - Name of share. Change forces creation of a new resource.
            op: "- The operator to apply for the value, one of: EQUAL, LIKE"
            owner: (Optional) -  User name/group name/sp application_id of the share owner.
            recipient_property_key: (Optional) - The key of a Delta Sharing recipient's property. For example databricks-account-id. When this field is set, field value can not be set.
            shared_as: (Optional) - A user-provided new name for the data object within the share. If this new name is not provided, the object's original name will be used as the shared_as name. The shared_as name must be unique within a Share. Change forces creation of a new resource.
            start_version: (Optional) -  The start version associated with the object for cdf. This allows data providers to control the lowest object version that is accessible by clients.
            status: "- Status of the object, one of: ACTIVE, PERMISSION_DENIED."
            value: (Optional) - The value of the partition column. When this value is not set, it means null value. When this field is set, field recipient_property_key can not be set.
        importStatements: []
    sql_alert Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "sql_alert Resource - terraform-provider-databricks"'
        name: sql_alert Resource - terraform-provider-databricks
        title: sql_alert Resource - terraform-provider-databricks
        argumentDocs:
            column: "- (Required, String) Name of column in the query result to compare in alert evaluation."
            custom_body: "- (Optional, String) Custom body of alert notification, if it exists. See Alerts API reference for custom templating instructions."
            custom_subject: "- (Optional, String) Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See Alerts API reference for custom templating instructions."
            empty_result_state: "- (Optional, String) State that alert evaluates to when query result is empty.  Currently supported values are unknown, triggered, ok - check API documentation for full list of supported values."
            id: "- unique ID of the SQL Alert."
            muted: "- (Optional, bool) Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered."
            name: "- (Required, String) Name of the alert."
            op: "- (Required, String Enum) Operator used to compare in alert evaluation. (Enum: >, >=, <, <=, ==, !=)"
            options: "- (Required) Alert configuration options."
            parent: "- (Optional, String) The identifier of the workspace folder containing the alert. The default is ther user's home folder. The folder identifier is formatted as folder/<folder_id>."
            query_id: "- (Required, String) ID of the query evaluated by the alert."
            rearm: "- (Optional, Integer) Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again."
            value: "- (Required, String) Value used to compare in alert evaluation."
        importStatements: []
    sql_dashboard Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "sql_dashboard Resource - terraform-provider-databricks"'
        name: sql_dashboard Resource - terraform-provider-databricks
        title: sql_dashboard Resource - terraform-provider-databricks
        argumentDocs:
            id: "- the unique ID of the SQL Dashboard."
        importStatements: []
    sql_endpoint Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "sql_endpoint Resource - terraform-provider-databricks"'
        name: sql_endpoint Resource - terraform-provider-databricks
        title: sql_endpoint Resource - terraform-provider-databricks
        argumentDocs:
            auto_stop_mins: "- Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop."
            channel: "block, consisting of following fields:"
            channel.name: "- Name of the Databricks SQL release channel. Possible values are: CHANNEL_NAME_PREVIEW and CHANNEL_NAME_CURRENT. Default is CHANNEL_NAME_CURRENT."
            cluster_size: '- (Required) The size of the clusters allocated to the endpoint: "2X-Small", "X-Small", "Small", "Medium", "Large", "X-Large", "2X-Large", "3X-Large", "4X-Large".'
            creator_name: "- The username of the user who created the endpoint."
            data_source_id: "- ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint."
            databricks_sql_access: on databricks_group or databricks_user.
            enable_photon: "- Whether to enable Photon. This field is optional and is enabled by default."
            enable_serverless_compute: "- Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly."
            "false": for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to true if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated terms of use, workspace admins are prompted in the Databricks SQL UI. A workspace must meet the requirements and might require an update to its instance profile role to add a trust relationship.
            health: "- Health status of the endpoint."
            id: "- the unique ID of the SQL warehouse."
            jdbc_url: "- JDBC connection string."
            max_num_clusters: "- Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to 1."
            min_num_clusters: "- Minimum number of clusters available when a SQL warehouse is running. The default is 1."
            name: "- (Required) Name of the SQL warehouse. Must be unique."
            num_active_sessions: "- The current number of clusters used by the endpoint."
            num_clusters: "- The current number of clusters used by the endpoint."
            odbc_params: "- ODBC connection params: odbc_params.hostname, odbc_params.path, odbc_params.protocol, and odbc_params.port."
            spot_instance_policy: "- The spot policy to use for allocating instances to clusters: COST_OPTIMIZED or RELIABILITY_OPTIMIZED. This field is optional. Default is COST_OPTIMIZED."
            state: "- The current state of the endpoint."
            tags: "- Databricks tags all endpoint resources with these tags."
            warehouse_type: "- SQL warehouse type. See for AWS or Azure. Set to PRO or CLASSIC. If the field enable_serverless_compute has the value true either explicitly or through the default logic (see that field above for details), the default is PRO, which is required for serverless SQL warehouses. Otherwise, the default is CLASSIC."
        importStatements: []
    sql_global_config Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "sql_global_config Resource - terraform-provider-databricks"'
        name: sql_global_config Resource - terraform-provider-databricks
        title: sql_global_config Resource - terraform-provider-databricks
        argumentDocs:
            data_access_config: (Optional, Map) - Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the documentation for a full list.  Apply will fail if you're specifying not permitted configuration.
            google_service_account: (Optional, String) - used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.
            instance_profile_arn: (Optional, String) - databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.
            security_policy: "(Optional, String) - The policy for controlling access to datasets. Default value: DATA_ACCESS_CONTROL, consult documentation for list of possible values"
            sql_config_params: (Optional, Map) - SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.
        importStatements: []
    sql_permissions Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "sql_permissions Resource - terraform-provider-databricks"'
        name: sql_permissions Resource - terraform-provider-databricks
        title: sql_permissions Resource - terraform-provider-databricks
        argumentDocs:
            anonymous function/: "- anonymous function. / suffix is mandatory."
            anonymous_function: "- (Boolean) If this access control for using anonymous function. Defaults to false."
            any file/: "- direct access to any file. / suffix is mandatory."
            any_file: "- (Boolean) If this access control for reading/writing any file. Defaults to false."
            catalog: "- (Boolean) If this access control for the entire catalog. Defaults to false."
            catalog/: "- entire catalog. / suffix is mandatory."
            cluster_id: "- (Optional) Id of an existing databricks_cluster, where the appropriate GRANT/REVOKE commands are executed. This cluster must have the appropriate data security mode (USER_ISOLATION or LEGACY_TABLE_ACL specified). If no cluster_id is specified, a single-node TACL cluster named terraform-table-acl is automatically created."
            database: "- Name of the database. Has default value of default."
            database/bar: "- bar database."
            privilege_assignments.CREATE: "- gives the ability to create an object (for example, a table in a database)."
            privilege_assignments.CREATE_NAMED_FUNCTION: "- gives the ability to create a named UDF in an existing catalog or database."
            privilege_assignments.MODIFY: "- gives the ability to add, delete, and modify data to or from an object."
            privilege_assignments.MODIFY_CLASSPATH: "- gives the ability to add files to the Spark class path."
            privilege_assignments.READ_METADATA: "- gives the ability to view an object and its metadata."
            privilege_assignments.SELECT: "- gives read access to an object."
            privilege_assignments.USAGE: "- do not give any abilities, but is an additional requirement to perform any action on a database object."
            privilege_assignments.principal: "- display_name for a databricks_group or databricks_user, application_id for a databricks_service_principal."
            privilege_assignments.privileges: "- set of available privilege names in upper case."
            table: "- Name of the table. Can be combined with database."
            table/default.foo: "- table foo in a default database. Database is always mandatory."
            view: "- Name of the view. Can be combined with database."
            view/bar.foo: "- view foo in bar database."
        importStatements: []
    sql_query Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "sql_query Resource - terraform-provider-databricks"'
        name: sql_query Resource - terraform-provider-databricks
        title: sql_query Resource - terraform-provider-databricks
        argumentDocs:
            data_source_id: "- Data source ID of a SQL warehouse"
            description: "- General description that conveys additional information about this query such as usage notes."
            id: "- the unique ID of the SQL Query."
            name: "- The title of this query that appears in list views, widget headings, and on the query page."
            parent: "- The identifier of the workspace folder containing the object."
            query: "- The text of the query to be run."
            run_as_role: "- Run as role. Possible values are viewer, owner."
            text.value: "- The default value for this parameter."
            title: "- The text displayed in a parameter picking widget."
        importStatements: []
    sql_table Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "sql_table Resource - terraform-provider-databricks"'
        name: sql_table Resource - terraform-provider-databricks
        title: sql_table Resource - terraform-provider-databricks
        argumentDocs:
            catalog_name: "- Name of parent catalog. Change forces creation of a new resource."
            cluster_id: "- (Optional) All table CRUD operations must be executed on a running cluster or SQL warehouse. If a cluster_id is specified, it will be used to execute SQL commands to manage this table. If empty, a cluster will be created automatically with the name terraform-sql-table."
            cluster_keys: "- (Optional) a subset of columns to liquid cluster the table by. Conflicts with partitions."
            comment: "- (Optional) User-supplied free-form text. Changing comment is not currently supported on VIEW table_type."
            data_source_format: "- (Optional) External tables are supported in multiple data source formats. The string constants identifying these formats are DELTA, CSV, JSON, AVRO, PARQUET, ORC, TEXT. Change forces creation of a new resource. Not supported for MANAGED tables or VIEW."
            id: "- ID of this table in form of <catalog_name>.<schema_name>.<name>."
            identity: "- (Optional) Whether field is an identity column. Can be default, always or unset. It is unset by default."
            name: "- Name of table relative to parent catalog and schema. Change forces creation of a new resource."
            nullable: "- (Optional) Whether field is nullable (Default: true)"
            options: "- (Optional) Map of user defined table options. Change forces creation of a new resource."
            owner: "- (Optional) Username/groupname/sp application_id of the schema owner."
            partitions: "- (Optional) a subset of columns to partition the table by. Change forces creation of a new resource. Conflicts with cluster_keys. Change forces creation of a new resource."
            properties: "- (Optional) Map of table properties."
            schema_name: "- Name of parent Schema relative to parent Catalog. Change forces creation of a new resource."
            storage_credential_name: "- (Optional) For EXTERNAL Tables only: the name of storage credential to use. Change forces creation of a new resource."
            storage_location: "- (Optional) URL of storage location for Table data (required for EXTERNAL Tables). Not supported for VIEW or MANAGED table_type."
            table_type: "- Distinguishes a view vs. managed/external Table. MANAGED, EXTERNAL or VIEW. Change forces creation of a new resource."
            type: "- Column type spec (with metadata) as SQL text. Not supported for VIEW table_type."
            view_definition: '- (Optional) SQL text defining the view (for table_type == "VIEW"). Not supported for MANAGED or EXTERNAL table_type.'
            warehouse_id: "- (Optional) All table CRUD operations must be executed on a running cluster or SQL warehouse. If a warehouse_id is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with cluster_id."
        importStatements: []
    sql_visualization Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "sql_visualization Resource - terraform-provider-databricks"'
        name: sql_visualization Resource - terraform-provider-databricks
        title: sql_visualization Resource - terraform-provider-databricks
        argumentDocs: {}
        importStatements: []
    sql_widget Resource - terraform-provider-databricks:
        subCategory: Databricks SQL
        description: '""page_title: "sql_widget Resource - terraform-provider-databricks"'
        name: sql_widget Resource - terraform-provider-databricks
        title: sql_widget Resource - terraform-provider-databricks
        argumentDocs: {}
        importStatements: []
    storage_credential Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "storage_credential Resource - terraform-provider-databricks"'
        name: storage_credential Resource - terraform-provider-databricks
        title: storage_credential Resource - terraform-provider-databricks
        argumentDocs:
            aws_iam_role.role_arn: "- The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF"
            azure_managed_identity.access_connector_id: "- The Resource ID of the Azure Databricks Access Connector resource, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name."
            azure_managed_identity.managed_identity_id: "- (Optional) The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name."
            azure_service_principal.application_id: "- The application ID of the application registration within the referenced AAD tenant"
            azure_service_principal.client_secret: "- The client secret generated for the above app ID in AAD. This field is redacted on output"
            azure_service_principal.directory_id: "- The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application"
            cloudflare_api_token.access_key_id: "- R2 API token access key ID"
            cloudflare_api_token.account_id: "- R2 account ID"
            cloudflare_api_token.secret_access_key: "- R2 API token secret access key"
            databricks_gcp_service_account.email: (output only) - The email of the GCP service account created, to be granted access to relevant buckets.
            databricks_storage_credential: represents authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal/managed identity for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.
            force_destroy: "- (Optional) Delete storage credential regardless of its dependencies."
            force_update: "- (Optional) Update storage credential regardless of its dependents."
            id: "- ID of this storage credential - same as the name."
            isolation_mode: "- (Optional) Whether the storage credential is accessible from all workspaces or a specific set of workspaces. Can be ISOLATION_MODE_ISOLATED or ISOLATION_MODE_OPEN. Setting the credential to ISOLATION_MODE_ISOLATED will automatically allow access from the current workspace."
            metastore_id: "- (Required for account-level) Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required."
            name: "- Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource."
            owner: "- (Optional) Username/groupname/sp application_id of the storage credential owner."
            read_only: "- (Optional) Indicates whether the storage credential is only usable for read operations."
            skip_validation: "- (Optional) Suppress validation errors if any & force save the storage credential."
            storage_credential_id: "- Unique ID of storage credential."
        importStatements: []
    system_schema Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "system_schema Resource - terraform-provider-databricks"'
        name: system_schema Resource - terraform-provider-databricks
        title: system_schema Resource - terraform-provider-databricks
        argumentDocs:
            full_name: "- the full name of the system schema, in form of system.<schema>."
            id: "- the ID of system schema in form of metastore_id|schema_name."
            schema: "- (Required) name of the system schema."
            state: "- The current state of enablement for the system schema."
        importStatements: []
    token Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "token Resource - terraform-provider-databricks"'
        name: token Resource - terraform-provider-databricks
        title: token Resource - terraform-provider-databricks
        argumentDocs:
            comment: "- (Optional) (String) Comment that will appear on the user’s settings page for this token."
            id: "- Canonical unique identifier for the token."
            lifetime_seconds: "- (Optional) (Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely."
            token_value: "- Sensitive value of the newly-created token."
        importStatements: []
    user Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "user Resource - terraform-provider-databricks"'
        name: user Resource - terraform-provider-databricks
        title: user Resource - terraform-provider-databricks
        argumentDocs:
            acl_principal_id: "- identifier for use in databricks_access_control_rule_set, e.g. users/mr.foo@example.com."
            active: "- (Optional) Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets."
            allow_cluster_create: "-  (Optional) Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy."
            allow_instance_pool_create: "-  (Optional) Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument."
            databricks_sql_access: "- (Optional) This is a field to allow the group to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint."
            disable_as_user_deletion: "- (Optional) Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to true when the provider is configured at the account-level and false when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags."
            display_name: "- (Optional) This is an alias for the username that can be the full name of the user."
            external_id: "- (Optional) ID of the user in an external identity provider."
            force: "- (Optional) Ignore cannot create user: User with username X already exists errors and implicitly import the specific user into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation."
            force_delete_home_dir: "- (Optional) This flag determines whether the user's home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default."
            force_delete_repos: "- (Optional) This flag determines whether the user's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default."
            home: "- Home folder of the user, e.g. /Users/mr.foo@example.com."
            id: "- Canonical unique identifier for the user."
            repos: "- Personal Repos location of the user, e.g. /Repos/mr.foo@example.com."
            user_name: "- (Required) This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters."
        importStatements: []
    user_instance_profile Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "user_instance_profile Resource - terraform-provider-databricks"'
        name: user_instance_profile Resource - terraform-provider-databricks
        title: user_instance_profile Resource - terraform-provider-databricks
        argumentDocs:
            id: "- The id in the format <user_id>|<instance_profile_id>."
            instance_profile_id: "-  (Required) This is the id of the instance profile resource."
            user_id: "- (Required) This is the id of the user resource."
        importStatements: []
    user_role Resource - terraform-provider-databricks:
        subCategory: Security
        description: '""page_title: "user_role Resource - terraform-provider-databricks"'
        name: user_role Resource - terraform-provider-databricks
        title: user_role Resource - terraform-provider-databricks
        argumentDocs:
            id: "- The id in the format <user_id>|<role>."
            role: "-  (Required) Either a role name or the ARN/ID of the instance profile resource."
            user_id: "- (Required) This is the id of the user resource."
        importStatements: []
    vector_search_endpoint Resource - terraform-provider-databricks:
        subCategory: Mosaic AI Vector Search
        description: '""page_title: "vector_search_endpoint Resource - terraform-provider-databricks"'
        name: vector_search_endpoint Resource - terraform-provider-databricks
        title: vector_search_endpoint Resource - terraform-provider-databricks
        argumentDocs:
            creation_timestamp: "- Timestamp of endpoint creation (milliseconds)."
            creator: "- Creator of the endpoint."
            endpoint_id: "- Unique internal identifier of the endpoint (UUID)."
            endpoint_status: "- Object describing the current status of the endpoint consisting of the following fields:"
            endpoint_type: "(Required) Type of Mosaic AI Vector Search Endpoint.  Currently only accepting single value: STANDARD (See documentation for the list of currently supported values)."
            id: "- The same as the name of the endpoint."
            last_updated_timestamp: "- Timestamp of the last update to the endpoint (milliseconds)."
            last_updated_user: "- User who last updated the endpoint."
            message: "- Additional status message."
            name: "- (Required) Name of the Mosaic AI Vector Search Endpoint to create."
            num_indexes: "- Number of indexes on the endpoint."
            state: "- Current state of the endpoint. Currently following values are supported: PROVISIONING, ONLINE, and OFFLINE."
        importStatements: []
    vector_search_index Resource - terraform-provider-databricks:
        subCategory: Mosaic AI Vector Search
        description: '""page_title: "vector_search_index Resource - terraform-provider-databricks"'
        name: vector_search_index Resource - terraform-provider-databricks
        title: vector_search_index Resource - terraform-provider-databricks
        argumentDocs:
            CONTINUOUS: ": If the pipeline uses continuous execution, the pipeline processes new data as it arrives in the source table to keep the vector index fresh."
            DELTA_SYNC: ": An index that automatically syncs with a source Delta Table, automatically and incrementally updating the index as the underlying data in the Delta Table changes."
            DIRECT_ACCESS: ": An index that supports the direct read and write of vectors and metadata through our REST and SDK APIs. With this model, the user manages index updates."
            TRIGGERED: ": If the pipeline uses the triggered execution mode, the system stops processing after successfully refreshing the source table in the pipeline once, ensuring the table is updated based on the data available when the update started."
            columns_to_sync: "- (optional) list of columns to sync. If not specified, all columns are syncronized."
            creator: "- Creator of the endpoint."
            delta_sync_index_spec: "- (object) Specification for Delta Sync Index. Required if index_type is DELTA_SYNC."
            direct_access_index_spec: "- (object) Specification for Direct Vector Access Index. Required if index_type is DIRECT_ACCESS."
            embedding_dimension: "- Dimension of the embedding vector."
            embedding_model_endpoint_name: "- The name of the embedding model endpoint"
            embedding_source_columns: "- (required if embedding_vector_columns isn't provided) array of objects representing columns that contain the embedding source.  Each entry consists of:"
            embedding_vector_columns: "- (required if embedding_source_columns isn't provided)  array of objects representing columns that contain the embedding vectors. Each entry consists of:"
            endpoint_name: "- (required) The name of the Mosaic AI Vector Search Endpoint that will be used for indexing the data."
            id: "- The same as the name of the index."
            index_type: "- (required) Mosaic AI Vector Search index type. Currently supported values are:"
            index_url: "- Index API Url to be used to perform operations on the index"
            indexed_row_count: "- Number of rows indexed"
            message: "- Message associated with the index status"
            name: "- (required) Three-level name of the Mosaic AI Vector Search Index to create (catalog.schema.index_name)."
            pipeline_id: "- ID of the associated Delta Live Table pipeline."
            pipeline_type: "- Pipeline execution mode. Possible values are:"
            primary_key: "- (required) The column name that will be used as a primary key."
            ready: "- Whether the index is ready for search"
            schema_json: "- The schema of the index in JSON format.  Check the API documentation for a list of supported data types."
            source_table: (required) The name of the source table.
            status: "- Object describing the current status of the index consisting of the following fields:"
        importStatements: []
    volume Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "volume Resource - terraform-provider-databricks"'
        name: volume Resource - terraform-provider-databricks
        title: volume Resource - terraform-provider-databricks
        argumentDocs:
            <catalogName>: ": The name of the catalog containing the Volume."
            <schemaName>: ": The name of the schema containing the Volume."
            <volumeName>: ": The name of the Volume. It identifies the volume object."
            catalog_name: "- Name of parent Catalog. Change forces creation of a new resource."
            comment: "- (Optional) Free-form text."
            id: "- ID of this Unity Catalog Volume in form of <catalog>.<schema>.<name>."
            name: "- Name of the Volume"
            owner: "- (Optional) Name of the volume owner."
            schema_name: "- Name of parent Schema relative to parent Catalog. Change forces creation of a new resource."
            storage_location: "- (Optional) Path inside an External Location. Only used for EXTERNAL Volumes. Change forces creation of a new resource."
            volume_path: "- base file path for this Unity Catalog Volume in form of /Volumes/<catalog>/<schema>/<name>."
            volume_type: "- Volume type. EXTERNAL or MANAGED. Change forces creation of a new resource."
        importStatements: []
    workspace_binding Resource - terraform-provider-databricks:
        subCategory: Unity Catalog
        description: '""page_title: "workspace_binding Resource - terraform-provider-databricks"'
        name: workspace_binding Resource - terraform-provider-databricks
        title: workspace_binding Resource - terraform-provider-databricks
        argumentDocs:
            binding_type: "- (Optional) Binding mode. Default to BINDING_TYPE_READ_WRITE. Possible values are BINDING_TYPE_READ_ONLY, BINDING_TYPE_READ_WRITE."
            securable_name: "- Name of securable. Change forces creation of a new resource."
            securable_type: "- Type of securable. Can be catalog, external-location or storage-credential. Default to catalog. Change forces creation of a new resource."
            workspace_id: "- ID of the workspace. Change forces creation of a new resource."
        importStatements: []
    workspace_conf Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "workspace_conf Resource - terraform-provider-databricks"'
        name: workspace_conf Resource - terraform-provider-databricks
        title: workspace_conf Resource - terraform-provider-databricks
        argumentDocs:
            custom_config: "- (Required) Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with enable or enforce will be reset to false value, regardless of initial default one."
            enableDeprecatedClusterNamedInitScripts: "- (boolean) Enable or disable legacy cluster-named init scripts for this workspace."
            enableDeprecatedGlobalInitScripts: "- (boolean) Enable or disable legacy global init scripts for this workspace."
            enableIpAccessLists: "- enables the use of databricks_ip_access_list resources"
            enableTokensConfig: "- (boolean) Enable or disable personal access tokens for this workspace."
            maxTokenLifetimeDays: "- (string) Maximum token lifetime of new tokens in days, as an integer. If zero, new tokens are permitted to have no lifetime limit. Negative numbers are unsupported. WARNING: This limit only applies to new tokens, so there may be tokens with lifetimes longer than this value, including unlimited lifetime. Such tokens may have been created before the current maximum token lifetime was set."
        importStatements: []
    workspace_file Resource - terraform-provider-databricks:
        subCategory: Workspace
        description: '""page_title: "workspace_file Resource - terraform-provider-databricks"'
        name: workspace_file Resource - terraform-provider-databricks
        title: workspace_file Resource - terraform-provider-databricks
        argumentDocs:
            content_base64: "- The base64-encoded file content. Conflicts with source. Use of content_base64 is discouraged, as it's increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a workspace file with configuration properties for a data pipeline."
            id: "-  Path of workspace file"
            object_id: "-  Unique identifier for a workspace file"
            path: '-  (Required) The absolute path of the workspace file, beginning with "/", e.g. "/Demo".'
            source: "- Path to file on local filesystem. Conflicts with content_base64."
            url: "- Routable URL of the workspace file"
            workspace_path: "- path on Workspace File System (WSFS) in form of /Workspace + path"
        importStatements: []
