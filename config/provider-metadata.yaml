name: databricks/databricks
resources:
    databricks_access_control_rule_set:
        subCategory: ""
        name: databricks_access_control_rule_set
        title: databricks_access_control_rule_set Resource
        examples:
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              references:
                grant_rules.principals: data.databricks_group.ds.acl_principal_id
              dependencies:
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              references:
                grant_rules.principals: databricks_group.ds.acl_principal_id
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              references:
                grant_rules.principals: databricks_group.ds.acl_principal_id
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "application_id": "00000000-0000-0000-0000-000000000000",
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              references:
                grant_rules.principals: databricks_group.ds.acl_principal_id
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: ds_group_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_user.john.acl_principal_id}"
                      ],
                      "role": "roles/group.manager"
                    }
                  ],
                  "name": "accounts/${local.account_id}/groups/${databricks_group.ds.id}/ruleSets/default"
                }
              references:
                grant_rules.principals: data.databricks_user.john.acl_principal_id
            - name: account_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_user.john.acl_principal_id}"
                      ],
                      "role": "roles/group.manager"
                    },
                    {
                      "principals": [
                        "${data.databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.manager"
                    },
                    {
                      "principals": [
                        "${data.databricks_group.marketplace_admins.acl_principal_id}"
                      ],
                      "role": "roles/marketplace.admin"
                    }
                  ],
                  "name": "accounts/${local.account_id}/ruleSets/default"
                }
              references:
                grant_rules.principals: data.databricks_group.marketplace_admins.acl_principal_id
            - name: budget_policy_usage
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_user.john.acl_principal_id}"
                      ],
                      "role": "roles/budgetPolicy.manager"
                    },
                    {
                      "principals": [
                        "${data.databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/budgetPolicy.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/budgetPolicies/${databricks_budget_policy.this.policy_id}/ruleSets/default"
                }
              references:
                grant_rules.principals: data.databricks_group.ds.acl_principal_id
              dependencies:
                databricks_budget_policy.this: |-
                    {
                      "custom_tags": [
                        {
                          "key": "mykey",
                          "value": "myvalue"
                        }
                      ],
                      "policy_name": "data-science-budget-policy"
                    }
        argumentDocs:
            accounts/{account_id}/budgetPolicies/{budget_policy_id}/ruleSets/default: '- access control for a specific budget policy.'
            accounts/{account_id}/groups/{group_id}/ruleSets/default: '- access control for a specific group.'
            accounts/{account_id}/ruleSets/default: '- account-level access control.'
            accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default: '- access control for a specific service principal.'
            grant_rules: '- (Required) The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.'
            grant_rules.principals: '- (Required) a list of principals who are granted a role. The following format is supported:'
            grant_rules.role: '- (Required) Role to be granted. The supported roles are listed below. For more information about these roles, refer to service principal roles, group roles, marketplace roles or budget policy permissions, depending on the name defined:'
            groups/{groupname}: (also exposed as acl_principal_id attribute of databricks_group resource).
            id: '- ID of the access control rule set - the same as name.'
            name: '- (Required) Unique identifier of a rule set. The name determines the resource to which the rule set applies. Changing the name recreates the resource!. Currently, only default rule sets are supported. The following rule set formats are supported:'
            roles/billing.admin: '- Billing administrator.'
            roles/budgetPolicy.manager: '- Manager of a budget policy.'
            roles/budgetPolicy.user: '- User of a budget policy.'
            roles/group.manager: '- Manager of a group.'
            roles/marketplace.admin: '- Databricks Marketplace administrator.'
            roles/servicePrincipal.manager: '- Manager of a service principal.'
            roles/servicePrincipal.user: '- User of a service principal.'
            servicePrincipals/{applicationId}: (also exposed as acl_principal_id attribute of databricks_service_principal resource).
            users/{username}: (also exposed as acl_principal_id attribute of databricks_user resource).
        importStatements: []
    databricks_account_federation_policy:
        subCategory: ""
        name: databricks_account_federation_policy
        title: databricks_account_federation_policy Resource
        examples:
            - name: this
              manifest: |-
                {
                  "oidc_policy": {
                    "issuer": "https://myidp.example.com",
                    "subject_claim": "sub"
                  },
                  "policy_id": "my-policy"
                }
        argumentDocs:
            audiences: |-
                (list of string, optional) - The allowed token audiences, as specified in the 'aud' claim of federated tokens.
                The audience identifier is intended to represent the recipient of the token.
                Can be any non-empty string value. As long as the audience in the token matches
                at least one audience in the policy, the token is considered a match. If audiences
                is unspecified, defaults to your Databricks account id
            create_time: (string) - Creation time of the federation policy
            description: (string, optional) - Description of the federation policy
            issuer: (string, optional) - The required token issuer, as specified in the 'iss' claim of federated tokens
            jwks_json: |-
                (string, optional) - The public keys used to validate the signature of federated tokens, in JWKS format.
                Most use cases should not need to specify this field. If jwks_uri and jwks_json
                are both unspecified (recommended), Databricks automatically fetches the public
                keys from your issuer’s well known endpoint. Databricks strongly recommends
                relying on your issuer’s well known endpoint for discovering public keys
            jwks_uri: |-
                (string, optional) - URL of the public keys used to validate the signature of federated tokens, in
                JWKS format. Most use cases should not need to specify this field. If jwks_uri
                and jwks_json are both unspecified (recommended), Databricks automatically
                fetches the public keys from your issuer’s well known endpoint. Databricks
                strongly recommends relying on your issuer’s well known endpoint for discovering
                public keys
            name: |-
                (string) - Resource name for the federation policy. Example values include
                accounts/<account-id>/federationPolicies/my-federation-policy for Account Federation Policies, and
                accounts/<account-id>/servicePrincipals/<service-principal-id>/federationPolicies/my-federation-policy
                for Service Principal Federation Policies. Typically an output parameter, which does not need to be
                specified in create or update requests. If specified in a request, must match the value in the
                request URL
            oidc_policy: (OidcFederationPolicy, optional)
            policy_id: (string) - The ID of the federation policy
            service_principal_id: (integer) - The service principal ID that this federation policy applies to. Only set for service principal federation policies
            subject: |-
                (string, optional) - The required token subject, as specified in the subject claim of federated tokens.
                Must be specified for service principal federation policies. Must not be specified
                for account federation policies
            subject_claim: |-
                (string, optional) - The claim that contains the subject of the token. If unspecified, the default value
                is 'sub'
            uid: (string) - Unique, immutable id of the federation policy
            update_time: (string) - Last update time of the federation policy
        importStatements: []
    databricks_account_network_policy:
        subCategory: ""
        name: databricks_account_network_policy
        title: databricks_account_network_policy Resource
        examples:
            - name: example_network_policy
              manifest: |-
                {
                  "egress": {
                    "network_access": {
                      "allowed_internet_destinations": [
                        {
                          "destination": "example.com",
                          "internet_destination_type": "DNS_NAME"
                        }
                      ],
                      "allowed_storage_destinations": [
                        {
                          "bucket_name": "example-aws-cloud-storage",
                          "region": "us-west-1",
                          "storage_destination_type": "AWS_S3"
                        }
                      ],
                      "policy_enforcement": {
                        "enforcement_mode": "ENFORCED"
                      },
                      "restriction_mode": "RESTRICTED_ACCESS"
                    }
                  },
                  "network_policy_id": "example-network-policy"
                }
        argumentDocs:
            account_id: (string, optional) - The associated account ID for this Network Policy object
            allowed_internet_destinations: (list of EgressNetworkPolicyNetworkAccessPolicyInternetDestination, optional) - List of internet destinations that serverless workloads are allowed to access when in RESTRICTED_ACCESS mode
            allowed_storage_destinations: (list of EgressNetworkPolicyNetworkAccessPolicyStorageDestination, optional) - List of storage destinations that serverless workloads are allowed to access when in RESTRICTED_ACCESS mode
            azure_storage_account: (string, optional) - The Azure storage account name
            azure_storage_service: (string, optional) - The Azure storage service type (blob, dfs, etc.)
            bucket_name: (string, optional)
            destination: (string, optional) - The internet destination to which access will be allowed. Format dependent on the destination type
            dry_run_mode_product_filter: |-
                (list of string, optional) - When empty, it means dry run for all products.
                When non-empty, it means dry run for specific products and for the other products, they will run in enforced mode
            egress: (NetworkPolicyEgress, optional) - The network policies applying for egress traffic
            enforcement_mode: |-
                (string, optional) - The mode of policy enforcement. ENFORCED blocks traffic that violates policy,
                while DRY_RUN only logs violations without blocking. When not specified,
                defaults to ENFORCED. Possible values are: DRY_RUN, ENFORCED
            internet_destination_type: '(string, optional) - The type of internet destination. Currently only DNS_NAME is supported. Possible values are: DNS_NAME'
            network_access: (EgressNetworkPolicyNetworkAccessPolicy, optional) - The access policy enforced for egress traffic to the internet
            network_policy_id: (string, optional) - The unique identifier for the network policy
            policy_enforcement: (EgressNetworkPolicyNetworkAccessPolicyPolicyEnforcement, optional) - Optional. When policy_enforcement is not provided, we default to ENFORCE_MODE_ALL_SERVICES
            region: (string, optional)
            restriction_mode: '(string, required) - The restriction mode that controls how serverless workloads can access the internet. Possible values are: FULL_ACCESS, RESTRICTED_ACCESS'
            storage_destination_type: '(string, optional) - The type of storage destination. Possible values are: AWS_S3, AZURE_STORAGE, GOOGLE_CLOUD_STORAGE'
        importStatements: []
    databricks_account_setting_v2 Resource:
        subCategory: ""
        name: databricks_account_setting_v2 Resource
        title: databricks_account_setting_v2 Resource
        argumentDocs:
            access_policy_type: '(string, required) - . Possible values are: ALLOW_ALL_DOMAINS, ALLOW_APPROVED_DOMAINS, DENY_ALL_DOMAINS'
            aibi_dashboard_embedding_access_policy: (AibiDashboardEmbeddingAccessPolicy, optional)
            aibi_dashboard_embedding_approved_domains: (AibiDashboardEmbeddingApprovedDomains, optional)
            approved_domains: (list of string, optional)
            automatic_cluster_update_workspace: '(ClusterAutoRestartMessage, optional) - todo: Mark these Public after onboarded to DSL'
            boolean_val: (BooleanMessage, optional)
            can_toggle: (boolean, optional)
            day_of_week: '(string, optional) - . Possible values are: FRIDAY, MONDAY, SATURDAY, SUNDAY, THURSDAY, TUESDAY, WEDNESDAY'
            default_data_security_mode: (DefaultDataSecurityModeMessage, optional)
            effective_aibi_dashboard_embedding_access_policy: (AibiDashboardEmbeddingAccessPolicy, optional)
            effective_aibi_dashboard_embedding_approved_domains: (AibiDashboardEmbeddingApprovedDomains, optional)
            effective_automatic_cluster_update_workspace: (ClusterAutoRestartMessage, optional)
            effective_boolean_val: (BooleanMessage)
            effective_default_data_security_mode: (DefaultDataSecurityModeMessage, optional)
            effective_integer_val: (IntegerMessage)
            effective_personal_compute: (PersonalComputeMessage, optional)
            effective_restrict_workspace_admins: (RestrictWorkspaceAdminsMessage, optional)
            effective_string_val: (StringMessage)
            enabled: (boolean, optional)
            enablement_details: (ClusterAutoRestartMessageEnablementDetails, optional)
            forced_for_compliance_mode: (boolean, optional) - The feature is force enabled if compliance mode is active
            frequency: '(string, optional) - . Possible values are: EVERY_WEEK, FIRST_AND_THIRD_OF_MONTH, FIRST_OF_MONTH, FOURTH_OF_MONTH, SECOND_AND_FOURTH_OF_MONTH, SECOND_OF_MONTH, THIRD_OF_MONTH'
            hours: (integer, optional)
            integer_val: (IntegerMessage, optional)
            maintenance_window: (ClusterAutoRestartMessageMaintenanceWindow, optional)
            minutes: (integer, optional)
            name: (string, optional) - Name of the setting
            personal_compute: (PersonalComputeMessage, optional)
            restart_even_if_no_updates_available: (boolean, optional)
            restrict_workspace_admins: (RestrictWorkspaceAdminsMessage, optional)
            status: '(string, required) - . Possible values are: NOT_SET, SINGLE_USER, USER_ISOLATION'
            string_val: (StringMessage, optional)
            unavailable_for_disabled_entitlement: (boolean, optional) - The feature is unavailable if the corresponding entitlement disabled (see getShieldEntitlementEnable)
            unavailable_for_non_enterprise_tier: (boolean, optional) - The feature is unavailable if the customer doesn't have enterprise tier
            value: (boolean, optional)
            week_day_based_schedule: (ClusterAutoRestartMessageMaintenanceWindowWeekDayBasedSchedule, optional)
            window_start_time: (ClusterAutoRestartMessageMaintenanceWindowWindowStartTime, optional)
        importStatements: []
    databricks_aibi_dashboard_embedding_access_policy_setting:
        subCategory: ""
        name: databricks_aibi_dashboard_embedding_access_policy_setting
        title: databricks_aibi_dashboard_embedding_access_policy_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "aibi_dashboard_embedding_access_policy": [
                    {
                      "access_policy_type": "ALLOW_APPROVED_DOMAINS"
                    }
                  ]
                }
        argumentDocs:
            access_policy_type: '- (Required) Configured embedding policy. Possible values are ALLOW_ALL_DOMAINS, ALLOW_APPROVED_DOMAINS, DENY_ALL_DOMAINS.'
            aibi_dashboard_embedding_access_policy: 'block with following attributes:'
        importStatements: []
    databricks_aibi_dashboard_embedding_approved_domains_setting:
        subCategory: ""
        name: databricks_aibi_dashboard_embedding_approved_domains_setting
        title: databricks_aibi_dashboard_embedding_approved_domains_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "aibi_dashboard_embedding_approved_domains": [
                    {
                      "approved_domains": [
                        "test.com"
                      ]
                    }
                  ],
                  "depends_on": [
                    "${databricks_aibi_dashboard_embedding_access_policy_setting.this}"
                  ]
                }
              dependencies:
                databricks_aibi_dashboard_embedding_access_policy_setting.this: |-
                    {
                      "aibi_dashboard_embedding_access_policy": [
                        {
                          "access_policy_type": "ALLOW_APPROVED_DOMAINS"
                        }
                      ]
                    }
        argumentDocs:
            aibi_dashboard_embedding_approved_domains: 'block with following attributes:'
            approved_domains: '- (Required) the list of approved domains. To allow all subdomains for a given domain, use a wildcard symbol (*) before the domain name, i.e., *.databricks.com will allow to embed into any site under the databricks.com.'
        importStatements: []
    databricks_alert:
        subCategory: ""
        name: databricks_alert
        title: databricks_alert Resource
        examples:
            - name: alert
              manifest: |-
                {
                  "condition": [
                    {
                      "op": "GREATER_THAN",
                      "operand": [
                        {
                          "column": [
                            {
                              "name": "value"
                            }
                          ]
                        }
                      ],
                      "threshold": [
                        {
                          "value": [
                            {
                              "double_value": 42
                            }
                          ]
                        }
                      ]
                    }
                  ],
                  "display_name": "TF new alert",
                  "parent_path": "${databricks_directory.shared_dir.path}",
                  "query_id": "${databricks_query.this.id}"
                }
              references:
                parent_path: databricks_directory.shared_dir.path
                query_id: databricks_query.this.id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
                databricks_query.this: |-
                    {
                      "display_name": "My Query Name",
                      "parent_path": "${databricks_directory.shared_dir.path}",
                      "query_text": "SELECT 42 as value",
                      "warehouse_id": "${databricks_sql_endpoint.example.id}"
                    }
            - name: alert
              manifest: |-
                {
                  "condition": [
                    {
                      "op": "GREATER_THAN",
                      "operand": [
                        {
                          "column": [
                            {
                              "name": "value"
                            }
                          ]
                        }
                      ],
                      "threshold": [
                        {
                          "value": [
                            {
                              "double_value": 42
                            }
                          ]
                        }
                      ]
                    }
                  ],
                  "display_name": "My Alert",
                  "parent_path": "${databricks_directory.shared_dir.path}",
                  "query_id": "${databricks_query.this.id}"
                }
              references:
                parent_path: databricks_directory.shared_dir.path
                query_id: databricks_query.this.id
        argumentDocs:
            bool_value: '- boolean value (true or false) to compare against boolean results.'
            column: '- (Required, Block) Block describing the column from the query result to use for comparison in alert evaluation:'
            condition: '- (Required) Trigger conditions of the alert. Block consists of the following attributes:'
            create_time: '- The timestamp string indicating when the alert was created.'
            custom_body: '- (Optional, String) Custom body of alert notification, if it exists. See Alerts API reference for custom templating instructions.'
            custom_subject: '- (Optional, String) Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See Alerts API reference for custom templating instructions.'
            databricks_permissions: .
            databricks_sql_alert: ', for example, by executing the terraform state show databricks_sql_alert.alert command.'
            display_name: '- (Required, String) Name of the alert.'
            double_value: '- double value to compare against integer and double results.'
            empty_result_state: '- (Optional, String Enum) Alert state if the result is empty (UNKNOWN, OK, TRIGGERED)'
            id: '- unique ID of the Alert.'
            lifecycle_state: '- The workspace state of the alert. Used for tracking trashed status. (Possible values are ACTIVE or TRASHED).'
            name: '- (Required, String) Name of the column.'
            notify_on_ok: '- (Optional, Boolean) Whether to notify alert subscribers when alert returns back to normal.'
            op: '- (Required, String Enum) Operator used for comparison in alert evaluation. (Enum: GREATER_THAN, GREATER_THAN_OR_EQUAL, LESS_THAN, LESS_THAN_OR_EQUAL, EQUAL, NOT_EQUAL, IS_NULL)'
            operand: '- (Required, Block) Name of the column from the query result to use for comparison in alert evaluation:'
            options: 'block is converted into the condition block with the following changes:'
            owner_user_name: '- (Optional, String) Alert owner''s username.'
            parent: (if exists) is renamed to parent_path attribute and should be converted from folders/object_id to the actual path.
            parent_path: '- (Optional, String) The path to a workspace folder containing the alert. The default is the user''s home folder.  If changed, the alert will be recreated.'
            query_id: '- (Required, String) ID of the query evaluated by the alert.'
            rearm: attribute is renamed to seconds_to_retrigger.
            seconds_to_retrigger: '- (Optional, Integer) Number of seconds an alert must wait after being triggered to rearm itself. After rearming, it can be triggered again. If 0 or not specified, the alert will not be triggered again.'
            state: '- Current state of the alert''s trigger status (UNKNOWN, OK, TRIGGERED). This field is set to UNKNOWN if the alert has not yet been evaluated or ran into an error during the last evaluation.'
            string_value: '- string value to compare against string results.'
            terraform import databricks_alert.alert <alert-id>: command.
            terraform import.databricks_permissions: .
            terraform import.import: 'and removed blocks like this:'
            terraform import.terraform apply: command to apply changes.
            terraform import.terraform plan: command to check possible changes, such as value type change, etc.
            terraform plan: command to check possible changes, such as value type change, etc.
            terraform state rm databricks_sql_alert.alert: command.
            threshold: '- (Optional for IS_NULL operation, Block) Threshold value used for comparison in alert evaluation:'
            trigger_time: '- The timestamp string when the alert was last triggered if the alert has been triggered before.'
            update_time: '- The timestamp string indicating when the alert was updated.'
            value: '- (Required, Block) actual value used in comparison (one of the attributes is required):'
        importStatements: []
    databricks_alert_v2:
        subCategory: ""
        name: databricks_alert_v2
        title: databricks_alert_v2 Resource
        examples:
            - name: basic_alert
              manifest: |-
                {
                  "display_name": "High Error Rate Alert",
                  "evaluation": {
                    "comparison_operator": "GREATER_THAN",
                    "empty_result_state": "OK",
                    "notification": {
                      "notify_on_ok": true,
                      "subscriptions": [
                        {
                          "user_email": "user@example.com"
                        }
                      ]
                    },
                    "source": {
                      "aggregation": "COUNT",
                      "display": "Error Count",
                      "name": "error_count"
                    },
                    "threshold": {
                      "value": {
                        "double_value": 100
                      }
                    }
                  },
                  "parent_path": "/Users/user@example.com",
                  "query_text": "SELECT count(*) as error_count FROM logs WHERE level = 'ERROR' AND timestamp \u003e now() - interval 1 hour",
                  "schedule": {
                    "pause_status": "UNPAUSED",
                    "quartz_cron_schedule": "0 0/15 * * * ?",
                    "timezone_id": "America/Los_Angeles"
                  },
                  "warehouse_id": "a7066a8ef796be84"
                }
        argumentDocs:
            aggregation: '(string, optional) - . Possible values are: AVG, COUNT, COUNT_DISTINCT, MAX, MEDIAN, MIN, STDDEV, SUM'
            bool_value: (boolean, optional)
            column: (AlertV2OperandColumn, optional)
            comparison_operator: '(string, optional) - Operator used for comparison in alert evaluation. Possible values are: EQUAL, GREATER_THAN, GREATER_THAN_OR_EQUAL, IS_NOT_NULL, IS_NULL, LESS_THAN, LESS_THAN_OR_EQUAL, NOT_EQUAL'
            create_time: (string) - The timestamp indicating when the alert was created
            custom_description: (string, optional) - Custom description for the alert. support mustache template
            custom_summary: (string, optional) - Custom summary for the alert. support mustache template
            destination_id: (string, optional)
            display: (string, optional)
            display_name: (string, optional) - The display name of the alert
            double_value: (number, optional)
            effective_run_as: |-
                (AlertV2RunAs) - The actual identity that will be used to execute the alert.
                This is an output-only field that shows the resolved run-as identity after applying
                permissions and defaults
            empty_result_state: '(string, optional) - Alert state if result is empty. Possible values are: ERROR, OK, TRIGGERED, UNKNOWN'
            evaluation: (AlertV2Evaluation, optional)
            id: (string) - UUID identifying the alert
            last_evaluated_at: (string) - Timestamp of the last evaluation
            lifecycle_state: '(string) - Indicates whether the query is trashed. Possible values are: ACTIVE, TRASHED'
            name: (string, optional)
            notification: (AlertV2Notification, optional) - User or Notification Destination to notify when alert is triggered
            notify_on_ok: (boolean, optional) - Whether to notify alert subscribers when alert returns back to normal
            owner_user_name: (string) - The owner's username. This field is set to "Unavailable" if the user has been deleted
            parent_path: (string, optional) - The workspace path of the folder containing the alert. Can only be set on create, and cannot be updated
            pause_status: '(string, optional) - Indicate whether this schedule is paused or not. Possible values are: PAUSED, UNPAUSED'
            quartz_cron_schedule: |-
                (string, optional) - A cron expression using quartz syntax that specifies the schedule for this pipeline.
                Should use the quartz format described here: http://www.quartz-scheduler.org/documentation/quartz-2.1.7/tutorials/tutorial-lesson-06.html
            query_text: (string, optional) - Text of the query to be run
            retrigger_seconds: (integer, optional) - Number of seconds an alert must wait after being triggered to rearm itself. After rearming, it can be triggered again. If 0 or not specified, the alert will not be triggered again
            run_as: |-
                (AlertV2RunAs, optional) - Specifies the identity that will be used to run the alert.
                This field allows you to configure alerts to run as a specific user or service principal.
            run_as_user_name: |-
                (string, optional, deprecated) - The run as username or application ID of service principal.
                On Create and Update, this field can be set to application ID of an active service principal. Setting this field requires the servicePrincipal/user role.
                Deprecated: Use run_as field instead. This field will be removed in a future release
            schedule: (CronSchedule, optional)
            service_principal_name: |-
                to the application ID. Requires the servicePrincipal/user role.
                If not specified, the alert will run as the request user
            source: (AlertV2OperandColumn, optional) - Source column from result to use to evaluate alert
            state: '(string) - Latest state of alert evaluation. Possible values are: ERROR, OK, TRIGGERED, UNKNOWN'
            string_value: (string, optional)
            subscriptions: (list of AlertV2Subscription, optional)
            threshold: (AlertV2Operand, optional) - Threshold to user for alert evaluation, can be a column or a value
            timezone_id: |-
                (string, optional) - A Java timezone id. The schedule will be resolved using this timezone.
                This will be combined with the quartz_cron_schedule to determine the schedule.
                See https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-conf-mgmt-set-timezone.html for details
            update_time: (string) - The timestamp indicating when the alert was updated
            user_email: (string, optional)
            user_name: to the email of an active workspace user. Users can only set this to their own email.
            value: (AlertV2OperandValue, optional)
            warehouse_id: (string, optional) - ID of the SQL warehouse attached to the alert
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_app:
        subCategory: ""
        name: databricks_app
        title: databricks_app Resource
        examples:
            - name: this
              manifest: |-
                {
                  "description": "My app",
                  "name": "my-custom-app",
                  "resources": [
                    {
                      "name": "sql-warehouse",
                      "sql_warehouse": {
                        "id": "e9ca293f79a74b5c",
                        "permission": "CAN_MANAGE"
                      }
                    },
                    {
                      "name": "serving-endpoint",
                      "serving_endpoint": {
                        "name": "databricks-meta-llama-3-1-70b-instruct",
                        "permission": "CAN_MANAGE"
                      }
                    },
                    {
                      "job": {
                        "id": "1234",
                        "permission": "CAN_MANAGE"
                      },
                      "name": "job"
                    }
                  ]
                }
        argumentDocs:
            app_status: attribute
            budget_policy_id: '- (Optional) The Budget Policy ID set for this resource.'
            compute_status: attribute
            create_time: '- The creation time of the app.'
            creator: '- The email of the user that created the app.'
            database: attribute
            database_name: '- The name of database.'
            default_source_code_path: '- The default workspace file system path of the source code from which app deployment are created. This field tracks the workspace source code path of the last active deployment.'
            description: '- (Optional) The description of the app.'
            effective_budget_policy_id: '- The effective budget policy ID.'
            effective_user_api_scopes: '- A list of effective api scopes granted to the user access token.'
            id: '- Id of the SQL warehouse to grant permission on.'
            instance_name: '- The name of database instance.'
            job: attribute
            key: '- Key of the secret to grant permission on.'
            message: '- Compute status message'
            name: '- (Required) The name of the app. The name must contain only lowercase alphanumeric characters and hyphens. It must be unique within the workspace.'
            permission: '- Permission to grant on the secret scope. For secrets, only one permission is allowed. Permission must be one of: READ, WRITE, MANAGE.'
            resources: '- (Optional) A list of resources that the app have access to.'
            scope: '- Scope of the secret to grant permission on.'
            secret: attribute
            securable_full_name: '- the full name of UC securable, i.e. my-catalog.my-schema.my-volume.'
            securable_type: '- the type of UC securable, i.e. VOLUME.'
            service_principal_id: '- id of the app service principal'
            service_principal_name: '- name of the app service principal'
            serving_endpoint: attribute
            sql_warehouse: attribute
            state: '- State of the app compute.'
            uc_securable: attribute (see the API docs for full list of supported UC objects)
            update_time: '- The update time of the app.'
            updater: '- The email of the user that last updated the app.'
            url: '- The URL of the app once it is deployed.'
            user_api_scopes: '- (Optional) A list of api scopes granted to the user access token.'
        importStatements: []
    databricks_apps_settings_custom_template:
        subCategory: ""
        name: databricks_apps_settings_custom_template
        title: databricks_apps_settings_custom_template Resource
        examples:
            - name: this
              manifest: |-
                {
                  "description": "A sample custom app template",
                  "git_provider": "github",
                  "git_repo": "https://github.com/example/repo.git",
                  "manifest": {
                    "name": "my-custom-app",
                    "version": 1
                  },
                  "name": "my-custom-template",
                  "path": "path-to-template"
                }
            - name: api_scopes_example
              manifest: |-
                {
                  "description": "A template that requests user API scopes",
                  "git_provider": "github",
                  "git_repo": "https://github.com/example/my-app.git",
                  "manifest": {
                    "description": "This app requires the SQL API scope.",
                    "name": "my-databricks-app",
                    "user_api_scopes": [
                      "sql"
                    ],
                    "version": 1
                  },
                  "name": "my-api-template",
                  "path": "templates/app"
                }
            - name: resources_example
              manifest: |-
                {
                  "description": "Template that requires secret and SQL warehouse access",
                  "git_provider": "github",
                  "git_repo": "https://github.com/example/resource-app.git",
                  "manifest": {
                    "description": "This app requires access to a secret and SQL warehouse.",
                    "name": "resource-consuming-app",
                    "resource_specs": [
                      {
                        "description": "A secret needed by the app",
                        "name": "my-secret",
                        "secret_spec": {
                          "permission": "READ"
                        }
                      },
                      {
                        "description": "Warehouse access",
                        "name": "warehouse",
                        "sql_warehouse_spec": {
                          "permission": "CAN_USE"
                        }
                      }
                    ],
                    "version": 1
                  },
                  "name": "my-resource-template",
                  "path": "resource-template"
                }
        argumentDocs:
            creator: (string)
            description: (string, optional) - The description of the template
            git_provider: (string, required) - The Git provider of the template
            git_repo: (string, required) - The Git repository URL that the template resides in
            job_spec: (AppManifestAppResourceJobSpec, optional)
            manifest: (AppManifest, required) - The manifest of the template. It defines fields and default values when installing the template
            name: |-
                (string, required) - The name of the template. It must contain only alphanumeric characters, hyphens, underscores, and whitespaces.
                It must be unique within the workspace
            path: (string, required) - The path to the template within the Git repository
            permission: '(string, required) - Permissions to grant on the Job. Supported permissions are: "CAN_MANAGE", "IS_OWNER", "CAN_MANAGE_RUN", "CAN_VIEW". Possible values are: CAN_MANAGE, CAN_MANAGE_RUN, CAN_VIEW, IS_OWNER'
            resource_specs: (list of AppManifestAppResourceSpec, optional)
            secret_spec: (AppManifestAppResourceSecretSpec, optional)
            securable_type: '(string, required) - . Possible values are: VOLUME'
            serving_endpoint_spec: (AppManifestAppResourceServingEndpointSpec, optional)
            sql_warehouse_spec: (AppManifestAppResourceSqlWarehouseSpec, optional)
            uc_securable_spec: (AppManifestAppResourceUcSecurableSpec, optional)
            version: (integer, required) - The manifest schema version, for now only 1 is allowed
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_artifact_allowlist:
        subCategory: ""
        name: databricks_artifact_allowlist
        title: databricks_artifact_allowlist Resource
        examples:
            - name: init_scripts
              manifest: |-
                {
                  "artifact_matcher": [
                    {
                      "artifact": "/Volumes/inits",
                      "match_type": "PREFIX_MATCH"
                    }
                  ],
                  "artifact_type": "INIT_SCRIPT"
                }
        argumentDocs:
            artifact_matcher.artifact: '- The artifact path or maven coordinate.'
            artifact_matcher.match_type: '- The pattern matching type of the artifact. Only PREFIX_MATCH is supported.'
            artifact_type: '- The artifact type of the allowlist. Can be INIT_SCRIPT, LIBRARY_JAR or LIBRARY_MAVEN. Change forces creation of a new resource.'
            created_at: '-  Time at which this artifact allowlist was set.'
            created_by: '-  Identity that set the artifact allowlist.'
            id: '- ID of the artifact allow list in form of metastore_id|artifact_type.'
            metastore_id: '- ID of the parent metastore.'
        importStatements: []
    databricks_automatic_cluster_update_setting Resource:
        subCategory: ""
        name: databricks_automatic_cluster_update_setting Resource
        title: databricks_automatic_cluster_update_setting Resource
        argumentDocs:
            automatic_cluster_update_workspace: (Required) block with following attributes
            day_of_week: '- the day of the week in uppercase, e.g. MONDAY or SUNDAY'
            enabled: '- (Required) The configuration details.'
            frequency: '- one of the FIRST_OF_MONTH, SECOND_OF_MONTH, THIRD_OF_MONTH, FOURTH_OF_MONTH, FIRST_AND_THIRD_OF_MONTH, SECOND_AND_FOURTH_OF_MONTH, EVERY_WEEK.'
            hours: '- hour to perform update: 0-23'
            maintenance_window: block that defines the maintenance frequency with the following arguments
            minutes: '- minute to perform update: 0-59'
            restart_even_if_no_updates_available: '- (Optional) To force clusters and other compute resources to restart during the maintenance window regardless of the availability of a new update.'
            week_day_based_schedule: block with the following arguments
            window_start_time: block that defines the time of your maintenance window. The default timezone is UTC and cannot be changed.
        importStatements: []
    databricks_budget:
        subCategory: ""
        name: databricks_budget
        title: databricks_budget Resource
        examples:
            - name: this
              manifest: |-
                {
                  "alert_configurations": [
                    {
                      "action_configurations": [
                        {
                          "action_type": "EMAIL_NOTIFICATION",
                          "target": "abc@gmail.com"
                        }
                      ],
                      "quantity_threshold": "840",
                      "quantity_type": "LIST_PRICE_DOLLARS_USD",
                      "time_period": "MONTH",
                      "trigger_type": "CUMULATIVE_SPENDING_EXCEEDED"
                    }
                  ],
                  "display_name": "databricks-workspace-budget",
                  "filter": [
                    {
                      "tags": [
                        {
                          "key": "Team",
                          "value": [
                            {
                              "operator": "IN",
                              "values": [
                                "Data Science"
                              ]
                            }
                          ]
                        },
                        {
                          "key": "Environment",
                          "value": [
                            {
                              "operator": "IN",
                              "values": [
                                "Development"
                              ]
                            }
                          ]
                        }
                      ],
                      "workspace_id": [
                        {
                          "operator": "IN",
                          "values": [
                            1234567890098765
                          ]
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            account_id: '- The ID of the Databricks Account.'
            action_configurations: '- (Required) List of action configurations to take when the budget alert is triggered. Consists of the following fields:'
            action_type: '- (Required, String Enum) The type of action to take when the budget alert is triggered. (Enum: EMAIL_NOTIFICATION)'
            budget_configuration_id: '- The ID of the budget configuration.'
            display_name: '- (Required) Name of the budget in Databricks Account.'
            key: '- (Required, String) The key of the tag.'
            operator: '- (Required, String Enum) The operator to use for the filter. (Enum: IN)'
            quantity_threshold: '- (Required, String) The threshold for the budget alert to determine if it is in a triggered state. The number is evaluated based on quantity_type.'
            quantity_type: '- (Required, String Enum) The way to calculate cost for this budget alert. This is what quantity_threshold is measured in. (Enum: LIST_PRICE_DOLLARS_USD)'
            tags: '- (Optional) List of tags to filter by. Consists of the following fields:'
            target: '- (Required, String) The target of the action. For EMAIL_NOTIFICATION, this is the email address to send the notification to.'
            time_period: '- (Required, String Enum) The time window of usage data for the budget. (Enum: MONTH)'
            trigger_type: '- (Required, String Enum) The evaluation method to determine when this budget alert is in a triggered state. (Enum: CUMULATIVE_SPENDING_EXCEEDED)'
            value: '- (Required) Consists of the following fields:'
            values: '- (Required, List of numbers) The values to filter by.'
            workspace_id: '- (Optional) Filter by workspace ID (if empty, include usage all usage for this account). Consists of the following fields:'
        importStatements: []
    databricks_budget_policy:
        subCategory: ""
        name: databricks_budget_policy
        title: databricks_budget_policy Resource
        examples:
            - name: this
              manifest: |-
                {
                  "custom_tags": [
                    {
                      "key": "mykey",
                      "value": "myvalue"
                    }
                  ],
                  "policy_name": "my-budget-policy"
                }
        argumentDocs:
            binding_workspace_ids: |-
                (list of integer, optional) - List of workspaces that this budget policy will be exclusively bound to.
                An empty binding implies that this budget policy is open to any workspace in the account
            custom_tags: (list of CustomPolicyTag, optional) - A list of tags defined by the customer. At most 20 entries are allowed per policy
            key: (string, required) - The key of the tag.
            policy_id: (string) - The Id of the policy. This field is generated by Databricks and globally unique
            policy_name: (string, optional) - The name of the policy.
            value: (string, optional) - The value of the tag
        importStatements: []
    databricks_catalog:
        subCategory: ""
        name: databricks_catalog
        title: databricks_catalog Resource
        examples:
            - name: sandbox
              manifest: |-
                {
                  "comment": "this catalog is managed by terraform",
                  "name": "sandbox",
                  "properties": {
                    "purpose": "testing"
                  }
                }
        argumentDocs:
            comment: '- (Optional) User-supplied free-form text.'
            connection_name: '- (Optional) For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.'
            enable_predictive_optimization: '- (Optional) Whether predictive optimization should be enabled for this object and objects under it. Can be ENABLE, DISABLE or INHERIT'
            force_destroy: '- (Optional) Delete catalog regardless of its contents.'
            id: '- ID of this catalog - same as the name.'
            isolation_mode: '- (Optional) Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be ISOLATED or OPEN. Setting the catalog to ISOLATED will automatically allow access from the current workspace.'
            metastore_id: '- ID of the parent metastore.'
            name: '- Name of Catalog relative to parent metastore.'
            options: '- (Optional) For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.'
            owner: '- (Optional) Username/groupname/sp application_id of the catalog owner.'
            properties: '- (Optional) Extensible Catalog properties.'
            provider_name: '- (Optional) For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.'
            share_name: '- (Optional) For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.'
            storage_root: '- (Optional if storage_root is specified for the metastore) Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.'
        importStatements: []
    databricks_catalog_workspace_binding:
        subCategory: ""
        name: databricks_catalog_workspace_binding
        title: databricks_catalog_workspace_binding Resource
        examples:
            - name: sandbox
              manifest: |-
                {
                  "securable_name": "${databricks_catalog.sandbox.name}",
                  "workspace_id": "${databricks_mws_workspaces.other.workspace_id}"
                }
              references:
                securable_name: databricks_catalog.sandbox.name
                workspace_id: databricks_mws_workspaces.other.workspace_id
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "isolation_mode": "ISOLATED",
                      "name": "sandbox"
                    }
        argumentDocs:
            binding_type: '- Binding mode. Default to BINDING_TYPE_READ_WRITE. Possible values are BINDING_TYPE_READ_ONLY, BINDING_TYPE_READ_WRITE'
            securable_name: '- Name of securable. Change forces creation of a new resource.'
            securable_type: '- Type of securable. Default to catalog. Change forces creation of a new resource.'
            workspace_id: '- ID of the workspace. Change forces creation of a new resource.'
        importStatements: []
    databricks_clean_room_asset Resource:
        subCategory: ""
        name: databricks_clean_room_asset Resource
        title: databricks_clean_room_asset Resource
        argumentDocs:
            added_at: (integer) - When the asset is added to the clean room, in epoch milliseconds
            asset_type: '(string, required) - The type of the asset. Possible values are: FOREIGN_TABLE, NOTEBOOK_FILE, TABLE, VIEW, VOLUME'
            clean_room_name: |-
                (string, optional) - The name of the clean room this asset belongs to.
                This field is required for create operations and populated by the server for responses
            columns: (list of ColumnInfo) - The metadata information of the columns in the foreign table
            comment: (string, optional) - Review comment
            created_at_millis: (integer, optional) - When the review was submitted, in epoch milliseconds
            etag: (string) - Server generated etag that represents the notebook version
            foreign_table: |-
                (CleanRoomAssetForeignTable, optional) - Foreign table details available to all collaborators of the clean room.
                Present if and only if asset_type is FOREIGN_TABLE
            foreign_table_local_details: |-
                (CleanRoomAssetForeignTableLocalDetails, optional) - Local details for a foreign that are only available to its owner.
                Present if and only if asset_type is FOREIGN_TABLE
            function_name: (string, optional) - The full name of the column mask SQL UDF
            local_name: |-
                (string, required) - The fully qualified name of the foreign table in its owner's local metastore,
                in the format of catalog.schema.foreign_table_name
            mask: (ColumnMask, optional)
            name: |-
                (string, required) - A fully qualified name that uniquely identifies the asset within the clean room.
                This is also the name displayed in the clean room UI.
            notebook: |-
                (CleanRoomAssetNotebook, optional) - Notebook details available to all collaborators of the clean room.
                Present if and only if asset_type is NOTEBOOK_FILE
            notebook_content: |-
                (string, required) - Base 64 representation of the notebook contents.
                This is the same format as returned by :method:workspace/export with the format of HTML
            nullable: '(boolean, optional) - Whether field may be Null (default: true)'
            op: '(string, optional) - The operator to apply for the value. Possible values are: EQUAL, LIKE'
            owner_collaborator_alias: (string) - The alias of the collaborator who owns this asset
            partition_index: (integer, optional) - Partition index for column
            partitions: (list of Partition, optional) - Partition filtering specification for a shared table
            position: (integer, optional) - Ordinal position of column (starting at position 0)
            recipient_property_key: |-
                (string, optional) - The key of a Delta Sharing recipient's property. For example "databricks-account-id".
                When this field is set, field value can not be set
            review_state: '(string, optional) - Review outcome. Possible values are: APPROVED, PENDING, REJECTED'
            review_sub_reason: '(string, optional) - Specified when the review was not explicitly made by a user. Possible values are: AUTO_APPROVED, BACKFILLED'
            reviewer_collaborator_alias: (string, optional) - Collaborator alias of the reviewer
            reviews: (list of CleanRoomNotebookReview) - All existing approvals or rejections
            runner_collaborator_aliases: (list of string, optional) - Aliases of collaborators that can run the notebook
            status: '(string) - Status of the asset. Possible values are: ACTIVE, PENDING, PERMISSION_DENIED'
            table: |-
                (CleanRoomAssetTable, optional) - Table details available to all collaborators of the clean room.
                Present if and only if asset_type is TABLE
            table_local_details: |-
                (CleanRoomAssetTableLocalDetails, optional) - Local details for a table that are only available to its owner.
                Present if and only if asset_type is TABLE
            type_interval_type: (string, optional) - Format of IntervalType
            type_json: (string, optional) - Full data type specification, JSON-serialized
            type_name: '(string, optional) - . Possible values are: ARRAY, BINARY, BOOLEAN, BYTE, CHAR, DATE, DECIMAL, DOUBLE, FLOAT, GEOGRAPHY, GEOMETRY, INT, INTERVAL, LONG, MAP, NULL, SHORT, STRING, STRUCT, TABLE_TYPE, TIMESTAMP, TIMESTAMP_NTZ, USER_DEFINED_TYPE, VARIANT'
            type_precision: (integer, optional) - Digits of precision; required for DecimalTypes
            type_scale: (integer, optional) - Digits to right of decimal; Required for DecimalTypes
            type_text: (string, optional) - Full data type specification as SQL/catalogString text
            using_column_names: |-
                (list of string, optional) - The list of additional table columns to be passed as input to the column mask function. The
                first arg of the mask function should be of the type of the column being masked and the
                types of the rest of the args should match the types of columns in 'using_column_names'
            value: |-
                (string, optional) - The value of the partition column. When this value is not set, it means null value.
                When this field is set, field recipient_property_key can not be set
            values: (list of PartitionValue, optional) - An array of partition values
            view: |-
                (CleanRoomAssetView, optional) - View details available to all collaborators of the clean room.
                Present if and only if asset_type is VIEW
            view_local_details: |-
                (CleanRoomAssetViewLocalDetails, optional) - Local details for a view that are only available to its owner.
                Present if and only if asset_type is VIEW
            volume_local_details: |-
                (CleanRoomAssetVolumeLocalDetails, optional) - Local details for a volume that are only available to its owner.
                Present if and only if asset_type is VOLUME
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_clean_room_auto_approval_rule Resource:
        subCategory: ""
        name: databricks_clean_room_auto_approval_rule Resource
        title: databricks_clean_room_auto_approval_rule Resource
        argumentDocs:
            author_collaborator_alias: |-
                (string, optional) - Collaborator alias of the author covered by the rule.
                Only one of author_collaborator_alias and author_scope can be set
            author_scope: |-
                (string, optional) - Scope of authors covered by the rule.
                Only one of author_collaborator_alias and author_scope can be set. Possible values are: ANY_AUTHOR
            clean_room_name: (string, optional) - The name of the clean room this auto-approval rule belongs to
            created_at: (integer) - Timestamp of when the rule was created, in epoch milliseconds
            rule_id: (string) - A generated UUID identifying the rule
            rule_owner_collaborator_alias: (string) - The owner of the rule to whom the rule applies
            runner_collaborator_alias: (string, optional) - Collaborator alias of the runner covered by the rule
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_clean_rooms_clean_room:
        subCategory: ""
        name: databricks_clean_rooms_clean_room
        title: databricks_clean_rooms_clean_room Resource
        examples:
            - name: this
              manifest: |-
                {
                  "name": "example-clean-room",
                  "owner": "example@databricks.com",
                  "remote_detailed_info": {
                    "cloud_vendor": "aws",
                    "collaborators": [
                      {
                        "collaborator_alias": "collaborator",
                        "global_metastore_id": "aws:us-east-1:12345678-1234-1234-1234-123456789012",
                        "invite_recipient_email": "example@databricks.com",
                        "invite_recipient_workspace_id": "123456789012345"
                      },
                      {
                        "collaborator_alias": "creator",
                        "global_metastore_id": "aws:us-east-1:12345678-1234-1234-1234-123456789012"
                      }
                    ],
                    "egress_network_policy": {
                      "internet_access": {
                        "restriction_mode": "RESTRICTED_ACCESS"
                      }
                    },
                    "region": "us-west-2"
                  }
                }
        argumentDocs:
            access_restricted: '(string) - Whether clean room access is restricted due to CSP. Possible values are: CSP_MISMATCH, NO_RESTRICTION'
            allowed_internet_destinations: (list of EgressNetworkPolicyInternetAccessPolicyInternetDestination, optional)
            allowed_paths: (list of string, optional)
            allowed_storage_destinations: (list of EgressNetworkPolicyInternetAccessPolicyStorageDestination, optional)
            azure_container: (string, optional)
            azure_dns_zone: (string, optional)
            azure_storage_account: (string, optional)
            azure_storage_service: (string, optional)
            bucket_name: (string, optional)
            catalog_name: |-
                (string, optional) - The name of the output catalog in UC.
                It should follow UC securable naming requirements.
                The field will always exist if status is CREATED
            central_clean_room_id: (string) - Central clean room ID
            cloud_vendor: (string, optional) - Cloud vendor (aws,azure,gcp) of the central clean room
            collaborator_alias: |-
                (string, required) - Collaborator alias specified by the clean room creator. It is unique across all collaborators of this clean room, and used to derive
                multiple values internally such as catalog alias and clean room name for single metastore clean rooms.
                It should follow UC securable naming requirements
            collaborators: |-
                (list of CleanRoomCollaborator, optional) - Collaborators in the central clean room. There should one and only one collaborator
                in the list that satisfies the owner condition:
            comment: (string, optional)
            compliance_security_profile: (ComplianceSecurityProfile)
            compliance_standards: (list of ComplianceStandard, optional) - The list of compliance standards that the compliance security profile is configured to enforce
            created_at: (integer) - When the clean room was created, in epoch milliseconds
            creator: (CleanRoomCollaborator) - Collaborator who creates the clean room
            destination: (string, optional)
            display_name: |-
                (string) - Generated display name for the collaborator. In the case of a single metastore clean room, it is the clean
                room name. For x-metastore clean rooms, it is the organization name of the metastore. It is not restricted to
                these values and could change in the future
            egress_network_policy: (EgressNetworkPolicy, optional) - Egress network policy to apply to the central clean room workspace
            global_metastore_id: (string, optional) - The global Unity Catalog metastore ID of the collaborator. The identifier is of format cloud:region:metastore-uuid
            internet_access: (EgressNetworkPolicyInternetAccessPolicy, optional) - The access policy enforced for egress traffic to the internet
            invite_recipient_email: |-
                (string, optional) - Email of the user who is receiving the clean room "invitation". It should be empty
                for the creator of the clean room, and non-empty for the invitees of the clean room.
                It is only returned in the output when clean room creator calls GET
            invite_recipient_workspace_id: |-
                (integer, optional) - Workspace ID of the user who is receiving the clean room "invitation". Must be specified if
                invite_recipient_email is specified.
                It should be empty when the collaborator is the creator of the clean room
            is_enabled: (boolean, optional) - Whether the compliance security profile is enabled
            local_collaborator_alias: (string) - The alias of the collaborator tied to the local clean room
            log_only_mode: (EgressNetworkPolicyInternetAccessPolicyLogOnlyMode, optional) - Optional. If not specified, assume the policy is enforced for all workloads
            log_only_mode_type: '(string, optional) - . Possible values are: ALL_SERVICES, SELECTED_SERVICES'
            name: |-
                (string, optional) - The name of the clean room.
                It should follow UC securable naming requirements
            organization_name: |-
                (string) - Organization name
                configured in the metastore
            output_catalog: |-
                (CleanRoomOutputCatalog) - Output catalog of the clean room. It is an output only field. Output catalog is manipulated
                using the separate CreateCleanRoomOutputCatalog API
            owner: (string, optional) - This is the Databricks username of the owner of the local clean room securable for permission management
            protocol: '(string, optional) - . Possible values are: TCP'
            region: (string, optional) - Region of the central clean room
            remote_detailed_info: |-
                (CleanRoomRemoteDetail, optional) - Central clean room details. During creation, users need to specify
                cloud_vendor, region, and collaborators.global_metastore_id.
                This field will not be filled in the ListCleanRooms call
            restriction_mode: '(string, optional) - . Possible values are: FULL_ACCESS, PRIVATE_ACCESS_ONLY, RESTRICTED_ACCESS'
            status: '(string) - Clean room status. Possible values are: ACTIVE, DELETED, FAILED, PROVISIONING'
            type: '(string, optional) - . Possible values are: FQDN'
            updated_at: (integer) - When the clean room was last updated, in epoch milliseconds
            workloads: (list of string, optional)
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_cluster:
        subCategory: ""
        name: databricks_cluster
        title: databricks_cluster Resource
        examples:
            - name: shared_autoscaling
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: shared_autoscaling
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_conf": {
                    "spark.databricks.io.cache.enabled": true,
                    "spark.databricks.io.cache.maxDiskUsage": "50g",
                    "spark.databricks.io.cache.maxMetaDataCache": "1g"
                  },
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: single_node
              manifest: |-
                {
                  "autotermination_minutes": 20,
                  "cluster_name": "Single Node",
                  "is_single_node": true,
                  "kind": "CLASSIC_PREVIEW",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: cluster_with_table_access_control
              manifest: |-
                {
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared High-Concurrency",
                  "custom_tags": {
                    "ResourceClass": "Serverless"
                  },
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "serverless",
                    "spark.databricks.repl.allowedLanguages": "python,sql"
                  },
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "aws_attributes": [
                    {
                      "availability": "SPOT",
                      "first_on_demand": 1,
                      "spot_bid_price_percent": 100,
                      "zone_id": "us-east-1"
                    }
                  ],
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "azure_attributes": [
                    {
                      "availability": "SPOT_WITH_FALLBACK_AZURE",
                      "first_on_demand": 1,
                      "spot_bid_max_price": 100
                    }
                  ],
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "gcp_attributes": [
                    {
                      "availability": "PREEMPTIBLE_WITH_FALLBACK_GCP",
                      "zone_id": "AUTO"
                    }
                  ],
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "docker_image": [
                    {
                      "basic_auth": [
                        {
                          "password": "${azurerm_container_registry.this.admin_password}",
                          "username": "${azurerm_container_registry.this.admin_username}"
                        }
                      ],
                      "url": "${docker_registry_image.this.name}"
                    }
                  ]
                }
              references:
                docker_image.basic_auth.password: azurerm_container_registry.this.admin_password
                docker_image.basic_auth.username: azurerm_container_registry.this.admin_username
                docker_image.url: docker_registry_image.this.name
              dependencies:
                docker_registry_image.this: |-
                    {
                      "build": [
                        {}
                      ],
                      "name": "${azurerm_container_registry.this.login_server}/sample:latest"
                    }
            - name: with_nfs
              manifest: |-
                {
                  "cluster_mount_info": [
                    {
                      "local_mount_dir_path": "/mnt/nfs-test",
                      "network_filesystem_info": [
                        {
                          "mount_options": "sec=sys,vers=3,nolock,proto=tcp",
                          "server_address": "${local.storage_account}.blob.core.windows.net"
                        }
                      ],
                      "remote_mount_dir_path": "${local.storage_account}/${local.storage_container}"
                    }
                  ]
                }
            - name: with_nfs
              manifest: |-
                {
                  "workload_type": [
                    {
                      "clients": [
                        {
                          "jobs": false,
                          "notebooks": true
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            AUTO: ': Databricks picks an availability zone to schedule the cluster on.'
            DATA_SECURITY_MODE_AUTO: ': Databricks will choose the most appropriate access mode depending on your compute configuration.'
            DATA_SECURITY_MODE_DEDICATED: ': Alias for SINGLE_USER.'
            DATA_SECURITY_MODE_STANDARD: ': Alias for USER_ISOLATION.'
            HA: '(default): High availability, spread nodes across availability zones for a Databricks deployment region.'
            allow_cluster_create: argument set would still be able to create clusters, but within the boundary of the policy.
            apply_policy_default_values: '- (Optional) Whether to use policy default values for missing cluster attributes.'
            autoscale.max_workers: '- (Optional) The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.'
            autoscale.min_workers: '- (Optional) The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.'
            autotermination_minutes: '- (Optional) Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to 60.  We highly recommend having this setting present for Interactive/BI clusters.'
            aws_attributes.availability: '- (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT, SPOT_WITH_FALLBACK and ON_DEMAND. Note: If first_on_demand is zero, this availability type will be used for the entire cluster. Backend default value is SPOT_WITH_FALLBACK and could change in the future'
            aws_attributes.ebs_volume_count: '- (Optional) The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.'
            aws_attributes.ebs_volume_size: '- (Optional) The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).'
            aws_attributes.ebs_volume_type: '- (Optional) The type of EBS volumes that will be launched with this cluster. Valid values are GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD. Use this option only if you''re not picking Delta Optimized  node types.'
            aws_attributes.first_on_demand: '- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. If unspecified, the default value is 0.'
            aws_attributes.instance_profile_arn: '- (Optional) Nodes for this cluster will only be placed on AWS instances with this instance profile. Please see databricks_instance_profile resource documentation for extended examples on adding a valid instance profile using Terraform.'
            aws_attributes.spot_bid_price_percent: '- (Optional) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000.'
            aws_attributes.zone_id: '- (Required) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-west-2a. The provided availability zone must be in the same region as the Databricks deployment. For example, us-west-2a is not a valid zone ID if the Databricks deployment resides in the us-east-1 region. Enable automatic availability zone selection ("Auto-AZ"), by setting the value auto. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.'
            azure_attributes.availability: '- (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT_AZURE, SPOT_WITH_FALLBACK_AZURE, and ON_DEMAND_AZURE. Note: If first_on_demand is zero, this availability type will be used for the entire cluster.'
            azure_attributes.first_on_demand: '- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.'
            azure_attributes.spot_bid_max_price: '- (Optional) The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance.'
            canned_acl: '- (Optional) Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.'
            cluster_mount_info.local_mount_dir_path: '- (Required) path inside the Spark container.'
            cluster_mount_info.network_filesystem_info: '- block specifying connection. It consists of:'
            cluster_mount_info.remote_mount_dir_path: '- (Optional) string specifying path to mount on the remote service.'
            cluster_name: '- (Optional) Cluster name, which doesn''t have to be unique. If not specified at creation, the cluster name will be an empty string.'
            custom_tags: '- (Optional) Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to default_tags. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an x_ when it is propagated.'
            data_security_mode: '- (Optional) Select the security features of the cluster (see API docs for full list of values). Unity Catalog requires SINGLE_USER or USER_ISOLATION mode. LEGACY_PASSTHROUGH for passthrough cluster and LEGACY_TABLE_ACL for Table ACL cluster. If omitted, default security features are enabled. To disable security features use NONE or legacy mode NO_ISOLATION.  If kind is specified, then the following options are available:'
            default_tags: '- (map) Tags that are added by Databricks by default, regardless of any custom_tags that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: , and any workspace and pool tags.'
            destination: '- S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.'
            docker_image.basic_auth: '- (Optional) basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.'
            docker_image.url: '- URL for the Docker image'
            driver_instance_pool_id: (Optional) - similar to instance_pool_id, but for driver node. If omitted, and instance_pool_id is specified, then the driver will be allocated from that pool.
            driver_node_type_id: '- (Optional) The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as node_type_id defined above.'
            enable_elastic_disk: '- (Optional) If you don''t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster''s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance''s local storage). To scale down EBS usage, make sure you have autotermination_minutes and autoscale attributes set. More documentation available at cluster configuration page.'
            enable_encryption: '- (Optional) Enable server-side encryption, false by default.'
            enable_local_disk_encryption: '- (Optional) Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster''s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.'
            encryption_type: '- (Optional) The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.'
            endpoint: '- (Optional) S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.'
            gcp_attributes.availability: ', and will be removed soon.'
            gcp_attributes.boot_disk_size: (optional, int) Boot disk size in GB
            gcp_attributes.first_on_demand: '- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.'
            gcp_attributes.google_service_account: '- (Optional, string) Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.'
            gcp_attributes.local_ssd_count: (optional, int) Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
            gcp_attributes.use_preemptible_executors: '- (Optional, bool) if we should use preemptible executors (GCP documentation). Warning: this field is deprecated in favor of'
            gcp_attributes.zone_id: '(optional)  Identifier for the availability zone in which the cluster resides. This can be one of the following:'
            id: '- Canonical unique identifier for the cluster.'
            idempotency_token: '- (Optional) An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster''s ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.'
            instance_pool_id: (Optional - required if node_type_id is not given) - To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to TERMINATED, the instances it used are returned to the pool and reused by a different cluster.
            instance_profile_arn: (AWS only) can control which data a given cluster can access through cloud-native controls.
            is_pinned: '- (Optional) boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters'' maximum number is limited to 100, so apply may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).'
            is_single_node: '- (Optional, Boolean, only with kind) When set to true, Databricks will automatically set single node related custom_tags, spark_conf, and num_workers.'
            kind: '- (Optional, enum) The kind of compute described by this compute specification.  Possible values (see API docs for full list): CLASSIC_PREVIEW (if corresponding public preview is enabled).'
            kms_key: '- (Optional) KMS key used if encryption is enabled and encryption type is set to sse-kms.'
            mount_options: '- (Optional) string that will be passed as options passed to the mount command.'
            no_wait: '- (Optional) If true, the provider will not wait for the cluster to reach RUNNING state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).'
            node_type_id: '- (Required - optional if instance_pool_id is given) Any supported databricks_node_type id. If instance_pool_id is specified, this field is not needed.'
            num_workers: '- (Optional) Number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes.'
            policy_id: '- (Optional) Identifier of Cluster Policy to validate cluster and preset certain defaults. The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters. For example, when you specify policy_id of external metastore policy, you still have to fill in relevant keys for spark_conf.  If relevant fields aren''t filled in, then it will cause the configuration drift detected on each plan/apply, and Terraform will try to apply the detected changes.'
            region: '- (Optional) S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.'
            runtime_engine: '- (Optional) The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: PHOTON, STANDARD.'
            server_address: '- (Required) host name.'
            single_user_name: '- (Optional) The optional user name of the user (or group name if kind if specified) to assign to an interactive cluster. This field is required when using data_security_mode set to SINGLE_USER or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).'
            spark.databricks.cluster.profile: set to serverless
            spark.databricks.repl.allowedLanguages: 'set to a list of supported languages, for example: python,sql, or python,sql,r.  Scala is not supported!'
            spark_conf: '- (Optional) Map with key-value pairs to fine-tune Spark clusters, where you can provide custom Spark configuration properties in a cluster configuration.'
            spark_env_vars: '- (Optional) Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X=''Y'') while launching the driver and workers.'
            spark_version: '- (Required) Runtime version of the cluster. Any supported databricks_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.'
            ssh_public_keys: '- (Optional) SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.'
            state: '- (string) State of the cluster.'
            use_ml_runtime: '- (Optional, Boolean, only with kind) Whenever ML runtime should be selected or not.  Actual runtime is determined by spark_version (DBR release), this field use_ml_runtime, and whether node_type_id is GPU node or not.'
            workload_type.jobs: '- (Optional) boolean flag defining if it''s possible to run Databricks Jobs on this cluster. Default: true.'
            workload_type.notebooks: '- (Optional) boolean flag defining if it''s possible to run notebooks on this cluster. Default: true.'
        importStatements: []
    databricks_cluster_policy:
        subCategory: ""
        name: databricks_cluster_policy
        title: databricks_cluster_policy Resource
        examples:
            - name: fair_use
              manifest: |-
                {
                  "definition": "${jsonencode(merge(local.default_policy, var.policy_overrides))}",
                  "libraries": [
                    {
                      "pypi": [
                        {
                          "package": "databricks-sdk==0.12.0"
                        }
                      ]
                    },
                    {
                      "maven": [
                        {
                          "coordinates": "com.oracle.database.jdbc:ojdbc8:XXXX"
                        }
                      ]
                    }
                  ],
                  "name": "${var.team} cluster policy"
                }
              dependencies:
                databricks_permissions.can_use_cluster_policyinstance_profile: |-
                    {
                      "access_control": [
                        {
                          "group_name": "${var.team}",
                          "permission_level": "CAN_USE"
                        }
                      ],
                      "cluster_policy_id": "${databricks_cluster_policy.fair_use.id}"
                    }
            - name: personal_vm
              manifest: |-
                {
                  "name": "Personal Compute",
                  "policy_family_definition_overrides": "${jsonencode(local.personal_vm_override)}",
                  "policy_family_id": "personal-vm"
                }
        argumentDocs:
            Free form: policy and create fully-configurable clusters.
            definition: '- Policy definition: JSON document expressed in Databricks Policy Definition Language. Cannot be used with policy_family_id'
            description: '- (Optional) Additional human-readable description of the cluster policy.'
            id: '- Canonical unique identifier for the cluster policy. This is equal to policy_id.'
            max_clusters_per_user: '- (Optional, integer) Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.'
            name: '- the name of the built-in cluster policy.'
            policy_family_definition_overrides: '- settings to override in the built-in cluster policy.'
            policy_family_id: '- the ID of the cluster policy family used for built-in cluster policy.'
            policy_id: '- Canonical unique identifier for the cluster policy.'
            spark_version: parameter in databricks_cluster and other resources.
        importStatements: []
    databricks_compliance_security_profile_setting Resource:
        subCategory: ""
        name: databricks_compliance_security_profile_setting Resource
        title: databricks_compliance_security_profile_setting Resource
        argumentDocs:
            compliance_security_profile_workspace: 'block with following attributes:'
            compliance_standards: '- (Required, list of strings) Enable one or more compliance standards on the workspace, e.g. HIPAA, PCI_DSS, FEDRAMP_MODERATE, etc. (See Go SDK documentation for the full list of supported values).'
            is_enabled: '- (Required) Enable the Compliance Security Profile on the workspace'
        importStatements: []
    databricks_connection:
        subCategory: ""
        name: databricks_connection
        title: databricks_connection Resource
        examples:
            - name: mysql
              manifest: |-
                {
                  "comment": "this is a connection to mysql db",
                  "connection_type": "MYSQL",
                  "name": "mysql_connection",
                  "options": {
                    "host": "test.mysql.database.azure.com",
                    "password": "password",
                    "port": "3306",
                    "user": "user"
                  },
                  "properties": {
                    "purpose": "testing"
                  }
                }
            - name: bigquery
              manifest: |-
                {
                  "comment": "this is a connection to BQ",
                  "connection_type": "BIGQUERY",
                  "name": "bq_connection",
                  "options": {
                    "GoogleServiceAccountKeyJson": "${jsonencode({\n      \"type\" : \"service_account\",\n      \"project_id\" : \"PROJECT_ID\",\n      \"private_key_id\" : \"KEY_ID\",\n      \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\" : \"SERVICE_ACCOUNT_EMAIL\",\n      \"client_id\" : \"CLIENT_ID\",\n      \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\",\n      \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n      \"universe_domain\" : \"googleapis.com\"\n    })}"
                  },
                  "properties": {
                    "purpose": "testing"
                  }
                }
            - name: hms
              manifest: |-
                {
                  "comment": "This is a connection to builtin HMS",
                  "connection_type": "HIVE_METASTORE",
                  "name": "hms-builtin",
                  "options": {
                    "builtin": "true"
                  }
                }
            - name: http_bearer
              manifest: |-
                {
                  "comment": "This is a connection to a HTTP service",
                  "connection_type": "HTTP",
                  "name": "http_bearer",
                  "options": {
                    "base_path": "/api/",
                    "bearer_token": "bearer_token",
                    "host": "https://example.com",
                    "port": "8433"
                  }
                }
            - name: http_oauth
              manifest: |-
                {
                  "comment": "This is a connection to a HTTP service",
                  "connection_type": "HTTP",
                  "name": "http_oauth",
                  "options": {
                    "base_path": "/api/",
                    "client_id": "client_id",
                    "client_secret": "client_secret",
                    "host": "https://example.com",
                    "oauth_scope": "channels:read channels:history chat:write",
                    "port": "8433",
                    "token_endpoint": "https://authorization-server.com/oauth/token"
                  }
                }
            - name: pbi
              manifest: |-
                {
                  "connection_type": "POWER_BI",
                  "name": "test-pbi",
                  "options": {
                    "authorization_endpoint": "https://login.microsoftonline.com/{tenant}/oauth2/v2.0/authorize",
                    "client_id": "client_id",
                    "client_secret": "client_secret"
                  }
                }
        argumentDocs:
            comment: '- (Optional) Free-form text. Change forces creation of a new resource.'
            connection_id: '- Unique ID of the connection.'
            connection_type: '- Connection type. MYSQL, POSTGRESQL, SNOWFLAKE, REDSHIFT SQLDW, SQLSERVER, DATABRICKS, SALESFORCE, BIGQUERY, WORKDAY_RAAS, HIVE_METASTORE, GA4_RAW_DATA, SERVICENOW, SALESFORCE_DATA_CLOUD, GLUE, ORACLE, TERADATA, HTTP or POWER_BI are supported. Up-to-date list of connection type supported is in the documentation. Change forces creation of a new resource.'
            created_at: '- Time at which this connection was created, in epoch milliseconds.'
            created_by: '-  Username of connection creator.'
            credential_type: '- The type of credential for this connection.'
            full_name: '- Full name of connection.'
            id: '- ID of this connection in form of <metastore_id>|<name>.'
            metastore_id: '- Unique ID of the UC metastore for this connection.'
            name: '- Name of the Connection.'
            options: '- The key value of options required by the connection, e.g. host, port, user, password, authorization_endpoint, client_id, client_secret or GoogleServiceAccountKeyJson. Please consult the documentation for the required option.'
            owner: '- (Optional) Name of the connection owner.'
            properties: '-  (Optional) Free-form connection properties. Change forces creation of a new resource.'
            provisioning_info: '- Object with the status of an asynchronously provisioned resource.'
            read_only: '- (Optional) Indicates whether the connection is read-only. Change forces creation of a new resource.'
            updated_at: '- Time at which connection this was last modified, in epoch milliseconds.'
            updated_by: '- Username of user who last modified the connection.'
            url: '- URL of the remote data source, extracted from options.'
        importStatements: []
    databricks_credential:
        subCategory: ""
        name: databricks_credential
        title: databricks_credential Resource
        examples:
            - name: external
              manifest: |-
                {
                  "aws_iam_role": [
                    {
                      "role_arn": "${aws_iam_role.external_data_access.arn}"
                    }
                  ],
                  "comment": "Managed by TF",
                  "name": "${aws_iam_role.external_data_access.name}",
                  "purpose": "SERVICE"
                }
              references:
                aws_iam_role.role_arn: aws_iam_role.external_data_access.arn
                name: aws_iam_role.external_data_access.name
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "credential": "${databricks_credential.external.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "ACCESS"
                          ]
                        }
                      ]
                    }
            - name: external_mi
              manifest: |-
                {
                  "azure_managed_identity": [
                    {
                      "access_connector_id": "${azurerm_databricks_access_connector.example.id}"
                    }
                  ],
                  "comment": "Managed identity credential managed by TF",
                  "name": "mi_credential",
                  "purpose": "SERVICE"
                }
              references:
                azure_managed_identity.access_connector_id: azurerm_databricks_access_connector.example.id
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "credential": "${databricks_credential.external_mi.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "ACCESS"
                          ]
                        }
                      ]
                    }
            - name: external_gcp_sa
              manifest: |-
                {
                  "comment": "GCP SA credential managed by TF",
                  "databricks_gcp_service_account": [
                    {}
                  ],
                  "name": "gcp_sa_credential",
                  "purpose": "SERVICE"
                }
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "credential": "${databricks_credential.external_gcp_sa.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "ACCESS"
                          ]
                        }
                      ]
                    }
        argumentDocs:
            aws_iam_role.role_arn: '- The Amazon Resource Name (ARN) of the AWS IAM role you want to use to setup the trust policy, of the form arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF'
            azure_managed_identity.access_connector_id: '- The Resource ID of the Azure Databricks Access Connector resource, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name.'
            azure_managed_identity.managed_identity_id: '- (Optional) The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name.'
            azure_service_principal.application_id: '- The application ID of the application registration within the referenced AAD tenant'
            azure_service_principal.client_secret: '- The client secret generated for the above app ID in AAD. This field is redacted on output'
            azure_service_principal.directory_id: '- The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application'
            credential_id: '- Unique ID of the credential.'
            databricks_gcp_service_account.email: (output only) - The email of the GCP service account created, to be granted access to relevant buckets.
            force_destroy: '- (Optional) Delete credential regardless of its dependencies.'
            force_update: '- (Optional) Update credential regardless of its dependents.'
            id: '- ID of this credential - same as the name.'
            isolation_mode: '- (Optional) Whether the credential is accessible from all workspaces or a specific set of workspaces. Can be ISOLATION_MODE_ISOLATED or ISOLATION_MODE_OPEN. Setting the credential to ISOLATION_MODE_ISOLATED will automatically restrict access to only from the current workspace.'
            name: '- Name of Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the credential owner.'
            purpose: '- Indicates the purpose of the credential. Can be SERVICE or STORAGE.'
            read_only: '- (Optional) Indicates whether the credential is only usable for read operations. Only applicable when purpose is STORAGE.'
            skip_validation: '- (Optional) Suppress validation errors if any & force save the credential.'
        importStatements: []
    databricks_custom_app_integration:
        subCategory: ""
        name: databricks_custom_app_integration
        title: databricks_custom_app_integration Resource
        examples:
            - name: this
              manifest: |-
                {
                  "name": "custom_integration_name",
                  "redirect_urls": [
                    "https://example.com"
                  ],
                  "scopes": [
                    "all-apis"
                  ],
                  "token_access_policy": [
                    {
                      "access_token_ttl_in_minutes": 15,
                      "refresh_token_ttl_in_minutes": 30
                    }
                  ]
                }
        argumentDocs:
            access_token_ttl_in_minutes: '- access token time to live (TTL) in minutes.'
            client_id: '- OAuth client-id generated by Databricks'
            client_secret: '- OAuth client-secret generated by the Databricks if this is a confidential OAuth app.'
            confidential: '- Indicates whether an OAuth client secret is required to authenticate this client. Default to false. Change requires a new resource.'
            integration_id: '- Unique integration id for the custom OAuth app.'
            name: '- (Required) Name of the custom OAuth app. Change requires a new resource.'
            redirect_urls: '- List of OAuth redirect urls.'
            refresh_token_ttl_in_minutes: '- refresh token TTL in minutes. The TTL of refresh token cannot be lower than TTL of access token.'
            scopes: '- OAuth scopes granted to the application. Supported scopes: all-apis, sql, offline_access, openid, profile, email.'
        importStatements: []
    databricks_dashboard:
        subCategory: ""
        name: databricks_dashboard
        title: databricks_dashboard Resource
        examples:
            - name: dashboard
              manifest: |-
                {
                  "display_name": "New Dashboard",
                  "embed_credentials": false,
                  "parent_path": "/Shared/provider-test",
                  "serialized_dashboard": "{\"pages\":[{\"name\":\"new_name\",\"displayName\":\"New Page\"}]}",
                  "warehouse_id": "${data.databricks_sql_warehouse.starter.id}"
                }
              references:
                warehouse_id: data.databricks_sql_warehouse.starter.id
            - name: dashboard
              manifest: |-
                {
                  "display_name": "New Dashboard",
                  "embed_credentials": false,
                  "file_path": "${path.module}/dashboard.json",
                  "parent_path": "/Shared/provider-test",
                  "warehouse_id": "${data.databricks_sql_warehouse.starter.id}"
                }
              references:
                warehouse_id: data.databricks_sql_warehouse.starter.id
        argumentDocs:
            display_name: '- (Required) The display name of the dashboard.'
            embed_credentials: '- (Optional) Whether to embed credentials in the dashboard. Default is true.'
            file_path: '- (Optional) The path to the dashboard JSON file. Conflicts with serialized_dashboard.'
            id: '- The unique ID of the dashboard.'
            parent_path: '- (Required) The workspace path of the folder containing the dashboard. Includes leading slash and no trailing slash.  If folder doesn''t exist, it will be created.'
            serialized_dashboard: '- (Optional) The contents of the dashboard in serialized string form. Conflicts with file_path.'
            warehouse_id: '- (Required) The warehouse ID used to run the dashboard.'
        importStatements: []
    databricks_database_database_catalog:
        subCategory: ""
        name: databricks_database_database_catalog
        title: databricks_database_database_catalog Resource
        examples:
            - name: this
              manifest: |-
                {
                  "database_instance_name": "my-database-instance",
                  "database_name": "databricks_postgres",
                  "name": "my_registered_catalog"
                }
            - name: this
              manifest: |-
                {
                  "create_database_if_not_exists": true,
                  "database_instance_name": "my-database-instance",
                  "database_name": "new_registered_catalog_database",
                  "name": "my_registered_catalog"
                }
            - name: catalog
              manifest: |-
                {
                  "create_database_if_not_exists": true,
                  "database_instance_name": "${databricks_database_instance.instance.name}",
                  "database_name": "new_registered_catalog_database",
                  "name": "my_registered_catalog"
                }
              references:
                database_instance_name: databricks_database_instance.instance.name
              dependencies:
                databricks_database_instance.instance: |-
                    {
                      "capacity": "CU_1",
                      "name": "my-database-instance"
                    }
        argumentDocs:
            create_database_if_not_exists: (boolean, optional)
            database_instance_name: (string, required) - The name of the DatabaseInstance housing the database
            database_name: (string, required) - The name of the database (in a instance) associated with the catalog
            name: (string, required) - The name of the catalog in UC
            uid: (string)
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_database_instance:
        subCategory: ""
        name: databricks_database_instance
        title: databricks_database_instance Resource
        examples:
            - name: this
              manifest: |-
                {
                  "capacity": "CU_2",
                  "name": "my-database-instance"
                }
            - name: this
              manifest: |-
                {
                  "capacity": "CU_2",
                  "enable_readable_secondaries": true,
                  "name": "my-database-instance",
                  "node_count": 2
                }
            - name: child
              manifest: |-
                {
                  "capacity": "CU_2",
                  "name": "my-database-instance",
                  "parent_instance_ref": {
                    "name": "my-parent-instance"
                  }
                }
        argumentDocs:
            branch_time: |-
                (string, optional) - Branch time of the ref database instance.
                For a parent ref instance, this is the point in time on the parent instance from which the
                instance was created.
                For a child ref instance, this is the point in time on the instance from which the child
                instance was created.
                Input: For specifying the point in time to create a child instance. Optional.
                Output: Only populated if provided as input to create a child instance
            capacity: (string, optional) - The sku of the instance. Valid values are "CU_1", "CU_2", "CU_4", "CU_8"
            child_instance_refs: |-
                (list of DatabaseInstanceRef) - The refs of the child instances. This is only available if the instance is
                parent instance
            creation_time: (string) - The timestamp when the instance was created
            creator: (string) - The email of the creator of the instance
            effective_enable_pg_native_login: |-
                (boolean) - xref AIP-129. enable_pg_native_login is owned by the client, while effective_enable_pg_native_login is owned by the server.
                enable_pg_native_login will only be set in Create/Update response messages if and only if the user provides the field via the request.
                effective_enable_pg_native_login on the other hand will always bet set in all response messages (Create/Update/Get/List)
            effective_enable_readable_secondaries: |-
                (boolean) - xref AIP-129. enable_readable_secondaries is owned by the client, while effective_enable_readable_secondaries is owned by the server.
                enable_readable_secondaries will only be set in Create/Update response messages if and only if the user provides the field via the request.
                effective_enable_readable_secondaries on the other hand will always bet set in all response messages (Create/Update/Get/List)
            effective_lsn: |-
                (string) - xref AIP-129. lsn is owned by the client, while effective_lsn is owned by the server.
                lsn will only be set in Create/Update response messages if and only if the user provides the field via the request.
                effective_lsn on the other hand will always bet set in all response messages (Create/Update/Get/List).
                For a parent ref instance, this is the LSN on the parent instance from which the
                instance was created.
                For a child ref instance, this is the LSN on the instance from which the child instance
                was created
            effective_node_count: |-
                (integer) - xref AIP-129. node_count is owned by the client, while effective_node_count is owned by the server.
                node_count will only be set in Create/Update response messages if and only if the user provides the field via the request.
                effective_node_count on the other hand will always bet set in all response messages (Create/Update/Get/List)
            effective_retention_window_in_days: |-
                (integer) - xref AIP-129. retention_window_in_days is owned by the client, while effective_retention_window_in_days is owned by the server.
                retention_window_in_days will only be set in Create/Update response messages if and only if the user provides the field via the request.
                effective_retention_window_in_days on the other hand will always bet set in all response messages (Create/Update/Get/List)
            effective_stopped: |-
                (boolean) - xref AIP-129. stopped is owned by the client, while effective_stopped is owned by the server.
                stopped will only be set in Create/Update response messages if and only if the user provides the field via the request.
                effective_stopped on the other hand will always bet set in all response messages (Create/Update/Get/List)
            enable_pg_native_login: (boolean, optional) - Whether the instance has PG native password login enabled. Defaults to true
            enable_readable_secondaries: (boolean, optional) - Whether to enable secondaries to serve read-only traffic. Defaults to false
            lsn: (string, optional) - User-specified WAL LSN of the ref database instance.
            name: (string, required) - The name of the instance. This is the unique identifier for the instance
            node_count: |-
                (integer, optional) - The number of nodes in the instance, composed of 1 primary and 0 or more secondaries. Defaults to
                1 primary and 0 secondaries
            parent_instance_ref: |-
                (DatabaseInstanceRef, optional) - The ref of the parent instance. This is only available if the instance is
                child instance.
                Input: For specifying the parent instance to create a child instance. Optional.
                Output: Only populated if provided as input to create a child instance
            pg_version: (string) - The version of Postgres running on the instance
            purge_on_delete: (boolean, optional) - Purge the resource on delete
            read_only_dns: |-
                (string) - The DNS endpoint to connect to the instance for read only access. This is only available if
                enable_readable_secondaries is true
            read_write_dns: (string) - The DNS endpoint to connect to the instance for read+write access
            retention_window_in_days: |-
                (integer, optional) - The retention window for the instance. This is the time window in days
                for which the historical data is retained. The default value is 7 days.
                Valid values are 2 to 35 days
            state: '(string) - The current state of the instance. Possible values are: AVAILABLE, DELETING, FAILING_OVER, STARTING, STOPPED, UPDATING'
            stopped: (boolean, optional) - Whether the instance is stopped
            uid: (string) - An immutable UUID identifier for the instance
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_database_synced_database_table:
        subCategory: ""
        name: databricks_database_synced_database_table
        title: databricks_database_synced_database_table Resource
        examples:
            - name: this
              manifest: |-
                {
                  "logical_database_name": "databricks_postgres",
                  "name": "my_database_catalog.public.synced_table",
                  "spec": {
                    "create_database_objects_if_missing": true,
                    "new_pipeline_spec": {
                      "storage_catalog": "source_delta",
                      "storage_schema": "tpch"
                    },
                    "primary_key_columns": [
                      "c_custkey"
                    ],
                    "scheduling_policy": "SNAPSHOT",
                    "source_table_full_name": "source_delta.tpch.customer"
                  }
                }
            - name: this
              manifest: |-
                {
                  "database_instance_name": "my-database-instance",
                  "logical_database_name": "databricks_postgres",
                  "name": "my_standard_catalog.public.synced_table",
                  "spec": {
                    "create_database_objects_if_missing": true,
                    "new_pipeline_spec": {
                      "storage_catalog": "source_delta",
                      "storage_schema": "tpch"
                    },
                    "primary_key_columns": [
                      "c_custkey"
                    ],
                    "scheduling_policy": "SNAPSHOT",
                    "source_table_full_name": "source_delta.tpch.customer"
                  }
                }
            - name: synced_table_1
              manifest: |-
                {
                  "database_instance_name": "${databricks_database_instance.instance.name}",
                  "logical_database_name": "databricks_postgres",
                  "name": "my_standard_catalog.public.synced_table1",
                  "spec": {
                    "create_database_objects_if_missing": true,
                    "new_pipeline_spec": {
                      "storage_catalog": "source_delta",
                      "storage_schema": "tpch"
                    },
                    "primary_key_columns": [
                      "c_custkey"
                    ],
                    "scheduling_policy": "SNAPSHOT",
                    "source_table_full_name": "source_delta.tpch.customer"
                  }
                }
              references:
                database_instance_name: databricks_database_instance.instance.name
              dependencies:
                databricks_database_instance.instance: |-
                    {
                      "capacity": "CU_1",
                      "name": "my-database-instance"
                    }
            - name: synced_table_2
              manifest: |-
                {
                  "database_instance_name": "${databricks_database_instance.instance.name}",
                  "logical_database_name": "databricks_postgres",
                  "name": "my_standard_catalog.public.synced_table2",
                  "spec": {
                    "create_database_objects_if_missing": true,
                    "existing_pipeline_id": "${databricks_database_synced_database_table.synced_table_1.data_synchronization_status.pipeline_id}",
                    "primary_key_columns": [
                      "c_custkey"
                    ],
                    "scheduling_policy": "SNAPSHOT",
                    "source_table_full_name": "source_delta.tpch.customer"
                  }
                }
              references:
                database_instance_name: databricks_database_instance.instance.name
              dependencies:
                databricks_database_instance.instance: |-
                    {
                      "capacity": "CU_1",
                      "name": "my-database-instance"
                    }
        argumentDocs:
            continuous_update_status: (SyncedTableContinuousUpdateStatus, optional)
            create_database_objects_if_missing: |-
                (boolean, optional) - If true, the synced table's logical database and schema resources in PG
                will be created if they do not already exist
            create_database_objects_is_missing: field in spec
            data_synchronization_status: (SyncedTableStatus) - Synced Table data synchronization status
            database_instance_name: |-
                (string, optional) - Name of the target database instance. This is required when creating synced database tables in standard catalogs.
                This is optional when creating synced database tables in registered catalogs. If this field is specified
                when creating synced database tables in registered catalogs, the database instance name MUST
                match that of the registered catalog (or the request will be rejected)
            delta_commit_timestamp: |-
                (string) - The timestamp when the above Delta version was committed in the source Delta table.
                Note: This is the Delta commit time, not the time the data was written to the synced table
            delta_commit_version: (integer) - The Delta Lake commit version that was last successfully synced
            delta_table_sync_info: (DeltaTableSyncInfo)
            detailed_state: '(string) - The state of the synced table. Possible values are: SYNCED_TABLED_OFFLINE, SYNCED_TABLE_OFFLINE_FAILED, SYNCED_TABLE_ONLINE, SYNCED_TABLE_ONLINE_CONTINUOUS_UPDATE, SYNCED_TABLE_ONLINE_NO_PENDING_UPDATE, SYNCED_TABLE_ONLINE_PIPELINE_FAILED, SYNCED_TABLE_ONLINE_TRIGGERED_UPDATE, SYNCED_TABLE_ONLINE_UPDATING_PIPELINE_RESOURCES, SYNCED_TABLE_PROVISIONING, SYNCED_TABLE_PROVISIONING_INITIAL_SNAPSHOT, SYNCED_TABLE_PROVISIONING_PIPELINE_RESOURCES'
            effective_database_instance_name: |-
                (string) - The name of the database instance that this table is registered to. This field is always returned, and for
                tables inside database catalogs is inferred database instance associated with the catalog
            effective_logical_database_name: (string) - The name of the logical database that this table is registered to
            estimated_completion_time_seconds: (number) - The estimated time remaining to complete this update in seconds
            existing_pipeline_id: (string, optional) - At most one of existing_pipeline_id and new_pipeline_spec should be defined.
            failed_status: (SyncedTableFailedStatus, optional)
            initial_pipeline_sync_progress: (SyncedTablePipelineProgress) - Progress of the initial data synchronization
            last_processed_commit_version: (integer) - The last source table Delta version that was successfully synced to the synced table
            last_sync: (SyncedTablePosition) - Summary of the last successful synchronization from source to destination.
            latest_version_currently_processing: |-
                (integer) - The source table Delta version that was last processed by the pipeline. The pipeline may not
                have completely processed this version yet
            logical_database_name: (string, optional) - Target Postgres database object (logical database) name for this table.
            message: (string) - A text description of the current state of the synced table
            name: (string, required) - Full three-part (catalog, schema, table) name of the table
            new_pipeline_spec: (NewPipelineSpec, optional) - At most one of existing_pipeline_id and new_pipeline_spec should be defined.
            pipeline_id: |-
                (string) - ID of the associated pipeline. The pipeline ID may have been provided by the client
                (in the case of bin packing), or generated by the server (when creating a new pipeline)
            primary_key_columns: (list of string, optional) - Primary Key columns to be used for data insert/update in the destination
            provisioning_phase: '(string) - The current phase of the data synchronization pipeline. Possible values are: PROVISIONING_PHASE_INDEX_SCAN, PROVISIONING_PHASE_INDEX_SORT, PROVISIONING_PHASE_MAIN'
            provisioning_status: (SyncedTableProvisioningStatus, optional)
            scheduling_policy: '(string, optional) - Scheduling policy of the underlying pipeline. Possible values are: CONTINUOUS, SNAPSHOT, TRIGGERED'
            source_table_full_name: (string, optional) - Three-part (catalog, schema, table) name of the source Delta table
            spec: (SyncedTableSpec, optional)
            storage_catalog: (string, optional) - This field needs to be specified if the destination catalog is a managed postgres catalog.
            storage_schema: (string, optional) - This field needs to be specified if the destination catalog is a managed postgres catalog.
            sync_end_timestamp: |-
                (string) - The end timestamp of the most recent successful synchronization.
                This is the time when the data is available in the synced table
            sync_progress_completion: (number) - The completion ratio of this update. This is a number between 0 and 1
            sync_start_timestamp: |-
                (string) - The starting timestamp of the most recent successful synchronization from the source table
                to the destination (synced) table.
                Note this is the starting timestamp of the sync operation, not the end time.
                E.g., for a batch, this is the time when the sync operation started
            synced_row_count: (integer) - The number of rows that have been synced in this update
            timeseries_key: (string, optional) - Time series key to deduplicate (tie-break) rows with the same primary key
            timestamp: |-
                (string) - The end timestamp of the last time any data was synchronized from the source table to the synced
                table. This is when the data is available in the synced table
            total_row_count: (integer) - The total number of rows that need to be synced in this update. This number may be an estimate
            triggered_update_progress: (SyncedTablePipelineProgress) - Progress of the active data synchronization pipeline
            triggered_update_status: (SyncedTableTriggeredUpdateStatus, optional)
            unity_catalog_provisioning_state: |-
                (string) - The provisioning state of the synced table entity in Unity Catalog. This is distinct from the
                state of the data synchronization pipeline (i.e. the table may be in "ACTIVE" but the pipeline
                may be in "PROVISIONING" as it runs asynchronously). Possible values are: ACTIVE, DEGRADED, DELETING, FAILED, PROVISIONING, UPDATING
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_database_synced_database_table:
        subCategory: ""
        name: databricks_database_synced_database_table
        title: databricks_database_synced_database_table Resource
        examples:
            - name: this
              manifest: |-
                {
                  "logical_database_name": "databricks_postgres",
                  "name": "my_database_catalog.public.synced_table",
                  "spec": {
                    "create_database_objects_if_missing": true,
                    "new_pipeline_spec": {
                      "storage_catalog": "source_delta",
                      "storage_schema": "tpch"
                    },
                    "primary_key_columns": [
                      "c_custkey"
                    ],
                    "scheduling_policy": "SNAPSHOT",
                    "source_table_full_name": "source_delta.tpch.customer"
                  }
                }
            - name: this
              manifest: |-
                {
                  "database_instance_name": "my-database-instance",
                  "logical_database_name": "databricks_postgres",
                  "name": "my_standard_catalog.public.synced_table",
                  "spec": {
                    "create_database_objects_if_missing": true,
                    "new_pipeline_spec": {
                      "storage_catalog": "source_delta",
                      "storage_schema": "tpch"
                    },
                    "primary_key_columns": [
                      "c_custkey"
                    ],
                    "scheduling_policy": "SNAPSHOT",
                    "source_table_full_name": "source_delta.tpch.customer"
                  }
                }
            - name: synced_table_1
              manifest: |-
                {
                  "database_instance_name": "${databricks_database_instance.instance.name}",
                  "logical_database_name": "databricks_postgres",
                  "name": "my_standard_catalog.public.synced_table1",
                  "spec": {
                    "create_database_objects_if_missing": true,
                    "new_pipeline_spec": {
                      "storage_catalog": "source_delta",
                      "storage_schema": "tpch"
                    },
                    "primary_key_columns": [
                      "c_custkey"
                    ],
                    "scheduling_policy": "SNAPSHOT",
                    "source_table_full_name": "source_delta.tpch.customer"
                  }
                }
              references:
                database_instance_name: databricks_database_instance.instance.name
              dependencies:
                databricks_database_instance.instance: |-
                    {
                      "capacity": "CU_1",
                      "name": "my-database-instance"
                    }
            - name: synced_table_2
              manifest: |-
                {
                  "database_instance_name": "${databricks_database_instance.instance.name}",
                  "logical_database_name": "databricks_postgres",
                  "name": "my_standard_catalog.public.synced_table2",
                  "spec": {
                    "create_database_objects_if_missing": true,
                    "existing_pipeline_id": "${databricks_database_synced_database_table.synced_table_1.data_synchronization_status.pipeline_id}",
                    "primary_key_columns": [
                      "c_custkey"
                    ],
                    "scheduling_policy": "SNAPSHOT",
                    "source_table_full_name": "source_delta.tpch.customer"
                  }
                }
              references:
                database_instance_name: databricks_database_instance.instance.name
              dependencies:
                databricks_database_instance.instance: |-
                    {
                      "capacity": "CU_1",
                      "name": "my-database-instance"
                    }
        argumentDocs:
            continuous_update_status: (SyncedTableContinuousUpdateStatus, optional)
            create_database_objects_if_missing: |-
                (boolean, optional) - If true, the synced table's logical database and schema resources in PG
                will be created if they do not already exist
            create_database_objects_is_missing: field in spec
            data_synchronization_status: (SyncedTableStatus) - Synced Table data synchronization status
            database_instance_name: |-
                (string, optional) - Name of the target database instance. This is required when creating synced database tables in standard catalogs.
                This is optional when creating synced database tables in registered catalogs. If this field is specified
                when creating synced database tables in registered catalogs, the database instance name MUST
                match that of the registered catalog (or the request will be rejected)
            delta_commit_timestamp: |-
                (string) - The timestamp when the above Delta version was committed in the source Delta table.
                Note: This is the Delta commit time, not the time the data was written to the synced table
            delta_commit_version: (integer) - The Delta Lake commit version that was last successfully synced
            delta_table_sync_info: (DeltaTableSyncInfo)
            detailed_state: '(string) - The state of the synced table. Possible values are: SYNCED_TABLED_OFFLINE, SYNCED_TABLE_OFFLINE_FAILED, SYNCED_TABLE_ONLINE, SYNCED_TABLE_ONLINE_CONTINUOUS_UPDATE, SYNCED_TABLE_ONLINE_NO_PENDING_UPDATE, SYNCED_TABLE_ONLINE_PIPELINE_FAILED, SYNCED_TABLE_ONLINE_TRIGGERED_UPDATE, SYNCED_TABLE_ONLINE_UPDATING_PIPELINE_RESOURCES, SYNCED_TABLE_PROVISIONING, SYNCED_TABLE_PROVISIONING_INITIAL_SNAPSHOT, SYNCED_TABLE_PROVISIONING_PIPELINE_RESOURCES'
            effective_database_instance_name: |-
                (string) - The name of the database instance that this table is registered to. This field is always returned, and for
                tables inside database catalogs is inferred database instance associated with the catalog
            effective_logical_database_name: (string) - The name of the logical database that this table is registered to
            estimated_completion_time_seconds: (number) - The estimated time remaining to complete this update in seconds
            existing_pipeline_id: (string, optional) - At most one of existing_pipeline_id and new_pipeline_spec should be defined.
            failed_status: (SyncedTableFailedStatus, optional)
            initial_pipeline_sync_progress: (SyncedTablePipelineProgress) - Progress of the initial data synchronization
            last_processed_commit_version: (integer) - The last source table Delta version that was successfully synced to the synced table
            last_sync: (SyncedTablePosition) - Summary of the last successful synchronization from source to destination.
            latest_version_currently_processing: |-
                (integer) - The source table Delta version that was last processed by the pipeline. The pipeline may not
                have completely processed this version yet
            logical_database_name: (string, optional) - Target Postgres database object (logical database) name for this table.
            message: (string) - A text description of the current state of the synced table
            name: (string, required) - Full three-part (catalog, schema, table) name of the table
            new_pipeline_spec: (NewPipelineSpec, optional) - At most one of existing_pipeline_id and new_pipeline_spec should be defined.
            pipeline_id: |-
                (string) - ID of the associated pipeline. The pipeline ID may have been provided by the client
                (in the case of bin packing), or generated by the server (when creating a new pipeline)
            primary_key_columns: (list of string, optional) - Primary Key columns to be used for data insert/update in the destination
            provisioning_phase: '(string) - The current phase of the data synchronization pipeline. Possible values are: PROVISIONING_PHASE_INDEX_SCAN, PROVISIONING_PHASE_INDEX_SORT, PROVISIONING_PHASE_MAIN'
            provisioning_status: (SyncedTableProvisioningStatus, optional)
            scheduling_policy: '(string, optional) - Scheduling policy of the underlying pipeline. Possible values are: CONTINUOUS, SNAPSHOT, TRIGGERED'
            source_table_full_name: (string, optional) - Three-part (catalog, schema, table) name of the source Delta table
            spec: (SyncedTableSpec, optional)
            storage_catalog: (string, optional) - This field needs to be specified if the destination catalog is a managed postgres catalog.
            storage_schema: (string, optional) - This field needs to be specified if the destination catalog is a managed postgres catalog.
            sync_end_timestamp: |-
                (string) - The end timestamp of the most recent successful synchronization.
                This is the time when the data is available in the synced table
            sync_progress_completion: (number) - The completion ratio of this update. This is a number between 0 and 1
            sync_start_timestamp: |-
                (string) - The starting timestamp of the most recent successful synchronization from the source table
                to the destination (synced) table.
                Note this is the starting timestamp of the sync operation, not the end time.
                E.g., for a batch, this is the time when the sync operation started
            synced_row_count: (integer) - The number of rows that have been synced in this update
            timeseries_key: (string, optional) - Time series key to deduplicate (tie-break) rows with the same primary key
            timestamp: |-
                (string) - The end timestamp of the last time any data was synchronized from the source table to the synced
                table. This is when the data is available in the synced table
            total_row_count: (integer) - The total number of rows that need to be synced in this update. This number may be an estimate
            triggered_update_progress: (SyncedTablePipelineProgress) - Progress of the active data synchronization pipeline
            triggered_update_status: (SyncedTableTriggeredUpdateStatus, optional)
            unity_catalog_provisioning_state: |-
                (string) - The provisioning state of the synced table entity in Unity Catalog. This is distinct from the
                state of the data synchronization pipeline (i.e. the table may be in "ACTIVE" but the pipeline
                may be in "PROVISIONING" as it runs asynchronously). Possible values are: ACTIVE, DEGRADED, DELETING, FAILED, PROVISIONING, UPDATING
        importStatements: []
    databricks_dbfs_file:
        subCategory: ""
        name: databricks_dbfs_file
        title: databricks_dbfs_file Resource
        examples:
            - name: this
              manifest: |-
                {
                  "path": "/tmp/main.tf",
                  "source": "${path.module}/main.tf"
                }
            - name: this
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    Hello, world!\n    Module is ${abspath(path.module)}\n    EOT\n  )}",
                  "path": "/tmp/this.txt"
                }
            - name: app
              manifest: |-
                {
                  "path": "/FileStore/baz.whl",
                  "source": "${path.module}/baz.whl"
                }
              dependencies:
                databricks_library.app: |-
                    {
                      "cluster_id": "${each.key}",
                      "for_each": "${data.databricks_clusters.all.ids}",
                      "whl": "${databricks_dbfs_file.app.dbfs_path}"
                    }
        argumentDocs:
            content_base64: '- Encoded file contents. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a data pipeline configuration file.'
            dbfs:/mnt/name: .
            dbfs_path: '- Path, but with dbfs: prefix.'
            file_size: '- The file size of the file that is being tracked by this resource in bytes.'
            id: '- Same as path.'
            path: '- (Required) The path of the file in which you wish to save.'
            source: '- The full absolute path to the file. Conflicts with content_base64.'
        importStatements: []
    databricks_default_namespace_setting:
        subCategory: ""
        name: databricks_default_namespace_setting
        title: databricks_default_namespace_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "namespace": [
                    {
                      "value": "namespace_value"
                    }
                  ]
                }
        argumentDocs:
            namespace: '- (Required) The configuration details.'
            value: '- (Required) The value for the setting.'
        importStatements: []
    databricks_directory:
        subCategory: ""
        name: databricks_directory
        title: databricks_directory Resource
        examples:
            - name: my_custom_directory
              manifest: |-
                {
                  "path": "/my_custom_directory"
                }
        argumentDocs:
            delete_recursive: '- Whether or not to trigger a recursive delete of this directory and its resources when deleting this on Terraform. Defaults to false'
            id: '- Path of directory on workspace'
            object_id: '- Unique identifier for a DIRECTORY'
            path: '- (Required) The absolute path of the directory, beginning with "/", e.g. "/Demo".'
            spark_version: parameter in databricks_cluster and other resources.
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_disable_legacy_access_setting:
        subCategory: ""
        name: databricks_disable_legacy_access_setting
        title: databricks_disable_legacy_access_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "disable_legacy_access": [
                    {
                      "value": true
                    }
                  ]
                }
        argumentDocs:
            disable_legacy_access: '- (Required) The configuration details.'
            value: '- (Required) The boolean value for the setting.'
        importStatements: []
    databricks_disable_legacy_dbfs_setting:
        subCategory: ""
        name: databricks_disable_legacy_dbfs_setting
        title: databricks_disable_legacy_dbfs_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "disable_legacy_dbfs": [
                    {
                      "value": true
                    }
                  ]
                }
        argumentDocs:
            disable_legacy_dbfs: 'block with following attributes:'
            value: '- (Required) The boolean value for the setting.'
        importStatements: []
    databricks_disable_legacy_features_setting:
        subCategory: ""
        name: databricks_disable_legacy_features_setting
        title: databricks_disable_legacy_features_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "disable_legacy_features": [
                    {
                      "value": true
                    }
                  ]
                }
              dependencies:
                databricks_default_namespace_setting.this: |-
                    {
                      "namespace": [
                        {
                          "value": "default_catalog"
                        }
                      ]
                    }
        argumentDocs:
            disable_legacy_features: 'block with following attributes:'
            value: '- (Required) The boolean value for the setting.'
        importStatements: []
    databricks_enhanced_security_monitoring_setting Resource:
        subCategory: ""
        name: databricks_enhanced_security_monitoring_setting Resource
        title: databricks_enhanced_security_monitoring_setting Resource
        argumentDocs:
            enhanced_security_monitoring_workspace: 'block with following attributes:'
            is_enabled: '- (Required) Enable the Enhanced Security Monitoring on the workspace'
        importStatements: []
    databricks_entitlements:
        subCategory: ""
        name: databricks_entitlements
        title: databricks_entitlements Resource
        examples:
            - name: me
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "user_id": "${data.databricks_user.me.id}"
                }
              references:
                user_id: data.databricks_user.me.id
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "service_principal_id": "${data.databricks_service_principal.this.sp_id}"
                }
              references:
                service_principal_id: data.databricks_service_principal.this.sp_id
            - name: workspace-users
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "group_id": "${data.databricks_group.users.id}"
                }
              references:
                group_id: data.databricks_group.users.id
        argumentDocs:
            allow_cluster_create: '-  (Optional) Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the principal to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            group/group_id: '- group group_id.'
            group_id: '- Canonical unique identifier for the group.'
            service_principal_id: '- Canonical unique identifier for the service principal.'
            spn/spn_id: '- service principal spn_id.'
            user/user_id: '- user user_id.'
            user_id: '-  Canonical unique identifier for the user.'
            workspace_access: '- (Optional) This is a field to allow the principal to have access to a Databricks Workspace.'
            workspace_consume: '- (Optional) This is a field to allow the principal to have access to a Databricks Workspace as consumer, with limited access to workspace UI.  Couldn''t be used with workspace_access or databricks_sql_access.'
        importStatements: []
    databricks_entity_tag_assignment Resource:
        subCategory: ""
        name: databricks_entity_tag_assignment Resource
        title: databricks_entity_tag_assignment Resource
        argumentDocs:
            entity_name: (string, required) - The fully qualified name of the entity to which the tag is assigned
            entity_type: '(string, required) - The type of the entity to which the tag is assigned. Allowed values are: catalogs, schemas, tables, columns, volumes'
            tag_key: (string, required) - The key of the tag
            tag_value: (string, optional) - The value of the tag
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_external_location:
        subCategory: ""
        name: databricks_external_location
        title: databricks_external_location Resource
        examples:
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.external.id}",
                  "name": "external",
                  "url": "s3://${aws_s3_bucket.external.id}/some"
                }
              references:
                credential_name: databricks_storage_credential.external.id
              dependencies:
                databricks_grants.some: |-
                    {
                      "external_location": "${databricks_external_location.some.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE",
                            "READ_FILES"
                          ]
                        }
                      ]
                    }
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.external.id}",
                  "depends_on": [
                    "${databricks_metastore_assignment.this}"
                  ],
                  "name": "external",
                  "url": "${format(\"abfss://%s@%s.dfs.core.windows.net\",\n    azurerm_storage_container.ext_storage.name,\n  azurerm_storage_account.ext_storage.name)}"
                }
              references:
                credential_name: databricks_storage_credential.external.id
              dependencies:
                databricks_grants.some: |-
                    {
                      "external_location": "${databricks_external_location.some.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE",
                            "READ_FILES"
                          ]
                        }
                      ]
                    }
                databricks_storage_credential.external: |-
                    {
                      "azure_service_principal": [
                        {
                          "application_id": "${azuread_application.ext_cred.application_id}",
                          "client_secret": "${azuread_application_password.ext_cred.value}",
                          "directory_id": "${var.tenant_id}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "depends_on": [
                        "${databricks_metastore_assignment.this}"
                      ],
                      "name": "${azuread_application.ext_cred.display_name}"
                    }
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.ext.id}",
                  "name": "the-ext-location",
                  "url": "gs://${google_storage_bucket.ext_bucket.name}"
                }
              references:
                credential_name: databricks_storage_credential.ext.id
              dependencies:
                databricks_grants.some: |-
                    {
                      "external_location": "${databricks_external_location.some.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE",
                            "READ_FILES"
                          ]
                        }
                      ]
                    }
                databricks_storage_credential.ext: |-
                    {
                      "databricks_gcp_service_account": [
                        {}
                      ],
                      "name": "the-creds"
                    }
        argumentDocs:
            access_point: '- (Optional) The ARN of the s3 access point to use with the external location (AWS).'
            algorithm: '- Encryption algorithm value. Sets the value of the x-amz-server-side-encryption header in S3 request.'
            aws_kms_key_arn: '- Optional ARN of the SSE-KMS key used with the S3 location, when algorithm = "SSE-KMS". Sets the value of the x-amz-server-side-encryption-aws-kms-key-id header.'
            comment: '- (Optional) User-supplied free-form text.'
            created_at: '- Time at which this external location was created, in epoch milliseconds.'
            created_by: '-  Username of external location creator.'
            credential_id: '- Unique ID of the location''s storage credential.'
            credential_name: '- Name of the databricks_storage_credential to use with this external location.'
            databricks_external_location: are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.
            enable_file_events: '- (Optional) indicates if managed file events are enabled for this external location.  Requires file_event_queue block.'
            fallback: '- (Optional) Indicates whether fallback mode is enabled for this external location. When fallback mode is enabled (disabled by default), the access to the location falls back to cluster credentials if UC credentials are not sufficient.'
            file_event_queue.managed_aqs: '- (Optional) Configuration for managed Azure Queue Storage queue.'
            file_event_queue.managed_pubsub: '- (Optional) Configuration for managed Google Cloud Pub/Sub queue.'
            file_event_queue.managed_sqs: '- (Optional) Configuration for managed Amazon SQS queue.'
            file_event_queue.provided_aqs: '- (Optional) Configuration for provided Azure Storage Queue.'
            file_event_queue.provided_pubsub: '- (Optional) Configuration for provided Google Cloud Pub/Sub queue.'
            file_event_queue.provided_sqs: '- (Optional) Configuration for provided Amazon SQS queue.'
            force_destroy: '- (Optional) Destroy external location regardless of its dependents.'
            force_update: '- (Optional) Update external location regardless of its dependents.'
            id: '- ID of this external location - same as name.'
            isolation_mode: '- (Optional) Whether the external location is accessible from all workspaces or a specific set of workspaces. Can be ISOLATION_MODE_ISOLATED or ISOLATION_MODE_OPEN. Setting the external location to ISOLATION_MODE_ISOLATED will automatically allow access from the current workspace.'
            managed_resource_id: '- (Computed) The ID of the managed resource.'
            name: '- Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the external location owner.'
            queue_url: '- (Required) The URL of the queue.'
            read_only: '- (Optional) Indicates whether the external location is read-only.'
            resource_group: '- (Required) The Azure resource group.'
            skip_validation: '- (Optional) Suppress validation errors if any & force save the external location'
            sse_encryption_details: '- a block describing server-Side Encryption properties for clients communicating with AWS S3. Consists of the following attributes:'
            subscription_id: '- (Required) The Azure subscription ID.'
            subscription_name: '- (Required) The name of the subscription.'
            updated_at: '- Time at which external location this was last modified, in epoch milliseconds.'
            updated_by: '- Username of user who last modified the external location.'
            url: '- Path URL in cloud storage, of the form: s3://[bucket-host]/[bucket-dir] (AWS), abfss://[user]@[host]/[path] (Azure), gs://[bucket-host]/[bucket-dir] (GCP).'
        importStatements: []
    databricks_external_metadata:
        subCategory: ""
        name: databricks_external_metadata
        title: databricks_external_metadata Resource
        examples:
            - name: this
              manifest: |-
                {
                  "columns": [
                    "type",
                    "message",
                    "details",
                    "date",
                    "time"
                  ],
                  "description": "A stream of security related events in the critical services.",
                  "entity_type": "Topic",
                  "name": "security_events_stream",
                  "properties": {
                    "compression.enabled": "true",
                    "compression.format": "zstd",
                    "topic": "prod.security.events.raw"
                  },
                  "system_type": "KAFKA",
                  "url": "https://kafka.com/12345"
                }
        argumentDocs:
            columns: (list of string, optional) - List of columns associated with the external metadata object
            create_time: (string) - Time at which this external metadata object was created
            created_by: (string) - Username of external metadata object creator
            description: (string, optional) - User-provided free-form text description
            entity_type: (string, required) - Type of entity within the external system
            id: (string) - Unique identifier of the external metadata object
            metastore_id: (string) - Unique identifier of parent metastore
            name: (string, required) - Name of the external metadata object
            owner: (string, optional) - Owner of the external metadata object
            properties: (object, optional) - A map of key-value properties attached to the external metadata object
            system_type: '(string, required) - Type of external system. Possible values are: AMAZON_REDSHIFT, AZURE_SYNAPSE, CONFLUENT, DATABRICKS, GOOGLE_BIGQUERY, KAFKA, LOOKER, MICROSOFT_FABRIC, MICROSOFT_SQL_SERVER, MONGODB, MYSQL, ORACLE, OTHER, POSTGRESQL, POWER_BI, SALESFORCE, SAP, SERVICENOW, SNOWFLAKE, TABLEAU, TERADATA, WORKDAY'
            update_time: (string) - Time at which this external metadata object was last modified
            updated_by: (string) - Username of user who last modified external metadata object
            url: (string, optional) - URL associated with the external metadata object
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_file:
        subCategory: ""
        name: databricks_file
        title: databricks_file Resource
        examples:
            - name: this
              manifest: |-
                {
                  "path": "${databricks_volume.this.volume_path}/fileName",
                  "source": "/full/path/on/local/system"
                }
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "metastore_id": "${databricks_metastore.this.id}",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "volume_type": "MANAGED"
                    }
            - name: init_script
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"Hello World\"\n    EOT\n  )}",
                  "path": "${databricks_volume.this.volume_path}/fileName"
                }
        argumentDocs:
            content_base64: '- Contents in base 64 format. Conflicts with source.'
            file_size: '- The file size of the file that is being tracked by this resource in bytes.'
            id: '- Same as path.'
            modification_time: '- The last time stamp when the file was modified'
            path: '- The path of the file in which you wish to save. For example, /Volumes/main/default/volume1/file.txt.'
            source: '- The full absolute path to the file. Conflicts with content_base64.'
        importStatements: []
    databricks_git_credential:
        subCategory: ""
        name: databricks_git_credential
        title: databricks_git_credential Resource
        examples:
            - name: ado
              manifest: |-
                {
                  "git_provider": "azureDevOpsServices",
                  "git_username": "myuser",
                  "personal_access_token": "sometoken"
                }
            - name: ado
              manifest: |-
                {
                  "git_provider": "azureDevOpsServicesAad"
                }
        argumentDocs:
            force: '- (Optional) specify if settings need to be enforced (i.e., to overwrite previously set credential for service principals).'
            git_provider: '-  (Required) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult Git Credentials API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition, awsCodeCommit, azureDevOpsServicesAad.'
            git_username: '- (Optional, required for some Git providers) user name at Git provider.'
            id: '- identifier of specific Git credential'
            is_default_for_provider: '- (Optional) boolean flag specifying if the credential is the default for the given provider type.'
            name: '- (Optional) the name of the git credential, used for identification and ease of lookup.'
            personal_access_token: '- (Optional, required for some Git providers) The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it''s sourced from the first environment variable of GITHUB_TOKEN, GITLAB_TOKEN, or AZDO_PERSONAL_ACCESS_TOKEN, that has a non-empty value.'
        importStatements: []
    databricks_global_init_script:
        subCategory: ""
        name: databricks_global_init_script
        title: databricks_global_init_script Resource
        examples:
            - name: init1
              manifest: |-
                {
                  "name": "my init script",
                  "source": "${path.module}/init.sh"
                }
            - name: init2
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"hello world\"\n    EOT\n  )}",
                  "name": "hello script"
                }
        argumentDocs:
            content_base64: '- The base64-encoded source code global init script. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances'
            enabled: '(bool, optional default: false) specifies if the script is enabled for execution, or not'
            id: '- ID assigned to a global init script by API'
            name: (string, required) - the name of the script.  It should be unique
            position: '(integer, optional default: null) - the position of a global init script, where 0 represents the first global init script to run, 1 is the second global init script to run, and so on. When omitted, the script gets the last position.'
            source: '- Path to script''s source code on local filesystem. Conflicts with content_base64'
        importStatements: []
    databricks_grant:
        subCategory: ""
        name: databricks_grant
        title: databricks_grant Resource
        examples:
            - name: sandbox_data_engineers
              manifest: |-
                {
                  "metastore": "metastore_id",
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_CATALOG",
                    "CREATE_EXTERNAL_LOCATION"
                  ]
                }
            - name: sandbox_data_sharer
              manifest: |-
                {
                  "metastore": "metastore_id",
                  "principal": "Data Sharer",
                  "privileges": [
                    "CREATE_RECIPIENT",
                    "CREATE_SHARE"
                  ]
                }
            - name: sandbox_data_scientists
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Scientists",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "CREATE_TABLE",
                    "SELECT"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: sandbox_data_engineers
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "CREATE_SCHEMA",
                    "CREATE_TABLE",
                    "MODIFY"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: sandbox_data_analyst
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Analyst",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "SELECT"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: things
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ],
                  "schema": "${databricks_schema.things.id}"
                }
              references:
                schema: databricks_schema.things.id
              dependencies:
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: customers_data_engineers
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "MODIFY",
                    "SELECT"
                  ],
                  "table": "main.reporting.customers"
                }
            - name: customers_data_analysts
              manifest: |-
                {
                  "principal": "Data Analysts",
                  "privileges": [
                    "SELECT"
                  ],
                  "table": "main.reporting.customers"
                }
            - name: things
              manifest: |-
                {
                  "for_each": "${data.databricks_tables.things.ids}",
                  "principal": "sensitive",
                  "privileges": [
                    "SELECT",
                    "MODIFY"
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_tables.things.ids
                table: each.value
            - name: customer360
              manifest: |-
                {
                  "principal": "Data Analysts",
                  "privileges": [
                    "SELECT"
                  ],
                  "table": "main.reporting.customer360"
                }
            - name: customers
              manifest: |-
                {
                  "for_each": "${data.databricks_views.customers.ids}",
                  "principal": "sensitive",
                  "privileges": [
                    "SELECT",
                    "MODIFY"
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_views.customers.ids
                table: each.value
            - name: volume
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "WRITE_VOLUME"
                  ],
                  "volume": "${databricks_volume.this.id}"
                }
              references:
                volume: databricks_volume.this.id
              dependencies:
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "storage_location": "${databricks_external_location.some.url}",
                      "volume_type": "EXTERNAL"
                    }
            - name: customers_data_engineers
              manifest: |-
                {
                  "model": "main.reporting.customer_model",
                  "principal": "Data Engineers",
                  "privileges": [
                    "APPLY_TAG",
                    "EXECUTE"
                  ]
                }
            - name: customers_data_analysts
              manifest: |-
                {
                  "model": "main.reporting.customer_model",
                  "principal": "Data Analysts",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: udf_data_engineers
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "principal": "Data Engineers",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: udf_data_analysts
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "principal": "Data Analysts",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: external_creds
              manifest: |-
                {
                  "credential": "${databricks_credential.external.id}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "ACCESS"
                  ]
                }
              references:
                credential: databricks_credential.external.id
              dependencies:
                databricks_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}",
                      "purpose": "SERVICE"
                    }
            - name: external_creds
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_EXTERNAL_TABLE"
                  ],
                  "storage_credential": "${databricks_storage_credential.external.id}"
                }
              references:
                storage_credential: databricks_storage_credential.external.id
              dependencies:
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some_data_engineers
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_EXTERNAL_TABLE",
                    "READ_FILES"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_service_principal
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_service_principal.my_sp.application_id}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_service_principal.my_sp.application_id
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_group
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_group.my_group.display_name}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_group.my_group.display_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_user
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_group.my_user.user_name}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_group.my_user.user_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some
              manifest: |-
                {
                  "foreign_connection": "${databricks_connection.mysql.name}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_FOREIGN_CATALOG",
                    "USE_CONNECTION"
                  ]
                }
              references:
                foreign_connection: databricks_connection.mysql.name
              dependencies:
                databricks_connection.mysql: |-
                    {
                      "comment": "this is a connection to mysql db",
                      "connection_type": "MYSQL",
                      "name": "mysql_connection",
                      "options": {
                        "host": "test.mysql.database.azure.com",
                        "password": "password",
                        "port": "3306",
                        "user": "user"
                      },
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: some
              manifest: |-
                {
                  "principal": "${databricks_recipient.some.name}",
                  "privileges": [
                    "SELECT"
                  ],
                  "share": "${databricks_share.some.name}"
                }
              references:
                principal: databricks_recipient.some.name
                share: databricks_share.some.name
              dependencies:
                databricks_recipient.some: |-
                    {
                      "name": "my_recipient"
                    }
                databricks_share.some: |-
                    {
                      "name": "my_share"
                    }
        argumentDocs:
            principal: '- User name, group name or service principal application ID.'
            privileges: '- One or more privileges that are specific to a securable type.'
        importStatements: []
    databricks_grants:
        subCategory: ""
        name: databricks_grants
        title: databricks_grants Resource
        examples:
            - name: sandbox
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_CATALOG",
                        "CREATE_EXTERNAL_LOCATION"
                      ]
                    },
                    {
                      "principal": "Data Sharer",
                      "privileges": [
                        "CREATE_RECIPIENT",
                        "CREATE_SHARE"
                      ]
                    }
                  ],
                  "metastore": "metastore_id"
                }
            - name: sandbox
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "grant": [
                    {
                      "principal": "Data Scientists",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "CREATE_TABLE",
                        "SELECT"
                      ]
                    },
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "CREATE_SCHEMA",
                        "CREATE_TABLE",
                        "MODIFY"
                      ]
                    },
                    {
                      "principal": "Data Analyst",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "SELECT"
                      ]
                    }
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: things
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "USE_SCHEMA",
                        "MODIFY"
                      ]
                    }
                  ],
                  "schema": "${databricks_schema.things.id}"
                }
              references:
                schema: databricks_schema.things.id
              dependencies:
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: customers
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "MODIFY",
                        "SELECT"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "main.reporting.customers"
                }
            - name: things
              manifest: |-
                {
                  "for_each": "${data.databricks_tables.things.ids}",
                  "grant": [
                    {
                      "principal": "sensitive",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    }
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_tables.things.ids
                table: each.value
            - name: customer360
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "main.reporting.customer360"
                }
            - name: customers
              manifest: |-
                {
                  "for_each": "${data.databricks_views.customers.ids}",
                  "grant": [
                    {
                      "principal": "sensitive",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    }
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_views.customers.ids
                table: each.value
            - name: volume
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "WRITE_VOLUME"
                      ]
                    }
                  ],
                  "volume": "${databricks_volume.this.id}"
                }
              references:
                volume: databricks_volume.this.id
              dependencies:
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "storage_location": "${databricks_external_location.some.url}",
                      "volume_type": "EXTERNAL"
                    }
            - name: customers
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "APPLY_TAG",
                        "EXECUTE"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "EXECUTE"
                      ]
                    }
                  ],
                  "model": "main.reporting.customer_model"
                }
            - name: udf
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "EXECUTE"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "EXECUTE"
                      ]
                    }
                  ]
                }
            - name: external_creds
              manifest: |-
                {
                  "credential": "${databricks_credential.external.id}",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_CONNECTION"
                      ]
                    }
                  ]
                }
              references:
                credential: databricks_credential.external.id
              dependencies:
                databricks_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}",
                      "purpose": "SERVICE"
                    }
            - name: external_creds
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE"
                      ]
                    }
                  ],
                  "storage_credential": "${databricks_storage_credential.external.id}"
                }
              references:
                storage_credential: databricks_storage_credential.external.id
              dependencies:
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    },
                    {
                      "principal": "${databricks_service_principal.my_sp.application_id}",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    },
                    {
                      "principal": "${databricks_group.my_group.display_name}",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    },
                    {
                      "principal": "${databricks_group.my_user.user_name}",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    }
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                grant.principal: databricks_group.my_user.user_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some
              manifest: |-
                {
                  "foreign_connection": "${databricks_connection.mysql.name}",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_FOREIGN_CATALOG",
                        "USE_CONNECTION"
                      ]
                    }
                  ]
                }
              references:
                foreign_connection: databricks_connection.mysql.name
              dependencies:
                databricks_connection.mysql: |-
                    {
                      "comment": "this is a connection to mysql db",
                      "connection_type": "MYSQL",
                      "name": "mysql_connection",
                      "options": {
                        "host": "test.mysql.database.azure.com",
                        "password": "password",
                        "port": "3306",
                        "user": "user"
                      },
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: some
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "${databricks_recipient.some.name}",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "share": "${databricks_share.some.name}"
                }
              references:
                grant.principal: databricks_recipient.some.name
                share: databricks_share.some.name
              dependencies:
                databricks_recipient.some: |-
                    {
                      "name": "my_recipient"
                    }
                databricks_share.some: |-
                    {
                      "name": "my_share"
                    }
        argumentDocs:
            databricks_grants.principal: '- User name, group name or service principal application ID.'
            databricks_grants.privileges: '- One or more privileges that are specific to a securable type.'
        importStatements: []
    databricks_group:
        subCategory: ""
        name: databricks_group
        title: databricks_group Resource
        examples:
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "display_name": "Some Group"
                }
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "display_name": "Some Group"
                }
              dependencies:
                databricks_group_member.vip_member: |-
                    {
                      "group_id": "${databricks_group.this.id}",
                      "member_id": "${databricks_user.this.id}"
                    }
                databricks_user.this: |-
                    {
                      "user_name": "someone@example.com"
                    }
            - name: this
              manifest: |-
                {
                  "display_name": "Some Group",
                  "provider": "${databricks.mws}"
                }
              references:
                provider: databricks.mws
            - name: this
              manifest: |-
                {
                  "display_name": "Some Group",
                  "provider": "${databricks.azure_account}"
                }
              references:
                provider: databricks.azure_account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. groups/Some Group.'
            allow_cluster_create: '-  (Optional) This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the group to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            display_name: '-  (Required) This is the display name for the given group.'
            external_id: '- (Optional) ID of the group in an external identity provider.'
            force: '- (Optional) Ignore cannot create group: Group with name X already exists. errors and implicitly import the specific group into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            id: '- Canonical unique identifier for the group (SCIM ID).'
            workspace_access: '- (Optional) This is a field to allow the group to have access to a Databricks Workspace.'
            workspace_consume: '- (Optional) This is a field to allow the group to have access to a Databricks Workspace as consumer, with limited access to workspace UI.  Couldn''t be used with workspace_access or databricks_sql_access.'
        importStatements: []
    databricks_group_instance_profile:
        subCategory: ""
        name: databricks_group_instance_profile
        title: databricks_group_instance_profile Resource
        examples:
            - name: my_group_instance_profile
              manifest: |-
                {
                  "group_id": "${databricks_group.my_group.id}",
                  "instance_profile_id": "${databricks_instance_profile.instance_profile.id}"
                }
              references:
                group_id: databricks_group.my_group.id
                instance_profile_id: databricks_instance_profile.instance_profile.id
              dependencies:
                databricks_group.my_group: |-
                    {
                      "display_name": "my_group_name"
                    }
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
        argumentDocs:
            group_id: '- (Required) This is the id of the group resource.'
            id: '- The id in the format <group_id>|<instance_profile_id>.'
            instance_profile_id: '-  (Required) This is the id of the instance profile resource.'
        importStatements: []
    databricks_group_member:
        subCategory: ""
        name: databricks_group_member
        title: databricks_group_member Resource
        examples:
            - name: ab
              manifest: |-
                {
                  "group_id": "${databricks_group.a.id}",
                  "member_id": "${databricks_group.b.id}"
                }
              references:
                group_id: databricks_group.a.id
                member_id: databricks_group.b.id
              dependencies:
                databricks_group.a: |-
                    {
                      "display_name": "A"
                    }
                databricks_group.b: |-
                    {
                      "display_name": "B"
                    }
                databricks_user.bradley: |-
                    {
                      "user_name": "bradley@example.com"
                    }
            - name: bb
              manifest: |-
                {
                  "group_id": "${databricks_group.b.id}",
                  "member_id": "${databricks_user.bradley.id}"
                }
              references:
                group_id: databricks_group.b.id
                member_id: databricks_user.bradley.id
              dependencies:
                databricks_group.a: |-
                    {
                      "display_name": "A"
                    }
                databricks_group.b: |-
                    {
                      "display_name": "B"
                    }
                databricks_user.bradley: |-
                    {
                      "user_name": "bradley@example.com"
                    }
        argumentDocs:
            group_id: '- (Required) This is the id attribute (SCIM ID) of the group resource.'
            id: '- The id for the databricks_group_member object which is in the format <group_id>|<member_id>.'
            member_id: '- (Required) This is the id attribute (SCIM ID) of the group, service principal, or user.'
        importStatements: []
    databricks_group_role:
        subCategory: ""
        name: databricks_group_role
        title: databricks_group_role Resource
        examples:
            - name: my_group_instance_profile
              manifest: |-
                {
                  "group_id": "${databricks_group.my_group.id}",
                  "role": "${databricks_instance_profile.instance_profile.id}"
                }
              references:
                group_id: databricks_group.my_group.id
                role: databricks_instance_profile.instance_profile.id
              dependencies:
                databricks_group.my_group: |-
                    {
                      "display_name": "my_group_name"
                    }
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
            - name: my_group_account_admin
              manifest: |-
                {
                  "group_id": "${databricks_group.my_group.id}",
                  "role": "account_admin"
                }
              references:
                group_id: databricks_group.my_group.id
              dependencies:
                databricks_group.my_group: |-
                    {
                      "display_name": "my_group_name"
                    }
        argumentDocs:
            group_id: '- (Required) This is the id of the group resource.'
            id: '- The id for the databricks_group_role object which is in the format <group_id>|<role>.'
            role: '- (Required) Either a role name or the ARN/ID of the instance profile resource.'
        importStatements: []
    databricks_instance_pool:
        subCategory: ""
        name: databricks_instance_pool
        title: databricks_instance_pool Resource
        examples:
            - name: smallest_nodes
              manifest: |-
                {
                  "aws_attributes": [
                    {
                      "availability": "ON_DEMAND",
                      "spot_bid_price_percent": "100",
                      "zone_id": "us-east-1a"
                    }
                  ],
                  "disk_spec": [
                    {
                      "disk_count": 1,
                      "disk_size": 80,
                      "disk_type": [
                        {
                          "ebs_volume_type": "GENERAL_PURPOSE_SSD"
                        }
                      ]
                    }
                  ],
                  "idle_instance_autotermination_minutes": 10,
                  "instance_pool_name": "Smallest Nodes",
                  "max_capacity": 300,
                  "min_idle_instances": 0,
                  "node_type_id": "${data.databricks_node_type.smallest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
            - name: this
              manifest: |-
                {
                  "preloaded_docker_image": [
                    {
                      "basic_auth": [
                        {
                          "password": "${azurerm_container_registry.this.admin_password}",
                          "username": "${azurerm_container_registry.this.admin_username}"
                        }
                      ],
                      "url": "${docker_registry_image.this.name}"
                    }
                  ]
                }
              references:
                preloaded_docker_image.basic_auth.password: azurerm_container_registry.this.admin_password
                preloaded_docker_image.basic_auth.username: azurerm_container_registry.this.admin_username
                preloaded_docker_image.url: docker_registry_image.this.name
              dependencies:
                docker_registry_image.this: |-
                    {
                      "build": [
                        {}
                      ],
                      "name": "${azurerm_container_registry.this.login_server}/sample:latest"
                    }
        argumentDocs:
            1 - 1023: GiB
            1- 1023: GiB
            100 - 4096: GiB
            500 - 4096: GiB
            availability: '- (Optional) (String) Availability type used for all instances in the pool. Only ON_DEMAND and SPOT are supported.'
            azure_attributes.availability: '- (Optional) Availability type used for all nodes. Valid values are SPOT_AZURE and ON_DEMAND_AZURE.'
            azure_attributes.spot_bid_max_price: '- (Optional) The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance.'
            custom_tags: '- (Optional) (Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS & Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the official documentation). Attempting to set the same tags in both cluster and instance pool will raise an error. Databricks allows at most 43 custom tags.'
            disk_count: '- (Optional) (Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.'
            disk_size: '- (Optional) (Integer) The size of each disk (in GiB) to attach.'
            enable_elastic_disk: '- (Optional) (Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.'
            gcp_attributes.gcp_availability: '- (Optional) Availability type used for all nodes. Valid values are PREEMPTIBLE_GCP, PREEMPTIBLE_WITH_FALLBACK_GCP and ON_DEMAND_GCP, default: ON_DEMAND_GCP.'
            gcp_attributes.local_ssd_count: (Optional, Int) Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
            gcp_attributes.zone_id: '- (Optional) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-central1-a. The provided availability zone must be in the same region as the Databricks workspace.'
            id: '- Canonical unique identifier for the instance pool.'
            idle_instance_autotermination_minutes: '- (Required) (Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.'
            instance_pool_name: '- (Required) (String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.'
            max_capacity: '- (Optional) (Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a best practice, this should be set based on anticipated usage.'
            min_idle_instances: '- (Optional) (Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.'
            node_type_id: '- (Required) (String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool''s idle instances are allocated based on this type. You can retrieve a list of available node types by using the List Node Types API call.'
            preloaded_docker_image.basic_auth: '- (Optional) basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.'
            preloaded_docker_image.url: '- URL for the Docker image'
            preloaded_spark_versions: '- (Optional) (List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks_spark_version data source or via  Runtime Versions API call.'
            spot_bid_price_percent: '- (Optional) (Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type''s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. For safety, this field cannot be greater than 10000.'
            zone_id: '- (Optional) (String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like "us-west-2a". The provided availability zone must be in the same region as the Databricks deployment. For example, "us-west-2a" is not a valid zone ID if the Databricks deployment resides in the "us-east-1" region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the List Zones API.'
        importStatements: []
    databricks_instance_profile:
        subCategory: ""
        name: databricks_instance_profile
        title: databricks_instance_profile Resource
        examples:
            - name: shared
              manifest: |-
                {
                  "instance_profile_arn": "${aws_iam_instance_profile.shared.arn}"
                }
              references:
                instance_profile_arn: aws_iam_instance_profile.shared.arn
              dependencies:
                aws_iam_instance_profile.shared: |-
                    {
                      "name": "shared-instance-profile",
                      "role": "${aws_iam_role.role_for_s3_access.name}"
                    }
                aws_iam_policy.pass_role_for_s3_access: |-
                    {
                      "name": "shared-pass-role-for-s3-access",
                      "path": "/",
                      "policy": "${data.aws_iam_policy_document.pass_role_for_s3_access.json}"
                    }
                aws_iam_role.role_for_s3_access: |-
                    {
                      "assume_role_policy": "${data.aws_iam_policy_document.assume_role_for_ec2.json}",
                      "description": "Role for shared access",
                      "name": "shared-ec2-role-for-s3"
                    }
                aws_iam_role_policy_attachment.cross_account: |-
                    {
                      "policy_arn": "${aws_iam_policy.pass_role_for_s3_access.arn}",
                      "role": "${var.crossaccount_role_name}"
                    }
                databricks_cluster.this: |-
                    {
                      "autoscale": [
                        {
                          "max_workers": 50,
                          "min_workers": 1
                        }
                      ],
                      "autotermination_minutes": 20,
                      "aws_attributes": [
                        {
                          "availability": "SPOT",
                          "first_on_demand": 1,
                          "instance_profile_arn": "${databricks_instance_profile.shared.id}",
                          "spot_bid_price_percent": 100,
                          "zone_id": "us-east-1"
                        }
                      ],
                      "cluster_name": "Shared Autoscaling",
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
            - name: this
              manifest: |-
                {
                  "instance_profile_arn": "${aws_iam_instance_profile.shared.id}"
                }
              references:
                instance_profile_arn: aws_iam_instance_profile.shared.id
              dependencies:
                databricks_group_instance_profile.all: |-
                    {
                      "group_id": "${data.databricks_group.users.id}",
                      "instance_profile_id": "${databricks_instance_profile.this.id}"
                    }
            - name: this
              manifest: |-
                {
                  "iam_role_arn": "${aws_iam_role.this.arn}",
                  "instance_profile_arn": "${aws_iam_instance_profile.this.arn}"
                }
              references:
                iam_role_arn: aws_iam_role.this.arn
                instance_profile_arn: aws_iam_instance_profile.this.arn
              dependencies:
                aws_iam_instance_profile.this: |-
                    {
                      "name": "my-databricks-sql-serverless-instance-profile",
                      "role": "${aws_iam_role.this.name}"
                    }
                aws_iam_role.this: |-
                    {
                      "assume_role_policy": "${data.aws_iam_policy_document.sql_serverless_assume_role.json}",
                      "name": "my-databricks-sql-serverless-role"
                    }
        argumentDocs:
            iam_role_arn: '- (Optional) The AWS IAM role ARN of the role associated with the instance profile. It must have the form arn:aws:iam::<account-id>:role/<name>. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.'
            id: '- ARN for EC2 Instance Profile, that is registered with Databricks.'
            instance_profile_arn: '- (Required) ARN attribute of aws_iam_instance_profile output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.'
            is_meta_instance_profile: '- (Optional) Whether the instance profile is a meta instance profile. Used only in IAM credential passthrough.'
            skip_validation: '- (Optional) For advanced usage only. If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. "Your requested instance type is not supported in your requested availability zone"), you can pass this flag to skip the validation and forcibly add the instance profile.'
        importStatements: []
    databricks_ip_access_list:
        subCategory: ""
        name: databricks_ip_access_list
        title: databricks_ip_access_list Resource
        examples:
            - name: allowed-list
              manifest: |-
                {
                  "depends_on": [
                    "${databricks_workspace_conf.this}"
                  ],
                  "ip_addresses": [
                    "1.1.1.1",
                    "1.2.3.0/24",
                    "1.2.5.0/24"
                  ],
                  "label": "allow_in",
                  "list_type": "ALLOW"
                }
              dependencies:
                databricks_workspace_conf.this: |-
                    {
                      "custom_config": {
                        "enableIpAccessLists": true
                      }
                    }
        argumentDocs:
            enabled: '- (Optional) Boolean true or false indicating whether this list should be active.  Defaults to true'
            id: '- Canonical unique identifier for the IP Access List, same as list_id.'
            ip_addresses: '- A string list of IP addresses and CIDR ranges.'
            label: '-  This is the display name for the given IP ACL List.'
            list_id: '- Canonical unique identifier for the IP Access List.'
            list_type: '-  Can only be "ALLOW" or "BLOCK".'
        importStatements: []
    databricks_job:
        subCategory: ""
        name: databricks_job
        title: databricks_job Resource
        examples:
            - name: this
              manifest: |-
                {
                  "description": "This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.",
                  "job_cluster": [
                    {
                      "job_cluster_key": "j",
                      "new_cluster": [
                        {
                          "node_type_id": "${data.databricks_node_type.smallest.id}",
                          "num_workers": 2,
                          "spark_version": "${data.databricks_spark_version.latest.id}"
                        }
                      ]
                    }
                  ],
                  "name": "Job with multiple tasks",
                  "task": [
                    {
                      "new_cluster": [
                        {
                          "node_type_id": "${data.databricks_node_type.smallest.id}",
                          "num_workers": 1,
                          "spark_version": "${data.databricks_spark_version.latest.id}"
                        }
                      ],
                      "notebook_task": [
                        {
                          "notebook_path": "${databricks_notebook.this.path}"
                        }
                      ],
                      "task_key": "a"
                    },
                    {
                      "depends_on": [
                        {
                          "task_key": "a"
                        }
                      ],
                      "existing_cluster_id": "${databricks_cluster.shared.id}",
                      "spark_jar_task": [
                        {
                          "main_class_name": "com.acme.data.Main"
                        }
                      ],
                      "task_key": "b"
                    },
                    {
                      "job_cluster_key": "j",
                      "notebook_task": [
                        {
                          "notebook_path": "${databricks_notebook.this.path}"
                        }
                      ],
                      "task_key": "c"
                    },
                    {
                      "pipeline_task": [
                        {
                          "pipeline_id": "${databricks_pipeline.this.id}"
                        }
                      ],
                      "task_key": "d"
                    }
                  ]
                }
              references:
                job_cluster.new_cluster.node_type_id: data.databricks_node_type.smallest.id
                job_cluster.new_cluster.spark_version: data.databricks_spark_version.latest.id
                task.existing_cluster_id: databricks_cluster.shared.id
                task.new_cluster.node_type_id: data.databricks_node_type.smallest.id
                task.new_cluster.spark_version: data.databricks_spark_version.latest.id
                task.notebook_task.notebook_path: databricks_notebook.this.path
                task.pipeline_task.pipeline_id: databricks_pipeline.this.id
            - name: sql_aggregation_job
              manifest: |-
                {
                  "name": "Example SQL Job",
                  "task": [
                    {
                      "sql_task": [
                        {
                          "query": [
                            {
                              "query_id": "${databricks_sql_query.agg_query.id}"
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_agg_query"
                    },
                    {
                      "sql_task": [
                        {
                          "dashboard": [
                            {
                              "dashboard_id": "${databricks_sql_dashboard.dash.id}",
                              "subscriptions": [
                                {
                                  "user_name": "user@domain.com"
                                }
                              ]
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_dashboard"
                    },
                    {
                      "sql_task": [
                        {
                          "alert": [
                            {
                              "alert_id": "${databricks_sql_alert.alert.id}",
                              "subscriptions": [
                                {
                                  "user_name": "user@domain.com"
                                }
                              ]
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_alert"
                    }
                  ]
                }
              references:
                task.sql_task.alert.alert_id: databricks_sql_alert.alert.id
                task.sql_task.dashboard.dashboard_id: databricks_sql_dashboard.dash.id
                task.sql_task.query.query_id: databricks_sql_query.agg_query.id
                task.sql_task.warehouse_id: databricks_sql_endpoint.sql_job_warehouse.id
            - name: this
              manifest: |-
                {
                  "task": [
                    {
                      "library": [
                        {
                          "pypi": [
                            {
                              "package": "databricks-mosaic==0.3.14"
                            }
                          ]
                        }
                      ],
                      "task_key": "some_task"
                    }
                  ]
                }
            - name: this
              manifest: |-
                {
                  "run_as": [
                    {
                      "service_principal_name": "8d23ae77-912e-4a19-81e4-b9c3f5cc9349"
                    }
                  ]
                }
            - name: this
              manifest: |-
                {
                  "tags": {
                    "environment": "dev",
                    "owner": "dream-team"
                  }
                }
            - name: this
              manifest: |-
                {
                  "name": "Terraform Demo (${data.databricks_current_user.me.alphanumeric})",
                  "new_cluster": [
                    {
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "num_workers": 1,
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
                  ],
                  "notebook_task": [
                    {
                      "notebook_path": "${databricks_notebook.this.path}"
                    }
                  ]
                }
              references:
                new_cluster.node_type_id: data.databricks_node_type.smallest.id
                new_cluster.spark_version: data.databricks_spark_version.latest.id
                notebook_task.notebook_path: databricks_notebook.this.path
              dependencies:
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    # created from ${abspath(path.module)}\n    display(spark.range(10))\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/Terraform"
                    }
        argumentDocs:
            '*_task': '- (Required) one of the specific task blocks described below:'
            GIT: ': The Python file is located in a remote Git repository.'
            PERFORMANCE_OPTIMIZED: ': (default value) Prioritizes fast startup and execution times through rapid scaling and optimized cluster performance.'
            STANDARD: ': Enables cost-efficient execution of serverless workloads.'
            WORKSPACE: ': The Python file is located in a Databricks workspace or at a cloud filesystem URI.'
            alert: '- (Optional) block consisting of following fields:'
            alert_id: '- (Required) (String) identifier of the Databricks Alert (databricks_alert).'
            alert_on_last_attempt: '- (Optional) (Bool) do not send notifications to recipients specified in on_start for the retried runs and do not send notifications to recipients specified in on_failure until the last retry of the run.'
            always_running: '- (Optional, Deprecated) (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with parameters specified in spark_jar_task or spark_submit_task or spark_python_task or notebook_task blocks.'
            authentication_method: (Required) How the published Power BI model authenticates to Databricks
            autotermination_minutes: ', is_pinned, workload_type aren''t supported!'
            base_parameters: '- (Optional) (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job''s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using dbutils.widgets.get.'
            branch: '- name of the Git branch to use. Conflicts with tag and commit.'
            budget_policy_id: '- (Optional) The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job.'
            catalog: '- (Optional) The name of the catalog to use inside Unity Catalog.'
            clean_room_name: (Required) The clean room that the notebook belongs to.
            commands: '- (Required) (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".'
            commit: '- hash of Git commit to use. Conflicts with branch and tag.'
            concurrency: '- (Optional) Controls the number of active iteration task runs. Default is 20, maximum allowed is 100.'
            connection_resource_name: (Required) The resource name of the UC connection to authenticate from Databricks to Power BI
            continuous: '- (Optional) Configuration block to configure pause status. See continuous Configuration Block.'
            continuous.pause_status: '- (Optional) Indicate whether this continuous job is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted in the block, the server will default to using UNPAUSED as a value for pause_status.'
            control_run_state: '- (Optional) (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the pause_status by stopping the current active run. This flag cannot be set for non-continuous jobs.'
            custom_subject: (Optional) Allows users to specify a custom subject line on the email sent to subscribers.
            dashboard: '- (Optional) block consisting of following fields:'
            dashboard_id: (Required) The identifier of the dashboard to refresh
            default: '- (Required) Default value of the parameter.'
            dependencies: '- (list of strings) List of pip dependencies, as supported by the version of pip in this environment. Each dependency is a pip requirement file line.  See API docs for more information.'
            depends_on: '- (Optional) block specifying dependency(-ies) for a given task.'
            description: '- (Optional) An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.'
            destination_id: (Optional) A snapshot of the dashboard will be sent to the destination when the destination_id field is present.
            disable_auto_optimization: '- (Optional) A flag to disable auto optimization in serverless tasks.'
            edit_mode: '- (Optional) If "UI_LOCKED", the user interface for the job will be locked. If "EDITABLE" (the default), the user interface will be editable.'
            email_notifications: '- (Optional) (List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.'
            enabled: '- (Required) If true, enable queueing for the job.'
            entry_point: '- (Optional) Python function as entry point for the task'
            environment_key: '- (Optional) identifier of an environment block that is used to specify libraries.  Required for some tasks (spark_python_task, python_wheel_task, ...) running on serverless compute.'
            environment_version: '- (Required, string) client version used by the environment. Each version comes with a specific Python version and a set of Python packages.'
            etag: (Optional) Checksum to validate the freshness of the notebook resource.
            existing_cluster_id: '- (Optional) Identifier of the interactive cluster to run job on.  Note: running tasks on interactive clusters may lead to increased costs!'
            file: '- (Optional) block consisting of single string fields:'
            full_refresh: '- (Optional) (Bool) Specifies if there should be full refresh of the pipeline.'
            git_source: '- (Optional) Specifies the a Git repository for task source code. See git_source Configuration Block below.'
            health: '- (Optional) An optional block that specifies the health conditions for the job documented below.'
            id: '- ID of the system notification that is notified when an event defined in webhook_notifications is triggered.'
            inputs: '- (Required) (String) Array for task to iterate on. This can be a JSON string or a reference to an array parameter.'
            interval: '- (Required) Specifies the interval at which the job should run. This value is required.'
            is_pinned: '- isn''t supported'
            job_cluster: '- (Optional) A list of job databricks_cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. Multi-task syntax'
            job_cluster.job_cluster_key: '- (Required) Identifier that can be referenced in task block, so that cluster is shared between tasks'
            job_cluster.new_cluster: '- Block with almost the same set of parameters as for databricks_cluster resource, except following (check the REST API documentation for full list of supported parameters):'
            job_cluster_key: '- (Optional) Identifier of the Job cluster specified in the job_cluster block.'
            job_id: '- (Required)(String) ID of the job'
            job_parameters: '- (Optional)(Map) Job parameters for the task'
            left: '- The left operand of the condition task. It could be a string value, job state, or a parameter reference.'
            library: '- (Optional) (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.'
            main_class_name: '- (Optional) The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use SparkContext.getOrCreate to obtain a Spark context; otherwise, runs of the job will fail.'
            max_concurrent_runs: '- (Optional) (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to 1.'
            max_retries: '- (Optional) (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED or INTERNAL_ERROR lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED or INTERNAL_ERROR.'
            metric: '- (Required) string specifying the metric to check, like RUN_DURATION_SECONDS, STREAMING_BACKLOG_FILES, etc. - check the Jobs REST API documentation for the full list of supported metrics.'
            min_retry_interval_millis: '- (Optional) (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.'
            min_time_between_triggers_seconds: '- (Optional) If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.'
            model_name: (Required) The name of the Power BI model
            name: '- (Optional) An optional name for the job. The default value is Untitled.'
            name.existing_cluster_id: '- (Optional) If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We strongly suggest to use new_cluster for greater reliability.'
            name.new_cluster: '- (Optional) Same set of parameters as for databricks_cluster resource.'
            named_parameters: '- (Optional) Named parameters for the task'
            new_cluster: '- (Optional) Task will run on a dedicated cluster.  See databricks_cluster documentation for specification. Some parameters, such as'
            no_alert_for_canceled_runs: '- (Optional) (Bool) don''t send alert for cancelled runs.'
            no_alert_for_skipped_runs: '- (Optional) (Bool) don''t send alert for skipped runs. (It''s recommended to use the corresponding setting in the notification_settings configuration block).'
            notebook_base_parameters: (Optional) (Map) Base parameters to be used for the clean room notebook job.
            notebook_name: (Required) Name of the notebook being run.
            notebook_path: '- (Required) The path of the databricks_notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.'
            notification_settings: '- (Optional) An optional block controlling the notification settings on the job level documented below.'
            on_duration_warning_threshold_exceeded: '- (Optional) (List) list of emails to notify when the duration of a run exceeds the threshold specified by the RUN_DURATION_SECONDS metric in the health block.'
            on_failure: '- (Optional) (List) list of emails to notify when the run fails.'
            on_start: '- (Optional) (List) list of emails to notify when the run starts.'
            on_streaming_backlog_exceeded: '- (Optional) (List) list of emails to notify when any streaming backlog thresholds are exceeded for any stream.'
            on_success: '- (Optional) (List) list of emails to notify when the run completes successfully.'
            op: '- The string specifying the operation used to compare operands.  Currently, following operators are supported: EQUAL_TO, GREATER_THAN, GREATER_THAN_OR_EQUAL, LESS_THAN, LESS_THAN_OR_EQUAL, NOT_EQUAL. (Check the API docs for the latest information).'
            outcome: '- (Optional, string) Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are "true" or "false".'
            overwrite_existing: (Optional) Whether to overwrite existing Power BI models. Default is false
            package_name: '- (Optional) Name of Python package'
            parameter: '- (Optional) Specifies job parameter for the job. See parameter Configuration Block'
            parameters: '- (Optional) Parameters for the task'
            path: '- If source is GIT: Relative path to the file in the repository specified in the git_source block with SQL commands to execute. If source is WORKSPACE: Absolute path to the file in the workspace with SQL commands to execute.'
            pause_subscriptions: '- (Optional) flag that specifies if subscriptions are paused or not.'
            paused: (Optional) When true, the subscription will not send emails.
            performance_target: '- (Optional) The performance mode on a serverless job. The performance target determines the level of compute performance or cost-efficiency for the run.  Supported values are:'
            pipeline_id: '- (Required) The pipeline''s unique ID.'
            power_bi_model: '(Required) The semantic model to update. Block consists of following fields:'
            profiles_directory: '- (Optional) The relative path to the directory in the repository specified by git_source where dbt should look in for the profiles.yml file. If not specified, defaults to the repository''s root directory. Equivalent to passing --profile-dir to a dbt command.'
            project_directory: '- (Required when source is WORKSPACE) The path where dbt should look for dbt_project.yml. Equivalent to passing --project-dir to the dbt CLI.'
            provider: '- (Optional, if it''s possible to detect Git provider by host name) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult Repos API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition.'
            python_file: '- (Required) The URI of the Python file to be executed. Cloud file URIs (e.g. s3:/, abfss:/, gs:/), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with /. For files stored in a remote repository, the path must be relative. This field is required.'
            query: '- (Optional) block consisting of single string field: query_id - identifier of the Databricks Query (databricks_query).'
            queue: '- (Optional) The queue status for the job. See queue Configuration Block below.'
            refresh_after_update: (Optional) Whether the model should be refreshed after the update. Default is false
            retry_on_timeout: '- (Optional) (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.'
            right: '- The right operand of the condition task. It could be a string value, job state, or parameter reference.'
            rules: '- (List) list of rules that are represented as objects with the following attributes:'
            run_as: '- (Optional) The user or the service principal the job runs as. See run_as Configuration Block below.'
            run_as.service_principal_name: '- (Optional) The application ID of an active service principal. Setting this field requires the servicePrincipal/user role.'
            run_as.user_name: '- (Optional) The email of an active workspace user. Non-admin users can only set this field to their own email.'
            run_if: '- (Optional) An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. One of ALL_SUCCESS, AT_LEAST_ONE_SUCCESS, NONE_FAILED, ALL_DONE, AT_LEAST_ONE_FAILED or ALL_FAILED. When omitted, defaults to ALL_SUCCESS.'
            schedule: '- (Optional) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below.'
            schedule.pause_status: '- (Optional) Indicate whether this schedule is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted and a schedule is provided, the server will default to using UNPAUSED as a value for pause_status.'
            schedule.quartz_cron_expression: '- (Required) A Cron expression using Quartz syntax that describes the schedule for a job. This field is required.'
            schedule.timezone_id: '- (Required) A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.'
            schema: '- (Optional) The name of the schema dbt should run in. Defaults to default.'
            source: '- (Optional) The source of the project. Possible values are WORKSPACE and GIT.  Defaults to GIT if a git_source block is present in the job definition.'
            spark_version: parameter in databricks_cluster and other resources.
            spec: '- block describing the Environment. Consists of following attributes:'
            storage_mode: (Required) The Power BI storage mode of the table
            subscribers: The list of subscribers to send the snapshot of the dashboard to.
            subscription: (Optional) Represents a subscription configuration for scheduled dashboard snapshots.
            subscriptions: '- (Optional) a list of subscription blocks consisting out of one of the required fields: user_name for user emails or destination_id - for Alert destination''s identifier.'
            tables: '(Required) (Array) The tables to be exported to Power BI. Block consists of following fields:'
            tag: '- name of the Git branch to use. Conflicts with branch and commit.'
            tags: '- (Optional) An optional map of the tags associated with the job. See tags Configuration Map'
            task: '- (Optional) A list of task specification that the job will execute. See task Configuration Block below.'
            task_key: '- (Required) string specifying an unique key for a given task.'
            timeout_seconds: '- (Optional) (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.'
            trigger: '- (Optional) The conditions that triggers the job to start. See trigger Configuration Block below.'
            trigger.file_arrival: '- (Optional) configuration block to define a trigger for File Arrival events consisting of following attributes:'
            trigger.pause_status: '- (Optional) Indicate whether this trigger is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted in the block, the server will default to using UNPAUSED as a value for pause_status.'
            trigger.periodic: '- (Optional) configuration block to define a trigger for Periodic Triggers consisting of the following attributes:'
            unit: '- (Required) Options are {"DAYS", "HOURS", "WEEKS"}.'
            url: '- (Required) URL to be monitored for file arrivals. The path must point to the root or a subpath of the external location. Please note that the URL must have a trailing slash character (/).'
            user_name: (Optional) A snapshot of the dashboard will be sent to the user's email when the user_name field is present.
            value: '- (Required) integer value used to compare to the given metric.'
            wait_after_last_change_seconds: '- (Optional) If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.'
            warehouse_id: (Optional) The warehouse id to execute the dashboard with for the schedule. If not specified, will use the default warehouse of dashboard
            webhook_notification.on_duration_warning_threshold_exceeded: '- (Optional) (List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the RUN_DURATION_SECONDS metric in the health block.'
            webhook_notification.on_failure: '- (Optional) (List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.'
            webhook_notification.on_start: '- (Optional) (List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.'
            webhook_notification.on_streaming_backlog_exceeded: '- (Optional) (List) list of notification IDs to call when any streaming backlog thresholds are exceeded for any stream.'
            webhook_notification.on_success: '- (Optional) (List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.'
            webhook_notifications: '- (Optional) (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.'
            workload_type: '- isn''t supported'
            workspace_name: (Required) The name of the Power BI workspace of the model
        importStatements: []
    databricks_lakehouse_monitor:
        subCategory: ""
        name: databricks_lakehouse_monitor
        title: databricks_lakehouse_monitor Resource
        examples:
            - name: testTimeseriesMonitor
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_lakehouse_monitoring/${databricks_sql_table.myTestTable.name}",
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_sql_table.myTestTable.name}",
                  "time_series": [
                    {
                      "granularities": [
                        "1 hour"
                      ],
                      "timestamp_col": "timestamp"
                    }
                  ]
                }
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_sql_table.myTestTable: |-
                    {
                      "catalog_name": "main",
                      "column": [
                        {
                          "name": "timestamp",
                          "type": "int"
                        }
                      ],
                      "data_source_format": "DELTA",
                      "name": "bar",
                      "schema_name": "${databricks_schema.things.name}",
                      "table_type": "MANAGED"
                    }
            - name: testMonitorInference
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_lakehouse_monitoring/${databricks_table.myTestTable.name}",
                  "inference_log": [
                    {
                      "granularities": [
                        "1 hour"
                      ],
                      "model_id_col": "model_id",
                      "prediction_col": "prediction",
                      "problem_type": "PROBLEM_TYPE_REGRESSION",
                      "timestamp_col": "timestamp"
                    }
                  ],
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_table.myTestTable.name}"
                }
            - name: testMonitorInference
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_lakehouse_monitoring/${databricks_table.myTestTable.name}",
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "snapshot": [
                    {}
                  ],
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_table.myTestTable.name}"
                }
        argumentDocs:
            assets_dir: '- (Required) - The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)'
            baseline_table_name: |-
                - Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
                table.
            custom_metrics: '- Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).'
            dashboard_id: '- The ID of the generated dashboard.'
            data_classification_config: '- The data classification config for the monitor'
            definition: '- create metric definition'
            drift_metrics_table_name: '- The full name of the drift metrics table. Format: catalog_name.schema_name.table_name.'
            granularities: '-  List of granularities to use when aggregating data into time windows based on their timestamp.'
            id: '-  ID of this monitor is the same as the full table name of the format {catalog}.{schema_name}.{table_name}'
            inference_log: '- Configuration for the inference log monitor'
            input_columns: '- Columns on the monitored table to apply the custom metrics to.'
            label_col: '- Column of the model label'
            model_id_col: '- Column of the model id or version'
            monitor_version: '- The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted'
            name: '- Name of the custom metric.'
            notifications: '- The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name email_addresses containing a list of emails to notify:'
            on_failure: '- who to send notifications to on monitor failure.'
            on_new_classification_tag_detected: '- Who to send notifications to when new data classification tags are detected.'
            output_data_type: '- The output type of the custom metric.'
            output_schema_name: '- (Required) - Schema where output metric tables are created'
            pause_status: '- optional string field that indicates whether a schedule is paused (PAUSED) or not (UNPAUSED).'
            prediction_col: '- Column of the model prediction'
            prediction_proba_col: '- Column of the model prediction probabilities'
            problem_type: '- Problem type the model aims to solve. Either PROBLEM_TYPE_CLASSIFICATION or PROBLEM_TYPE_REGRESSION'
            profile_metrics_table_name: '- The full name of the profile metrics table. Format: catalog_name.schema_name.table_name.'
            quartz_cron_expression: '- string expression that determines when to run the monitor. See Quartz documentation for examples.'
            schedule: '- The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:'
            skip_builtin_dashboard: '- Whether to skip creating a default dashboard summarizing data quality metrics.'
            slicing_exprs: '- List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.'
            snapshot: '- Configuration for monitoring snapshot tables.'
            status: '- Status of the Monitor'
            table_name: '- (Required) - The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}'
            time_series: '- Configuration for monitoring timeseries tables.'
            timestamp_col: '- Column of the timestamp of predictions'
            timezone_id: '- string with timezone id (e.g., PST) in which to evaluate the Quartz expression.'
            type: '- The type of the custom metric.'
            warehouse_id: '- Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.'
        importStatements: []
    databricks_library:
        subCategory: ""
        name: databricks_library
        title: databricks_library Resource
        examples:
            - name: cli
              manifest: |-
                {
                  "cluster_id": "${each.key}",
                  "for_each": "${data.databricks_clusters.all.ids}",
                  "pypi": [
                    {
                      "package": "databricks-cli"
                    }
                  ]
                }
              references:
                cluster_id: each.key
                for_each: data.databricks_clusters.all.ids
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "jar": "${databricks_file.app.path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                jar: databricks_file.app.path
              dependencies:
                databricks_file.app: |-
                    {
                      "path": "/Volumes/catalog/schema/volume/app-0.0.1.jar",
                      "source": "${path.module}/app-0.0.1.jar"
                    }
            - name: deequ
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "maven": [
                    {
                      "coordinates": "com.amazon.deequ:deequ:1.0.4",
                      "exclusions": [
                        "org.apache.avro:avro"
                      ]
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "whl": "${databricks_file.app.path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                whl: databricks_file.app.path
              dependencies:
                databricks_file.app: |-
                    {
                      "path": "/Volumes/catalog/schema/volume/baz.whl",
                      "source": "${path.module}/baz.whl"
                    }
            - name: fbprophet
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "pypi": [
                    {
                      "package": "fbprophet==0.6"
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
            - name: libraries
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "requirements": "/Workspace/path/to/requirements.txt"
                }
              references:
                cluster_id: databricks_cluster.this.id
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "egg": "${databricks_dbfs_file.app.dbfs_path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                egg: databricks_dbfs_file.app.dbfs_path
              dependencies:
                databricks_dbfs_file.app: |-
                    {
                      "path": "/FileStore/foo.egg",
                      "source": "${path.module}/foo.egg"
                    }
            - name: rkeops
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "cran": [
                    {
                      "package": "rkeops"
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
        argumentDocs: {}
        importStatements: []
    databricks_materialized_features_feature_tag Resource:
        subCategory: ""
        name: databricks_materialized_features_feature_tag Resource
        title: databricks_materialized_features_feature_tag Resource
        argumentDocs:
            key: (string, required)
            value: (string, optional)
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_metastore:
        subCategory: ""
        name: databricks_metastore
        title: databricks_metastore Resource
        examples:
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "us-east-1",
                  "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                }
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "eastus",
                  "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                }
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "${us-east1}",
                  "storage_root": "gs://${google_storage_bucket.unity_metastore.name}"
                }
              references:
                region: us-east1
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
        argumentDocs:
            delta_sharing_organization_name: '- (Optional) The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.'
            delta_sharing_recipient_token_lifetime_in_seconds: '- (Optional) Required along with delta_sharing_scope. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.'
            delta_sharing_scope: '- (Optional) Required along with delta_sharing_recipient_token_lifetime_in_seconds. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.'
            force_destroy: '- (Optional) Destroy metastore regardless of its contents.'
            id: '- system-generated ID of this Unity Catalog Metastore.'
            name: '- Name of metastore.'
            owner: '- (Optional) Username/groupname/sp application_id of the metastore owner.'
            region: '- (Mandatory for account-level) The region of the metastore'
            storage_root: '- (Optional) Path on cloud storage account, where managed databricks_table are stored. Change forces creation of a new resource. If no storage_root is defined for the metastore, each catalog must have a storage_root defined.'
        importStatements: []
    databricks_metastore_assignment:
        subCategory: ""
        name: databricks_metastore_assignment
        title: databricks_metastore_assignment Resource
        examples:
            - name: this
              manifest: |-
                {
                  "metastore_id": "${databricks_metastore.this.id}",
                  "workspace_id": "${local.workspace_id}"
                }
              references:
                metastore_id: databricks_metastore.this.id
                workspace_id: local.workspace_id
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "us-east-1",
                      "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                    }
        argumentDocs:
            default_catalog_name: '- (Deprecated) Default catalog used for this assignment. Please use databricks_default_namespace_setting instead.'
            id: '- ID of this metastore assignment in form of <workspace_id>|<metastore_id>.'
            metastore_id: '- Unique identifier of the parent Metastore'
            workspace_id: '- id of the workspace for the assignment'
        importStatements: []
    databricks_metastore_data_access:
        subCategory: ""
        name: databricks_metastore_data_access
        title: databricks_metastore_data_access Resource
        examples:
            - name: this
              manifest: |-
                {
                  "aws_iam_role": [
                    {
                      "role_arn": "${aws_iam_role.metastore_data_access.arn}"
                    }
                  ],
                  "is_default": true,
                  "metastore_id": "${databricks_metastore.this.id}",
                  "name": "${aws_iam_role.metastore_data_access.name}"
                }
              references:
                aws_iam_role.role_arn: aws_iam_role.metastore_data_access.arn
                metastore_id: databricks_metastore.this.id
                name: aws_iam_role.metastore_data_access.name
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "us-east-1",
                      "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                    }
            - name: this
              manifest: |-
                {
                  "azure_managed_identity": [
                    {
                      "access_connector_id": "${var.access_connector_id}"
                    }
                  ],
                  "is_default": true,
                  "metastore_id": "${databricks_metastore.this.id}",
                  "name": "mi_dac"
                }
              references:
                azure_managed_identity.access_connector_id: var.access_connector_id
                metastore_id: databricks_metastore.this.id
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "eastus",
                      "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                    }
        argumentDocs:
            id: '- ID of this data access configuration in form of <metastore_id>|<name>.'
            is_default: '-  whether to set this credential as the default for the metastore. In practice, this should always be true.'
        importStatements: []
    databricks_mlflow_experiment:
        subCategory: ""
        name: databricks_mlflow_experiment
        title: databricks_mlflow_experiment Resource
        examples:
            - name: this
              manifest: |-
                {
                  "artifact_location": "s3://bucket/my-experiment",
                  "name": "${data.databricks_current_user.me.home}/Sample",
                  "tags": [
                    {
                      "key": "key1",
                      "value": "value1"
                    },
                    {
                      "key": "key2",
                      "value": "value2"
                    }
                  ]
                }
        argumentDocs:
            artifact_location: '- Path to artifact location of the MLflow experiment.'
            id: '- ID of the MLflow experiment.'
            name: '- (Required) Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. /Users/<some-username>/my-experiment. For more information about changes to experiment naming conventions, see mlflow docs.'
            tags: '- Tags for the MLflow experiment.'
        importStatements: []
    databricks_mlflow_model:
        subCategory: ""
        name: databricks_mlflow_model
        title: databricks_mlflow_model Resource
        examples:
            - name: test
              manifest: |-
                {
                  "description": "My MLflow model description",
                  "name": "My MLflow Model",
                  "tags": [
                    {
                      "key": "key1",
                      "value": "value1"
                    },
                    {
                      "key": "key2",
                      "value": "value2"
                    }
                  ]
                }
        argumentDocs:
            description: '- The description of the MLflow model.'
            id: '- ID of the MLflow model, the same as name.'
            name: '- (Required) Name of MLflow model. Change of name triggers new resource.'
            tags: '- Tags for the MLflow model.'
        importStatements: []
    databricks_mlflow_webhook:
        subCategory: ""
        name: databricks_mlflow_webhook
        title: databricks_mlflow_webhook Resource
        examples:
            - name: job
              manifest: |-
                {
                  "description": "Databricks Job webhook trigger",
                  "events": [
                    "TRANSITION_REQUEST_CREATED"
                  ],
                  "job_spec": [
                    {
                      "access_token": "${databricks_token.pat_for_webhook.token_value}",
                      "job_id": "${databricks_job.this.id}",
                      "workspace_url": "${data.databricks_current_user.me.workspace_url}"
                    }
                  ],
                  "status": "ACTIVE"
                }
              references:
                job_spec.access_token: databricks_token.pat_for_webhook.token_value
                job_spec.job_id: databricks_job.this.id
                job_spec.workspace_url: data.databricks_current_user.me.workspace_url
              dependencies:
                databricks_job.this: |-
                    {
                      "name": "Terraform MLflowWebhook Demo (${data.databricks_current_user.me.alphanumeric})",
                      "task": [
                        {
                          "new_cluster": [
                            {
                              "node_type_id": "${data.databricks_node_type.smallest.id}",
                              "num_workers": 1,
                              "spark_version": "${data.databricks_spark_version.latest.id}"
                            }
                          ],
                          "notebook_task": [
                            {
                              "notebook_path": "${databricks_notebook.this.path}"
                            }
                          ],
                          "task_key": "task1"
                        }
                      ]
                    }
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    import json\n \n    event_message = dbutils.widgets.get(\"event_message\")\n    event_message_dict = json.loads(event_message)\n    print(f\"event data={event_message_dict}\")\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/MLFlowWebhook"
                    }
                databricks_token.pat_for_webhook: |-
                    {
                      "comment": "MLflow Webhook",
                      "lifetime_seconds": 86400000
                    }
            - name: url
              manifest: |-
                {
                  "description": "URL webhook trigger",
                  "events": [
                    "TRANSITION_REQUEST_CREATED"
                  ],
                  "http_url_spec": [
                    {
                      "url": "https://my_cool_host/webhook"
                    }
                  ]
                }
        argumentDocs:
            access_token: '- (Required, Sensitive) The personal access token used to authorize webhook''s job runs.'
            authorization: '- (Optional) Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form <auth type> <credentials>, e.g. Bearer <access_token>. If set to an empty string, no authorization header will be included in the request.'
            description: '- Optional description of the MLflow webhook.'
            enable_ssl_verification: '- (Optional) Enable/disable SSL certificate validation. Default is true. For self-signed certificates, this field must be false AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.'
            events: '- (Required) The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, MODEL_VERSION_CREATED, MODEL_VERSION_TRANSITIONED_STAGE, TRANSITION_REQUEST_CREATED, etc.  Refer to the Webhooks API documentation for a full list of supported events.'
            id: '- Unique ID of the MLflow Webhook.'
            job_id: '- (Required) ID of the Databricks job that the webhook runs.'
            model_name: '- (Optional) Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.'
            secret: '- (Optional, Sensitive) Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as X-Databricks-Signature: encoded_payload.'
            status: '- Optional status of webhook. Possible values are ACTIVE, TEST_MODE, DISABLED. Default is ACTIVE.'
            url: '- (Required) External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to documentation for more details.'
            workspace_url: '- (Optional) URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.'
        importStatements: []
    databricks_model_serving:
        subCategory: ""
        name: databricks_model_serving
        title: databricks_model_serving Resource
        examples:
            - name: this
              manifest: |-
                {
                  "config": [
                    {
                      "served_entities": [
                        {
                          "entity_name": "ads-model",
                          "entity_version": "2",
                          "name": "prod_model",
                          "scale_to_zero_enabled": true,
                          "workload_size": "Small"
                        },
                        {
                          "entity_name": "ads-model",
                          "entity_version": "4",
                          "name": "candidate_model",
                          "scale_to_zero_enabled": false,
                          "workload_size": "Small"
                        }
                      ],
                      "traffic_config": [
                        {
                          "routes": [
                            {
                              "served_model_name": "prod_model",
                              "traffic_percentage": 90
                            },
                            {
                              "served_model_name": "candidate_model",
                              "traffic_percentage": 10
                            }
                          ]
                        }
                      ]
                    }
                  ],
                  "name": "ads-serving-endpoint"
                }
            - name: llama
              manifest: |-
                {
                  "ai_gateway": [
                    {
                      "usage_tracking_config": [
                        {
                          "enabled": true
                        }
                      ]
                    }
                  ],
                  "config": [
                    {
                      "served_entities": [
                        {
                          "entity_name": "system.ai.llama_v3_2_3b_instruct",
                          "entity_version": "2",
                          "max_provisioned_throughput": 44000,
                          "name": "meta_llama_v3_2_3b_instruct-3",
                          "scale_to_zero_enabled": true
                        }
                      ]
                    }
                  ],
                  "name": "llama_3_2_3b_instruct"
                }
            - name: gpt_4o
              manifest: |-
                {
                  "ai_gateway": [
                    {
                      "guardrails": [
                        {
                          "input": [
                            {
                              "invalid_keywords": [
                                "SuperSecretProject"
                              ],
                              "pii": [
                                {
                                  "behavior": "BLOCK"
                                }
                              ]
                            }
                          ],
                          "output": [
                            {
                              "pii": [
                                {
                                  "behavior": "BLOCK"
                                }
                              ]
                            }
                          ]
                        }
                      ],
                      "inference_table_config": [
                        {
                          "catalog_name": "ml",
                          "enabled": true,
                          "schema_name": "ai_gateway",
                          "table_name_prefix": "gpt-4o-mini"
                        }
                      ],
                      "rate_limits": [
                        {
                          "calls": 10,
                          "key": "endpoint",
                          "renewal_period": "minute"
                        }
                      ],
                      "usage_tracking_config": [
                        {
                          "enabled": true
                        }
                      ]
                    }
                  ],
                  "config": [
                    {
                      "served_entities": [
                        {
                          "external_model": [
                            {
                              "name": "gpt-4o-mini",
                              "openai_config": [
                                {
                                  "openai_api_key": "{{secrets/llm_scope/openai_api_key}}"
                                }
                              ],
                              "provider": "openai",
                              "task": "llm/v1/chat"
                            }
                          ],
                          "name": "gpt-4o-mini"
                        }
                      ]
                    }
                  ],
                  "name": "gpt-4o-mini"
                }
        argumentDocs:
            _plaintext: suffix) or in plain text (parameters with _plaintext suffix)!
            ai_gateway: '- (Optional) A block with AI Gateway configuration for the serving endpoint. Note: only external model endpoints are supported as of now.'
            ai_gateway.fallback_config: '- (Optional) block with configuration for traffic fallback which auto fallbacks to other served entities if the request to a served entity fails with certain error codes, to increase availability.'
            ai_gateway.guardrails: '- (Optional) Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:'
            ai_gateway.inference_table_config: '- (Optional) Block describing the configuration of usage tracking. Consists of the following attributes:'
            ai_gateway.rate_limits: '- (Optional) Block describing rate limits for AI gateway. For details see the description of rate_limits block above.'
            ai_gateway.usage_tracking_config: '- (Optional) Block with configuration for payload logging using inference tables. For details see the description of auto_capture_config block above.'
            ai21labs_api_key: '- The Databricks secret key reference for an AI21Labs API key.'
            ai21labs_api_key_plaintext: '- An AI21 Labs API key provided as a plaintext string.'
            ai21labs_config: '- AI21Labs Config'
            amazon_bedrock_config: '- Amazon Bedrock Config'
            anthropic_api_key: '- The Databricks secret key reference for an Anthropic API key.'
            anthropic_api_key_plaintext: '- The Anthropic API key provided as a plaintext string.'
            anthropic_config: '- Anthropic Config'
            api_key_auth: '- (Optional) API key authentication for the custom provider API. Conflicts with bearer_token_auth.'
            auto_capture_config: '- Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.'
            auto_capture_config.catalog_name: '- The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.'
            auto_capture_config.enabled: '- If inference tables are enabled or not. NOTE: If you have already disabled payload logging once, you cannot enable it again.'
            auto_capture_config.schema_name: '- The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.'
            auto_capture_config.table_name_prefix: '- The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.'
            aws_access_key_id: '- The Databricks secret key reference for an AWS Access Key ID with permissions to interact with Bedrock services.'
            aws_access_key_id_plaintext: '- An AWS access key ID with permissions to interact with Bedrock services provided as a plaintext string.'
            aws_region: '- The AWS region to use. Bedrock has to be enabled there.'
            aws_secret_access_key: '- The Databricks secret key reference for an AWS Secret Access Key paired with the access key ID, with permissions to interact with Bedrock services.'
            aws_secret_access_key_plaintext: '-  An AWS secret access key paired with the access key ID, with permissions to interact with Bedrock services provided as a plaintext string.'
            bearer_token_auth: (Optional) - bearer token authentication for the custom provider API.  Conflicts with api_key_auth.
            bedrock_provider: '- The underlying provider in Amazon Bedrock. Supported values (case insensitive) include: Anthropic, Cohere, AI21Labs, Amazon.'
            behavior: '- a string that describes the behavior for PII filter. Currently only BLOCK value is supported.'
            budget_policy_id: '- (Optiona) The Budget Policy ID set for this serving endpoint.'
            cohere_api_key: '- The Databricks secret key reference for a Cohere API key.'
            cohere_api_key_plaintext: '- The Cohere API key provided as a plaintext string.'
            cohere_config: '- Cohere Config'
            config: '- The model serving endpoint configuration. This is optional and can be added and modified after creation. If config was provided in a previous apply but is not provided in the current apply, no change to the model serving endpoint will occur. To recreate the model serving endpoint without the config block, the model serving endpoint must be destroyed and recreated.'
            custom_provider_config: '- Custom Provider Config. Only required if the provider is ''custom''.'
            custom_provider_url: (Required) - URL of the custom provider API.
            databricks_api_token: '- The Databricks secret key reference for a Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model.'
            databricks_api_token_plaintext: '- The Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model provided as a plaintext string.'
            databricks_model_serving_config: '- Databricks Model Serving Config'
            databricks_workspace_url: '- The URL of the Databricks workspace containing the model serving endpoint pointed to by this external model.'
            description: '- (Optional) The description of the model serving endpoint.'
            email_notifications: '- (Optional) A block with Email notification setting.'
            email_notifications.on_update_failure: '- (Optional) a list of email addresses to be notified when an endpoint fails to update its configuration or state.'
            email_notifications.on_update_success: '- (Optional) a list of email addresses to be notified when an endpoint successfully updates its configuration or state.'
            enabled: '-  Whether to enable traffic fallback. When a served entity in the serving endpoint returns specific error codes (e.g. 500), the request will automatically be round-robin attempted with other served entities in the same endpoint, following the order of served entity list, until a successful response is returned.'
            endpoint_url: '- Invocation url of the endpoint.'
            google_cloud_vertex_ai_config: '- Google Cloud Vertex AI Config.'
            id: '- Equal to the name argument and used to identify the serving endpoint.'
            input: '- A block with configuration for input guardrail filters:'
            instance_profile_arn: '- Optional ARN of the instance profile that the external model will use to access AWS resources. You must authenticate using an instance profile or access keys.'
            invalid_keywords: '- (Deprecated) List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.'
            key: (Required) - The name of the API key parameter used for authentication.
            microsoft_entra_client_id: '- This field is only required for Azure AD OpenAI and is the Microsoft Entra Client ID.'
            microsoft_entra_client_secret: '- The Databricks secret key reference for a client secret used for Microsoft Entra ID authentication.'
            microsoft_entra_client_secret_plaintext: '- The client secret used for Microsoft Entra ID authentication provided as a plaintext string.'
            microsoft_entra_tenant_id: '- This field is only required for Azure AD OpenAI and is the Microsoft Entra Tenant ID.'
            name: '- (Required) The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.'
            openai_api_base: '- This is the base URL for the OpenAI API (default: "https://api.openai.com/v1"). For Azure OpenAI, this field is required and is the base URL for the Azure OpenAI API service provided by Azure.'
            openai_api_key: '- The Databricks secret key reference for an OpenAI or Azure OpenAI API key.'
            openai_api_key_plaintext: '- The OpenAI API key using the OpenAI or Azure service provided as a plaintext string.'
            openai_api_type: '- This is an optional field to specify the type of OpenAI API to use. For Azure OpenAI, this field is required, and this parameter represents the preferred security access validation protocol. For access token validation, use azure. For authentication using Azure Active Directory (Azure AD) use, azuread.'
            openai_api_version: '- This is an optional field to specify the OpenAI API version. For Azure OpenAI, this field is required and is the version of the Azure OpenAI service to utilize, specified by a date.'
            openai_config: '- OpenAI Config'
            openai_deployment_name: '- This field is only required for Azure OpenAI and is the name of the deployment resource for the Azure OpenAI service.'
            openai_organization: '- This is an optional field to specify the organization in OpenAI or Azure OpenAI.'
            output: '- A block with configuration for output guardrail filters.  Has the same structure as input block.'
            palm_api_key: '- The Databricks secret key reference for a PaLM API key.'
            palm_api_key_plaintext: '- The PaLM API key provided as a plaintext string.'
            palm_config: '- PaLM Config'
            pii: '- Block with configuration for guardrail PII filter:'
            private_key: '- The Databricks secret key reference for a private key for the service account that has access to the Google Cloud Vertex AI Service.'
            private_key_plaintext: '- The private key for the service account that has access to the Google Cloud Vertex AI Service is provided as a plaintext secret.'
            project_id: '- This is the Google Cloud project id that the service account is associated with.'
            provider: '- (Required) The name of the provider for the external model. Currently, the supported providers are ai21labs, anthropic, amazon-bedrock, cohere, databricks-model-serving, google-cloud-vertex-ai, openai, and palm.'
            rate_limits: '- (Deprecated, use ai_gateway to manage rate limits) A list of rate limit blocks to be applied to the serving endpoint. Note: only external and foundation model endpoints are supported as of now.'
            rate_limits.calls: '- (Required) Used to specify how many calls are allowed for a key within the renewal_period.'
            rate_limits.key: '- (Optional) Key field for a serving endpoint rate limit. Currently, user, user_group, service_principal, and endpoint are supported, with endpoint being the default if not specified.'
            rate_limits.principal: '- (Optional) Principal field for a user, user group, or service principal to apply rate limiting to. Accepts a user email, group name, or service principal application ID.'
            rate_limits.renewal_period: '- (Required) Renewal period field for a serving endpoint rate limit. Currently, only minute is supported.'
            rate_limits.tokens: '- (Optional, int) Specifies how many tokens are allowed for a key within the renewal_period.'
            region: '- This is the region for the Google Cloud Vertex AI Service.'
            route_optimized: '- (Optional) A boolean enabling route optimization for the endpoint. Note: only available for custom models.'
            safety: '- the boolean flag that indicates whether the safety filter is enabled.'
            served_entities: '- A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities.'
            served_entities.entity_name: '- The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type FEATURE_SPEC in the UC. If it is a UC object, the full name of the object should be given in the form of catalog_name.schema_name.model_name.'
            served_entities.entity_version: '- The version of the model in Databricks Model Registry to be served or empty if the entity is a FEATURE_SPEC.'
            served_entities.environment_vars: '- An object containing a set of optional, user-specified environment variable key-value pairs used for serving this entity. Note: this is an experimental feature and is subject to change. Example entity environment variables that refer to Databricks secrets: {"OPENAI_API_KEY": "{{secrets/my_scope/my_key}}", "DATABRICKS_TOKEN": "{{secrets/my_scope2/my_key2}}"}'
            served_entities.external_model: '- The external model to be served. NOTE: Only one of external_model and (entity_name, entity_version, workload_size, workload_type, and scale_to_zero_enabled) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an external_model is present, the served entities list can only have one served_entity object. An existing endpoint with external_model can not be updated to an endpoint without external_model. If the endpoint is created without external_model, users cannot update it to add external_model later.'
            served_entities.instance_profile_arn: '- ARN of the instance profile that the served entity uses to access AWS resources.'
            served_entities.max_provisioned_concurrency: '- The maximum provisioned concurrency that the endpoint can scale up to. Conflicts with workload_size.'
            served_entities.max_provisioned_throughput: '- The maximum tokens per second that the endpoint can scale up to.'
            served_entities.min_provisioned_concurrency: '- The minimum provisioned concurrency that the endpoint can scale down to. Conflicts with workload_size.'
            served_entities.min_provisioned_throughput: '- The minimum tokens per second that the endpoint can scale down to.'
            served_entities.name: '- The name of a served entity. It must be unique across an endpoint. A served entity name can consist of alphanumeric characters, dashes, and underscores. If not specified for an external model, this field defaults to external_model.name, with ''.'' and '':'' replaced with ''-'', and if not specified for other entities, it defaults to -.'
            served_entities.scale_to_zero_enabled: '- Whether the compute resources for the served entity should scale down to zero.'
            served_entities.workload_size: '- The workload size of the served entity. The workload size corresponds to a range of provisioned concurrency that the compute autoscales between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency). If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size is 0. Conflicts with min_provisioned_concurrency and max_provisioned_concurrency.'
            served_entities.workload_type: '- The workload type of the served entity. The workload type selects which type of compute to use in the endpoint. The default value for this parameter is CPU. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the available GPU types.'
            served_entity_name: '- (Required) The name of the served entity this route configures traffic for. This needs to match the name of a served_entity block.'
            served_models: '- (Deprecated, use served_entities instead) Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.'
            served_models.environment_vars: '- (Optional) a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.'
            served_models.instance_profile_arn: '- (Optional) ARN of the instance profile that the served model will use to access AWS resources.'
            served_models.model_name: '- (Required) The name of the model in Databricks Model Registry to be served.'
            served_models.model_version: '- (Required) The version of the model in Databricks Model Registry to be served.'
            served_models.name: '- The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.'
            served_models.scale_to_zero_enabled: '- Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.'
            served_models.workload_size: '- (Required) The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency).'
            served_models.workload_type: '- The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU.'
            serving_endpoint_id: '- Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.'
            tags: '- Tags to be attached to the serving endpoint and automatically propagated to billing logs.'
            tags.key: '- The key field for a tag.'
            tags.value: '- The value field for a tag.'
            task: '- The task type of the external model.'
            token: (Optional) -  The Databricks secret key reference for a token.
            token_plaintext: (Optional) - The token provided as a plaintext string.
            traffic_config: '- A single block represents the traffic split configuration amongst the served models.'
            traffic_config.routes: '- (Required) Each block represents a route that defines traffic to each served entity. Each served_entity block needs to have a corresponding routes block.'
            traffic_percentage: '- (Required) The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.'
            valid_topics: '- (Deprecated) The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.'
            value: (Optional) - The Databricks secret key reference for an API Key.
            value_plaintext: (Optional) - The API Key provided as a plaintext string.
        importStatements: []
    databricks_model_serving_provisioned_throughput:
        subCategory: ""
        name: databricks_model_serving_provisioned_throughput
        title: databricks_model_serving_provisioned_throughput Resource
        examples:
            - name: llama
              manifest: |-
                {
                  "ai_gateway": [
                    {
                      "usage_tracking_config": [
                        {
                          "enabled": true
                        }
                      ]
                    }
                  ],
                  "config": [
                    {
                      "served_entities": [
                        {
                          "entity_name": "system.ai.llama-4-maverick",
                          "entity_version": "1",
                          "provisioned_model_units": 100
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            ai_gateway: '- (Optional) A block with AI Gateway configuration for the serving endpoint. Note: only external model endpoints are supported as of now.'
            ai_gateway.guardrails: '- (Optional) Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:'
            ai_gateway.inference_table_config: '- (Optional) Block describing the configuration of usage tracking. Consists of the following attributes:'
            ai_gateway.rate_limits: '- (Optional) Block describing rate limits for AI gateway. For details see the description of rate_limits block above.'
            ai_gateway.usage_tracking_config: '- (Optional) Block with configuration for payload logging using inference tables. For details see the description of auto_capture_config block above.'
            behavior: '- a string that describes the behavior for PII filter. Currently only BLOCK value is supported.'
            budget_policy_id: '- (Optiona) The Budget Policy ID set for this serving endpoint.'
            config: '- The model serving endpoint configuration.'
            email_notifications: '- (Optional) A block with Email notification setting.'
            email_notifications.on_update_failure: '- (Optional) a list of email addresses to be notified when an endpoint fails to update its configuration or state.'
            email_notifications.on_update_success: '- (Optional) a list of email addresses to be notified when an endpoint successfully updates its configuration or state.'
            enabled: '- boolean flag specifying if usage tracking is enabled.'
            id: '- Equal to the name argument and used to identify the serving endpoint.'
            input: '- A block with configuration for input guardrail filters:'
            invalid_keywords: '- List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.'
            name: '- (Required) The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.'
            output: '- A block with configuration for output guardrail filters.  Has the same structure as input block.'
            pii: '- Block with configuration for guardrail PII filter:'
            safety: '- the boolean flag that indicates whether the safety filter is enabled.'
            served_entities: '- A list of served entities for the endpoint to serve.'
            served_entities.entity_name: '- The full path of the UC model to be served, given in the form of catalog_name.schema_name.model_name.'
            served_entities.entity_version: '- The version of the model in UC to be served.'
            served_entities.name: '- The name of a served entity. It must be unique across an endpoint. A served entity name can consist of alphanumeric characters, dashes, and underscores. If not specified for an external model, this field will be created from the entity_name and entity_version'
            served_entities.provisioned_model_units: '- The number of model units to be provisioned.'
            served_entity_name: '- (Required) The name of the served entity this route configures traffic for. This needs to match the name of a served_entity block.'
            serving_endpoint_id: '- Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.'
            tags: '- Tags to be attached to the serving endpoint and automatically propagated to billing logs.'
            tags.key: '- The key field for a tag.'
            tags.value: '- The value field for a tag.'
            traffic_config: '- A single block represents the traffic split configuration amongst the served models.'
            traffic_config.routes: '- (Required) Each block represents a route that defines traffic to each served entity. Each served_entity block needs to have a corresponding routes block.'
            traffic_percentage: '- (Required) The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.'
            valid_topics: '- The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.'
        importStatements: []
    databricks_mount:
        subCategory: ""
        name: databricks_mount
        title: databricks_mount Resource
        examples:
            - name: this
              manifest: |-
                {
                  "extra_configs": {
                    "fs.azure.account.auth.type": "OAuth",
                    "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
                    "fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/${local.tenant_id}/oauth2/token",
                    "fs.azure.account.oauth2.client.id": "${local.client_id}",
                    "fs.azure.account.oauth2.client.secret": "{{secrets/${local.secret_scope}/${local.secret_key}}}",
                    "fs.azure.createRemoteFileSystemDuringInitialization": "false"
                  },
                  "name": "tf-abfss",
                  "uri": "abfss://${local.container}@${local.storage_acc}.dfs.core.windows.net"
                }
            - name: passthrough
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.shared_passthrough.id}",
                  "extra_configs": {
                    "fs.azure.account.auth.type": "CustomAccessToken",
                    "fs.azure.account.custom.token.provider.class": "{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}"
                  },
                  "name": "passthrough-test",
                  "uri": "abfss://${var.container}@${var.storage_acc}.dfs.core.windows.net"
                }
              references:
                cluster_id: databricks_cluster.shared_passthrough.id
              dependencies:
                databricks_cluster.shared_passthrough: |-
                    {
                      "autotermination_minutes": 10,
                      "cluster_name": "Shared Passthrough for mount",
                      "custom_tags": {
                        "ResourceClass": "Serverless"
                      },
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "num_workers": 1,
                      "spark_conf": {
                        "spark.databricks.cluster.profile": "serverless",
                        "spark.databricks.passthrough.enabled": "true",
                        "spark.databricks.pyspark.enableProcessIsolation": "true",
                        "spark.databricks.repl.allowedLanguages": "python,sql"
                      },
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
            - name: this
              manifest: |-
                {
                  "name": "experiments",
                  "s3": [
                    {
                      "bucket_name": "${aws_s3_bucket.this.bucket}",
                      "instance_profile": "${databricks_instance_profile.ds.id}"
                    }
                  ]
                }
              references:
                s3.bucket_name: aws_s3_bucket.this.bucket
                s3.instance_profile: databricks_instance_profile.ds.id
            - name: marketing
              manifest: |-
                {
                  "abfs": [
                    {
                      "client_id": "${data.azurerm_client_config.current.client_id}",
                      "client_secret_key": "${databricks_secret.service_principal_key.key}",
                      "client_secret_scope": "${databricks_secret_scope.terraform.name}",
                      "initialize_file_system": true
                    }
                  ],
                  "name": "marketing",
                  "resource_id": "${azurerm_storage_container.this.resource_manager_id}"
                }
              references:
                abfs.client_id: data.azurerm_client_config.current.client_id
                abfs.client_secret_key: databricks_secret.service_principal_key.key
                abfs.client_secret_scope: databricks_secret_scope.terraform.name
                resource_id: azurerm_storage_container.this.resource_manager_id
              dependencies:
                azurerm_role_assignment.this: |-
                    {
                      "principal_id": "${data.azurerm_client_config.current.object_id}",
                      "role_definition_name": "Storage Blob Data Contributor",
                      "scope": "${azurerm_storage_account.this.id}"
                    }
                azurerm_storage_account.this: |-
                    {
                      "account_kind": "StorageV2",
                      "account_replication_type": "GRS",
                      "account_tier": "Standard",
                      "is_hns_enabled": true,
                      "location": "${var.resource_group_location}",
                      "name": "${var.prefix}datalake",
                      "resource_group_name": "${var.resource_group_name}"
                    }
                azurerm_storage_container.this: |-
                    {
                      "container_access_type": "private",
                      "name": "marketing",
                      "storage_account_name": "${azurerm_storage_account.this.name}"
                    }
                databricks_secret.service_principal_key: |-
                    {
                      "key": "service_principal_key",
                      "scope": "${databricks_secret_scope.terraform.name}",
                      "string_value": "${var.ARM_CLIENT_SECRET}"
                    }
                databricks_secret_scope.terraform: |-
                    {
                      "initial_manage_principal": "users",
                      "name": "application"
                    }
            - name: this_gs
              manifest: |-
                {
                  "gs": [
                    {
                      "bucket_name": "mybucket",
                      "service_account": "acc@company.iam.gserviceaccount.com"
                    }
                  ],
                  "name": "gs-mount"
                }
            - name: mount
              manifest: |-
                {
                  "adl": [
                    {
                      "client_id": "${data.azurerm_client_config.current.client_id}",
                      "client_secret_key": "${databricks_secret.service_principal_key.key}",
                      "client_secret_scope": "${databricks_secret_scope.terraform.name}",
                      "spark_conf_prefix": "fs.adl",
                      "storage_resource_name": "{env.TEST_STORAGE_ACCOUNT_NAME}",
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
                  ],
                  "name": "{var.RANDOM}"
                }
              references:
                adl.client_id: data.azurerm_client_config.current.client_id
                adl.client_secret_key: databricks_secret.service_principal_key.key
                adl.client_secret_scope: databricks_secret_scope.terraform.name
                adl.tenant_id: data.azurerm_client_config.current.tenant_id
            - name: marketing
              manifest: |-
                {
                  "name": "marketing",
                  "wasb": [
                    {
                      "auth_type": "ACCESS_KEY",
                      "container_name": "${azurerm_storage_container.marketing.name}",
                      "storage_account_name": "${azurerm_storage_account.blobaccount.name}",
                      "token_secret_key": "${databricks_secret.storage_key.key}",
                      "token_secret_scope": "${databricks_secret_scope.terraform.name}"
                    }
                  ]
                }
              references:
                wasb.container_name: azurerm_storage_container.marketing.name
                wasb.storage_account_name: azurerm_storage_account.blobaccount.name
                wasb.token_secret_key: databricks_secret.storage_key.key
                wasb.token_secret_scope: databricks_secret_scope.terraform.name
              dependencies:
                azurerm_storage_account.blobaccount: |-
                    {
                      "account_kind": "StorageV2",
                      "account_replication_type": "LRS",
                      "account_tier": "Standard",
                      "location": "${var.resource_group_location}",
                      "name": "${var.prefix}blob",
                      "resource_group_name": "${var.resource_group_name}"
                    }
                azurerm_storage_container.marketing: |-
                    {
                      "container_access_type": "private",
                      "name": "marketing",
                      "storage_account_name": "${azurerm_storage_account.blobaccount.name}"
                    }
                databricks_secret.storage_key: |-
                    {
                      "key": "blob_storage_key",
                      "scope": "${databricks_secret_scope.terraform.name}",
                      "string_value": "${azurerm_storage_account.blobaccount.primary_access_key}"
                    }
                databricks_secret_scope.terraform: |-
                    {
                      "initial_manage_principal": "users",
                      "name": "application"
                    }
        argumentDocs:
            abfs: '- to mount ADLS Gen2 using Azure Blob Filesystem (ABFS) driver'
            abfs.client_id: '- (Required) (String) This is the client_id (Application Object ID) for the enterprise application for the service principal.'
            abfs.client_secret_key: '- (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.'
            abfs.client_secret_scope: '- (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.'
            abfs.container_name: '- (Required) (String) ADLS gen2 container name. (Could be omitted if resource_id is provided)'
            abfs.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            abfs.initialize_file_system: '- (Required) (Bool) either or not initialize FS for the first use'
            abfs.storage_account_name: '- (Required) (String) The name of the storage resource in which the data is. (Could be omitted if resource_id is provided)'
            abfs.tenant_id: '- (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract tenant_id from it).'
            adl: '- to mount ADLS Gen1 using Azure Data Lake (ADL) driver'
            adl.client_id: '- (Required) (String) This is the client_id for the enterprise application for the service principal.'
            adl.client_secret_key: '- (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.'
            adl.client_secret_scope: '- (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.'
            adl.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            adl.spark_conf_prefix: '- (Optional) (String) This is the spark configuration prefix for adls gen 1 mount. The options are fs.adl, dfs.adls. Use fs.adl for runtime 6.0 and above for the clusters. Otherwise use dfs.adls. The default value is: fs.adl.'
            adl.storage_resource_name: '- (Required) (String) The name of the storage resource in which the data is for ADLS gen 1. This is what you are trying to mount. (Could be omitted if resource_id is provided)'
            adl.tenant_id: '- (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract tenant_id from it)'
            bucket_name: for AWS S3 and Google Cloud Storage
            cluster_id: '- (Optional, String) Cluster to use for mounting. If no cluster is specified, a new cluster will be created and will mount the bucket for all of the clusters in this workspace. If the cluster is not running - it''s going to be started, so be aware to set auto-termination rules on it.'
            container_name: for ADLS Gen2 and Azure Blob Storage
            encryption_type: '- (Optional, String) encryption type. Currently used only for AWS S3 mounts'
            extra_configs: '- (Optional, String map) configuration parameters that are necessary for mounting of specific storage'
            gs: '- to mount Google Cloud Storage'
            gs.bucket_name: '- (Required) (String) GCS bucket name to be mounted.'
            gs.service_account: '- (Optional) (String) email of registered Google Service Account for data access.  If it''s not specified, then the cluster_id should be provided, and the cluster should have a Google service account attached to it.'
            id: '- mount name'
            mount_name: to name
            name: '- (Optional, String) Name, under which mount will be accessible in dbfs:/mnt/<MOUNT_NAME>. If not specified, provider will try to infer it from depending on the resource type:'
            resource_id: '- (Optional, String) resource ID for a given storage account. Could be used to fill defaults, such as storage account & container names on Azure.'
            s3: '- to mount AWS S3'
            s3.bucket_name: '- (Required) (String) S3 bucket name to be mounted.'
            s3.instance_profile: '- (Optional) (String) ARN of registered instance profile for data access.  If it''s not specified, then the cluster_id should be provided, and the cluster should have an instance profile attached to it. If both cluster_id & instance_profile are specified, then cluster_id takes precedence.'
            s3_bucket_name: to bucket_name
            source: '- (String) HDFS-compatible url'
            storage_resource_name: for ADLS Gen1
            uri: '- (Optional, String) the URI for accessing specific storage (s3a://...., abfss://...., gs://...., etc.)'
            wasb: '- to mount Azure Blob Storage using Windows Azure Storage Blob (WASB) driver'
            wasb.auth_type: '- (Required) (String) This is the auth type for blob storage. This can either be SAS tokens (SAS) or account access keys (ACCESS_KEY).'
            wasb.container_name: '- (Required) (String) The container in which the data is. This is what you are trying to mount. (Could be omitted if resource_id is provided)'
            wasb.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            wasb.storage_account_name: '- (Required) (String) The name of the storage resource in which the data is. (Could be omitted if resource_id is provided)'
            wasb.token_secret_key: '- (Required) (String) This is the secret key in which your auth type token is stored.'
            wasb.token_secret_scope: '- (Required) (String) This is the secret scope in which your auth type token is stored.'
        importStatements: []
    databricks_mws_credentials:
        subCategory: ""
        name: databricks_mws_credentials
        title: databricks_mws_credentials Resource
        examples:
            - name: this
              manifest: |-
                {
                  "credentials_name": "${var.prefix}-creds",
                  "provider": "${databricks.mws}",
                  "role_arn": "${aws_iam_role.cross_account_role.arn}"
                }
              references:
                provider: databricks.mws
                role_arn: aws_iam_role.cross_account_role.arn
              dependencies:
                aws_iam_role.cross_account_role: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.this.json}",
                      "name": "${var.prefix}-crossaccount",
                      "tags": "${var.tags}"
                    }
                aws_iam_role_policy.this: |-
                    {
                      "name": "${var.prefix}-policy",
                      "policy": "${data.databricks_aws_crossaccount_policy.this.json}",
                      "role": "${aws_iam_role.cross_account_role.id}"
                    }
        argumentDocs:
            account_id: '- (Deprecated) Maintained for backwards compatibility and will be removed in a later version. It should now be specified under a provider instance where host = "https://accounts.cloud.databricks.com"'
            creation_time: '- (Integer) time of credentials registration'
            credentials_id: '- (String) identifier of credentials'
            credentials_name: '- (Required) name of credentials to register'
            id: '- Canonical unique identifier for the mws credentials.'
            role_arn: '- (Required) ARN of cross-account role'
        importStatements: []
    databricks_mws_customer_managed_keys:
        subCategory: ""
        name: databricks_mws_customer_managed_keys
        title: databricks_mws_customer_managed_keys Resource
        examples:
            - name: managed_services
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_key_info": [
                    {
                      "key_alias": "${aws_kms_alias.managed_services_customer_managed_key_alias.name}",
                      "key_arn": "${aws_kms_key.managed_services_customer_managed_key.arn}"
                    }
                  ],
                  "use_cases": [
                    "MANAGED_SERVICES"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                aws_key_info.key_alias: aws_kms_alias.managed_services_customer_managed_key_alias.name
                aws_key_info.key_arn: aws_kms_key.managed_services_customer_managed_key.arn
              dependencies:
                aws_kms_alias.managed_services_customer_managed_key_alias: |-
                    {
                      "name": "alias/managed-services-customer-managed-key-alias",
                      "target_key_id": "${aws_kms_key.managed_services_customer_managed_key.key_id}"
                    }
                aws_kms_key.managed_services_customer_managed_key: |-
                    {
                      "policy": "${data.aws_iam_policy_document.databricks_managed_services_cmk.json}"
                    }
            - name: managed_services
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_key_info": [
                    {
                      "kms_key_id": "${var.cmek_resource_id}"
                    }
                  ],
                  "use_cases": [
                    "MANAGED_SERVICES"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_key_info.kms_key_id: var.cmek_resource_id
            - name: storage
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_key_info": [
                    {
                      "key_alias": "${aws_kms_alias.storage_customer_managed_key_alias.name}",
                      "key_arn": "${aws_kms_key.storage_customer_managed_key.arn}"
                    }
                  ],
                  "use_cases": [
                    "STORAGE"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                aws_key_info.key_alias: aws_kms_alias.storage_customer_managed_key_alias.name
                aws_key_info.key_arn: aws_kms_key.storage_customer_managed_key.arn
              dependencies:
                aws_kms_alias.storage_customer_managed_key_alias: |-
                    {
                      "name": "alias/storage-customer-managed-key-alias",
                      "target_key_id": "${aws_kms_key.storage_customer_managed_key.key_id}"
                    }
                aws_kms_key.storage_customer_managed_key: |-
                    {
                      "policy": "${data.aws_iam_policy_document.databricks_storage_cmk.json}"
                    }
            - name: storage
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_key_info": [
                    {
                      "kms_key_id": "${var.cmek_resource_id}"
                    }
                  ],
                  "use_cases": [
                    "STORAGE"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_key_info.kms_key_id: var.cmek_resource_id
        argumentDocs:
            MANAGED_SERVICES: '- for encryption of the workspace objects (notebooks, secrets) that are stored in the control plane'
            STORAGE: '- for encryption of the DBFS Storage & Cluster EBS Volumes'
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            aws_key_info: '- This field is a block and is documented below. This conflicts with gcp_key_info'
            aws_key_info.key_alias: '- (Optional) The AWS KMS key alias.'
            aws_key_info.key_arn: '- The AWS KMS key''s Amazon Resource Name (ARN).'
            aws_key_info.key_region: '- (Optional) (Computed) The AWS region in which KMS key is deployed to. This is not required.'
            creation_time: '- (Integer) Time in epoch milliseconds when the customer key was created.'
            customer_managed_key_id: '- (String) ID of the encryption key configuration object.'
            gcp_key_info: '- This field is a block and is documented below. This conflicts with aws_key_info'
            gcp_key_info.kms_key_id: '- The GCP KMS key''s resource name.'
            id: '- Canonical unique identifier for the mws customer managed keys.'
            use_cases: '- (since v0.3.4) List of use cases for which this key will be used. If you''ve used the resource before, please add  Possible values are:'
            use_cases = ["MANAGED_SERVICES"]: to keep the previous behaviour.
        importStatements: []
    databricks_mws_log_delivery:
        subCategory: ""
        name: databricks_mws_log_delivery
        title: databricks_mws_log_delivery Resource
        examples:
            - name: usage_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Usage Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "billable-usage",
                  "log_type": "BILLABLE_USAGE",
                  "output_format": "CSV",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
              dependencies:
                aws_iam_role.logdelivery: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.logdelivery.json}",
                      "description": "(${var.prefix}) UsageDelivery role",
                      "name": "${var.prefix}-logdelivery",
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket.logdelivery: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-logdelivery",
                      "force_destroy": true,
                      "tags": "${merge(var.tags, {\n    Name = \"${var.prefix}-logdelivery\"\n  })}"
                    }
                aws_s3_bucket_policy.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "policy": "${data.databricks_aws_bucket_policy.logdelivery.json}"
                    }
                aws_s3_bucket_public_access_block.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "ignore_public_acls": true
                    }
                aws_s3_bucket_versioning.logdelivery_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.log_writer: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "Usage Delivery",
                      "depends_on": [
                        "${time_sleep.wait}"
                      ],
                      "role_arn": "${aws_iam_role.logdelivery.arn}"
                    }
                databricks_mws_storage_configurations.log_bucket: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.logdelivery.bucket}",
                      "storage_configuration_name": "Usage Logs"
                    }
                time_sleep.wait: |-
                    {
                      "create_duration": "10s",
                      "depends_on": [
                        "${aws_iam_role.logdelivery}"
                      ]
                    }
            - name: audit_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Audit Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "audit-logs",
                  "log_type": "AUDIT_LOGS",
                  "output_format": "JSON",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
              dependencies:
                aws_iam_role.logdelivery: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.logdelivery.json}",
                      "description": "(${var.prefix}) UsageDelivery role",
                      "name": "${var.prefix}-logdelivery",
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket.logdelivery: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-logdelivery",
                      "force_destroy": true,
                      "tags": "${merge(var.tags, {\n    Name = \"${var.prefix}-logdelivery\"\n  })}"
                    }
                aws_s3_bucket_policy.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "policy": "${data.databricks_aws_bucket_policy.logdelivery.json}"
                    }
                aws_s3_bucket_public_access_block.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "ignore_public_acls": true
                    }
                aws_s3_bucket_versioning.logdelivery_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.log_writer: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "Usage Delivery",
                      "depends_on": [
                        "${time_sleep.wait}"
                      ],
                      "role_arn": "${aws_iam_role.logdelivery.arn}"
                    }
                databricks_mws_storage_configurations.log_bucket: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.logdelivery.bucket}",
                      "storage_configuration_name": "Usage Logs"
                    }
                time_sleep.wait: |-
                    {
                      "create_duration": "10s",
                      "depends_on": [
                        "${aws_iam_role.logdelivery}"
                      ]
                    }
            - name: usage_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Usage Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "billable-usage",
                  "log_type": "BILLABLE_USAGE",
                  "output_format": "CSV",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
            - name: audit_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Audit Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "audit-logs",
                  "log_type": "AUDIT_LOGS",
                  "output_format": "JSON",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console.'
            config_id: '- Databricks log delivery configuration ID.'
            config_name: '- The optional human-readable name of the log delivery configuration. Defaults to empty.'
            credentials_id: '- The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.'
            delivery_path_prefix: '- (Optional) Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.'
            delivery_start_time: '- (Optional) The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.'
            id: '- the ID of log delivery configuration in form of account_id|config_id.'
            log_type: '- The type of log delivery. BILLABLE_USAGE and AUDIT_LOGS are supported.'
            output_format: '- The file type of log delivery. Currently CSV (for BILLABLE_USAGE) and JSON (for AUDIT_LOGS) are supported.'
            status: '- Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.'
            storage_configuration_id: '- The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.'
            workspace_ids_filter: '- (Optional) By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the multitenant version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.'
        importStatements: []
    databricks_mws_ncc_binding:
        subCategory: ""
        name: databricks_mws_ncc_binding
        title: databricks_mws_ncc_binding Resource
        examples:
            - name: ncc_binding
              manifest: |-
                {
                  "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                  "provider": "${databricks.account}",
                  "workspace_id": "${var.databricks_workspace_id}"
                }
              references:
                network_connectivity_config_id: databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id
                provider: databricks.account
                workspace_id: var.databricks_workspace_id
              dependencies:
                databricks_mws_network_connectivity_config.ncc: |-
                    {
                      "name": "ncc-for-${var.prefix}",
                      "provider": "${databricks.account}",
                      "region": "${var.region}"
                    }
        argumentDocs:
            network_connectivity_config_id: '- Canonical unique identifier of Network Connectivity Config in Databricks Account.'
            workspace_id: '- Identifier of the workspace to attach the NCC to. Change forces creation of a new resource.'
        importStatements: []
    databricks_mws_ncc_private_endpoint_rule:
        subCategory: ""
        name: databricks_mws_ncc_private_endpoint_rule
        title: databricks_mws_ncc_private_endpoint_rule Resource
        examples:
            - name: storage
              manifest: |-
                {
                  "group_id": "blob",
                  "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                  "provider": "${databricks.account}",
                  "resource_id": "/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa"
                }
              references:
                network_connectivity_config_id: databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id
                provider: databricks.account
              dependencies:
                databricks_mws_network_connectivity_config.ncc: |-
                    {
                      "name": "ncc-for-${var.prefix}",
                      "provider": "${databricks.account}",
                      "region": "${var.region}"
                    }
            - name: slb
              manifest: |-
                {
                  "domain_names": [
                    "my-example.exampledomain.com"
                  ],
                  "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                  "provider": "${databricks.account}",
                  "resource_id": "/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Network/privatelinkServices/example-private-link-service"
                }
              references:
                network_connectivity_config_id: databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id
                provider: databricks.account
              dependencies:
                databricks_mws_network_connectivity_config.ncc: |-
                    {
                      "name": "ncc-for-${var.prefix}",
                      "provider": "${databricks.account}",
                      "region": "${var.region}"
                    }
            - name: storage
              manifest: |-
                {
                  "endpoint_service": "com.amazonaws.us-east-1.s3",
                  "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                  "provider": "${databricks.account}",
                  "resource_names": [
                    "bucket"
                  ]
                }
              references:
                network_connectivity_config_id: databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id
                provider: databricks.account
              dependencies:
                databricks_mws_network_connectivity_config.ncc: |-
                    {
                      "name": "ncc-for-${var.prefix}",
                      "provider": "${databricks.account}",
                      "region": "${var.region}"
                    }
            - name: vpce
              manifest: |-
                {
                  "domain_names": [
                    "subdomain.internal.net"
                  ],
                  "endpoint_service": "com.amazonaws.vpce.us-west-2.vpce-svc-xyz",
                  "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                  "provider": "${databricks.account}"
                }
              references:
                network_connectivity_config_id: databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id
                provider: databricks.account
              dependencies:
                databricks_mws_network_connectivity_config.ncc: |-
                    {
                      "name": "ncc-for-${var.prefix}",
                      "provider": "${databricks.account}",
                      "region": "${var.region}"
                    }
        argumentDocs:
            DISCONNECTED: ': Connection was removed by the private link resource owner, the private endpoint becomes informative and should be deleted for clean-up.'
            ESTABLISHED: ': The endpoint has been approved and is ready to be used in your serverless compute resources.'
            EXPIRED: ': If the endpoint was created but not approved in 14 days, it will be EXPIRED.'
            PENDING: ': The endpoint has been created and pending approval.'
            REJECTED: ': Connection was rejected by the private link resource owner.'
            connection_state: |-
                - The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.
                The possible values are:
            creation_time: '- Time in epoch milliseconds when this object was created.'
            deactivated: '- Whether this private endpoint is deactivated.'
            deactivated_at: '- Time in epoch milliseconds when this object was deactivated.'
            domain_names: ""
            enabled: '- (AWS only) Activation status. Only used by private endpoints towards an AWS S3 service. Update this field to activate/deactivate this private endpoint to allow egress access from serverless compute resources. Can only be updated after a private endpoint rule towards an AWS S3 service is successfully created.'
            endpoint_name: '- The name of the Azure private endpoint resource, e.g. "databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234"'
            endpoint_service: '- (AWS only) Example com.amazonaws.vpce.us-east-1.vpce-svc-123abcc1298abc123. The full target AWS endpoint service name that connects to the destination resources of the private endpoint. Change forces creation of a new resource.'
            group_id: '- (Azure only) Not used by customer-managed private endpoint services. The sub-resource type (group ID) of the target resource. Must be one of supported resource types (i.e., blob, dfs, sqlServer , etc. Consult the Azure documentation for full list of supported resources). Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for blob and one for dfs. Change forces creation of a new resource. Conflicts with domain_names.'
            network_connectivity_config_id: '- Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.'
            resource_id: '- (Azure only) The Azure resource ID of the target resource. Change forces creation of a new resource.'
            resource_names: .
            rule_id: '- the ID of a private endpoint rule.'
            updated_time: '- Time in epoch milliseconds when this object was updated.'
            vpc_endpoint_id: '- The AWS VPC endpoint ID. You can use this ID to identify the VPC endpoint created by Databricks.'
        importStatements: []
    databricks_mws_network_connectivity_config:
        subCategory: ""
        name: databricks_mws_network_connectivity_config
        title: databricks_mws_network_connectivity_config Resource
        examples:
            - name: ncc
              manifest: |-
                {
                  "name": "ncc-for-${var.prefix}",
                  "provider": "${databricks.account}",
                  "region": "${var.region}"
                }
              references:
                provider: databricks.account
                region: var.region
              dependencies:
                databricks_mws_ncc_binding.ncc_binding: |-
                    {
                      "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                      "provider": "${databricks.account}",
                      "workspace_id": "${var.databricks_workspace_id}"
                    }
        argumentDocs:
            aws_private_endpoint_rules: (AWS only) - list containing information about configure AWS Private Endpoints.
            aws_stable_ip_rule: '(AWS only) - block with information about stable AWS IP CIDR blocks. You can use these to configure the firewall of your resources to allow traffic from your Databricks workspace.  Consists of the following fields:'
            azure_private_endpoint_rules: (Azure only) - list containing information about configure Azure Private Endpoints.
            azure_service_endpoint_rule: '(Azure only) - block with information about stable Azure service endpoints. You can configure the firewall of your Azure resources to allow traffic from your Databricks serverless compute resources.  Consists of the following fields:'
            cidr_blocks: '- list of IP CIDR blocks.'
            creation_time: '- time in epoch milliseconds when this object was created.'
            default_rules: '- block describing network connectivity rules that are applied by default without resource specific configurations.  Consists of the following fields:'
            egress_config: '- block containing information about network connectivity rules that apply to network traffic from your serverless compute resources. Consists of the following fields:'
            id: '- combination of account_id and network_connectivity_config_id separated by / character'
            name: '- Name of the network connectivity configuration. The name can contain alphanumeric characters, hyphens, and underscores. The length must be between 3 and 30 characters. The name must match the regular expression ^[0-9a-zA-Z-_]{3,30}$. Change forces creation of a new resource.'
            network_connectivity_config_id: '- Canonical unique identifier of Network Connectivity Config in Databricks Account'
            region: '- Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource.'
            subnets: '- list of subnets from which Databricks network traffic originates when accessing your Azure resources.'
            target_region: '- the Azure region in which this service endpoint rule applies.'
            target_rules: '- block describing network connectivity rules that configured for each destinations. These rules override default rules.  Consists of the following fields:'
            target_services: '- the Azure services to which this service endpoint rule applies to.'
            updated_time: '- time in epoch milliseconds when this object was updated.'
        importStatements: []
    databricks_mws_networks:
        subCategory: ""
        name: databricks_mws_networks
        title: databricks_mws_networks Resource
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "network_name": "${local.prefix}-network",
                  "provider": "${databricks.mws}",
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "subnet_ids": "${module.vpc.private_subnets}",
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                security_group_ids: module.vpc.default_security_group_id
                subnet_ids: module.vpc.private_subnets
                vpc_id: module.vpc.vpc_id
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "depends_on": [
                    "${aws_vpc_endpoint.workspace}",
                    "${aws_vpc_endpoint.relay}"
                  ],
                  "network_name": "${local.prefix}-network",
                  "provider": "${databricks.mws}",
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "subnet_ids": "${module.vpc.private_subnets}",
                  "vpc_endpoints": [
                    {
                      "dataplane_relay": [
                        "${databricks_mws_vpc_endpoint.relay.vpc_endpoint_id}"
                      ],
                      "rest_api": [
                        "${databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id}"
                      ]
                    }
                  ],
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                security_group_ids: module.vpc.default_security_group_id
                subnet_ids: module.vpc.private_subnets
                vpc_endpoints.dataplane_relay: databricks_mws_vpc_endpoint.relay.vpc_endpoint_id
                vpc_endpoints.rest_api: databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id
                vpc_id: module.vpc.vpc_id
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_network_info": [
                    {
                      "network_project_id": "${var.google_project}",
                      "subnet_id": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.name}",
                      "subnet_region": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.region}",
                      "vpc_id": "${google_compute_network.dbx_private_vpc.name}"
                    }
                  ],
                  "network_name": "test-demo-${random_string.suffix.result}"
                }
              references:
                account_id: var.databricks_account_id
                gcp_network_info.network_project_id: var.google_project
                gcp_network_info.subnet_id: google_compute_subnetwork.network_with_private_secondary_ip_ranges.name
                gcp_network_info.subnet_region: google_compute_subnetwork.network_with_private_secondary_ip_ranges.region
                gcp_network_info.vpc_id: google_compute_network.dbx_private_vpc.name
              dependencies:
                google_compute_network.dbx_private_vpc: |-
                    {
                      "auto_create_subnetworks": false,
                      "name": "tf-network-${random_string.suffix.result}",
                      "project": "${var.google_project}"
                    }
                google_compute_router.router: |-
                    {
                      "name": "my-router-${random_string.suffix.result}",
                      "network": "${google_compute_network.dbx_private_vpc.id}",
                      "region": "${google_compute_subnetwork.network-with-private-secondary-ip-ranges.region}"
                    }
                google_compute_router_nat.nat: |-
                    {
                      "name": "my-router-nat-${random_string.suffix.result}",
                      "nat_ip_allocate_option": "AUTO_ONLY",
                      "region": "${google_compute_router.router.region}",
                      "router": "${google_compute_router.router.name}",
                      "source_subnetwork_ip_ranges_to_nat": "ALL_SUBNETWORKS_ALL_IP_RANGES"
                    }
                google_compute_subnetwork.network-with-private-secondary-ip-ranges: |-
                    {
                      "ip_cidr_range": "10.0.0.0/16",
                      "name": "test-dbx-${random_string.suffix.result}",
                      "network": "${google_compute_network.dbx_private_vpc.id}",
                      "private_ip_google_access": true,
                      "region": "us-central1"
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_network_info": [
                    {
                      "network_project_id": "${var.google_project}",
                      "subnet_id": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.name}",
                      "subnet_region": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.region}",
                      "vpc_id": "${google_compute_network.dbx_private_vpc.name}"
                    }
                  ],
                  "network_name": "test-demo-${random_string.suffix.result}",
                  "vpc_endpoints": [
                    {
                      "dataplane_relay": [
                        "${databricks_mws_vpc_endpoint.relay.vpc_endpoint_id}"
                      ],
                      "rest_api": [
                        "${databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id}"
                      ]
                    }
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_network_info.network_project_id: var.google_project
                gcp_network_info.subnet_id: google_compute_subnetwork.network_with_private_secondary_ip_ranges.name
                gcp_network_info.subnet_region: google_compute_subnetwork.network_with_private_secondary_ip_ranges.region
                gcp_network_info.vpc_id: google_compute_network.dbx_private_vpc.name
                vpc_endpoints.dataplane_relay: databricks_mws_vpc_endpoint.relay.vpc_endpoint_id
                vpc_endpoints.rest_api: databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            gcp_network_info: '- (GCP only) a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:'
            id: '- Canonical unique identifier for the mws networks.'
            network_id: '- (String) id of network to be used for databricks_mws_workspaces resource.'
            network_name: '- name under which this network is registered'
            network_project_id: '- The Google Cloud project ID of the VPC network.'
            security_group_ids: '- (AWS only) ids of aws_security_group'
            subnet_id: '- The ID of the subnet associated with this network.'
            subnet_ids: '- (AWS only) ids of aws_subnet'
            subnet_region: '- The Google Cloud region of the workspace data plane. For example, us-east4.'
            vpc_endpoints: '- (Optional) mapping of databricks_mws_vpc_endpoint for PrivateLink or Private Service Connect connections'
            vpc_id: '- (AWS only) aws_vpc id'
            vpc_status: '- (String) VPC attachment status'
            workspace_id: '- (Integer) id of associated workspace'
        importStatements: []
    databricks_mws_permission_assignment:
        subCategory: ""
        name: databricks_mws_permission_assignment
        title: databricks_mws_permission_assignment Resource
        examples:
            - name: add_admin_group
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${databricks_group.data_eng.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_group.data_eng.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_group.data_eng: |-
                    {
                      "display_name": "Data Engineering"
                    }
            - name: add_user
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${databricks_user.me.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_user.me.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_user.me: |-
                    {
                      "user_name": "me@example.com"
                    }
            - name: add_admin_spn
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${databricks_service_principal.sp.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_service_principal.sp.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_service_principal.sp: |-
                    {
                      "display_name": "Automation-only SP"
                    }
        argumentDocs:
            '"ADMIN"': '- Adds principal to the workspace admins group. This gives workspace admin privileges to manage users and groups, workspace configurations, and more.'
            '"USER"': '- Adds principal to the workspace users group. This gives basic workspace access.'
            id: '- ID of the permission assignment in form of workspace_id|principal_id.'
            permissions: '- The list of workspace permissions to assign to the principal:'
            principal_id: '- Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks_service_principal or databricks_group data sources.'
            workspace_id: '- Databricks workspace ID.'
        importStatements: []
    databricks_mws_private_access_settings:
        subCategory: ""
        name: databricks_mws_private_access_settings
        title: databricks_mws_private_access_settings Resource
        examples:
            - name: pas
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "private_access_settings_name": "Private Access Settings for ${local.prefix}",
                  "provider": "${databricks.mws}",
                  "public_access_enabled": true,
                  "region": "${var.region}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                region: var.region
        argumentDocs:
            allowed_vpc_endpoint_ids: '- (Optional) An array of databricks_mws_vpc_endpoint vpc_endpoint_id (not id). Only used when private_access_level is set to ENDPOINT. This is an allow list of databricks_mws_vpc_endpoint that in your account that can connect to your databricks_mws_workspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting public_access_enabled to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.'
            id: '- the ID of the Private Access Settings in form of account_id/private_access_settings_id.'
            private_access_level: '- (Optional) The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. ACCOUNT level access (default) lets only databricks_mws_vpc_endpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. ENDPOINT level access lets only specified databricks_mws_vpc_endpoint connect to your workspace. Please see the allowed_vpc_endpoint_ids documentation for more details.'
            private_access_settings_id: '- Canonical unique identifier of Private Access Settings in Databricks Account'
            private_access_settings_name: '- Name of Private Access Settings in Databricks Account'
            public_access_enabled: (Boolean, Optional, false by default on AWS, true by default on GCP) - If true, the databricks_mws_workspaces can be accessed over the databricks_mws_vpc_endpoint as well as over the public network. In such a case, you could also configure an databricks_ip_access_list for the workspace, to restrict the source networks that could be used to access it over the public network. If false, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.
            region: '- Region of AWS VPC or the Google Cloud VPC network'
            status: '- (AWS only) Status of Private Access Settings'
        importStatements: []
    databricks_mws_storage_configurations:
        subCategory: ""
        name: databricks_mws_storage_configurations
        title: databricks_mws_storage_configurations Resource
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "bucket_name": "${aws_s3_bucket.root_storage_bucket.bucket}",
                  "provider": "${databricks.mws}",
                  "storage_configuration_name": "${var.prefix}-storage"
                }
              references:
                account_id: var.databricks_account_id
                bucket_name: aws_s3_bucket.root_storage_bucket.bucket
                provider: databricks.mws
              dependencies:
                aws_s3_bucket.root_storage_bucket: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-rootbucket"
                    }
                aws_s3_bucket_versioning.root_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            bucket_name: '- name of AWS S3 bucket'
            id: '- Canonical unique identifier for the mws storage configurations.'
            storage_configuration_id: '- (String) id of storage config to be used for databricks_mws_workspace resource.'
            storage_configuration_name: '- name under which this storage configuration is stored'
        importStatements: []
    databricks_mws_vpc_endpoint:
        subCategory: ""
        name: databricks_mws_vpc_endpoint
        title: databricks_mws_vpc_endpoint Resource
        examples:
            - name: workspace
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_vpc_endpoint_id": "${aws_vpc_endpoint.workspace.id}",
                  "depends_on": [
                    "${aws_vpc_endpoint.workspace}"
                  ],
                  "provider": "${databricks.mws}",
                  "region": "${var.region}",
                  "vpc_endpoint_name": "VPC Relay for ${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                aws_vpc_endpoint_id: aws_vpc_endpoint.workspace.id
                provider: databricks.mws
                region: var.region
            - name: relay
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_vpc_endpoint_id": "${aws_vpc_endpoint.relay.id}",
                  "depends_on": [
                    "${aws_vpc_endpoint.relay}"
                  ],
                  "provider": "${databricks.mws}",
                  "region": "${var.region}",
                  "vpc_endpoint_name": "VPC Relay for ${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                aws_vpc_endpoint_id: aws_vpc_endpoint.relay.id
                provider: databricks.mws
                region: var.region
            - name: workspace
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_vpc_endpoint_info": [
                    {
                      "endpoint_region": "${var.subnet_region}",
                      "project_id": "${var.google_project}",
                      "psc_endpoint_name": "PSC Rest API endpoint"
                    }
                  ],
                  "provider": "${databricks.mws}",
                  "vpc_endpoint_name": "PSC Rest API endpoint"
                }
              references:
                account_id: var.databricks_account_id
                gcp_vpc_endpoint_info.endpoint_region: var.subnet_region
                gcp_vpc_endpoint_info.project_id: var.google_project
                provider: databricks.mws
            - name: relay
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_vpc_endpoint_info": [
                    {
                      "endpoint_region": "${var.subnet_region}",
                      "project_id": "${var.google_project}",
                      "psc_endpoint_name": "PSC Relay endpoint"
                    }
                  ],
                  "provider": "${databricks.mws}",
                  "vpc_endpoint_name": "PSC Relay endpoint"
                }
              references:
                account_id: var.databricks_account_id
                gcp_vpc_endpoint_info.endpoint_region: var.subnet_region
                gcp_vpc_endpoint_info.project_id: var.google_project
                provider: databricks.mws
        argumentDocs:
            account_id: '- Account Id that could be found in the Accounts Console for AWS or GCP'
            aws_endpoint_service_id: '- (AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the Databricks PrivateLink documentation'
            aws_vpc_endpoint_id: '- (AWS only) ID of configured aws_vpc_endpoint'
            endpoint_region: '- Region of the PSC endpoint.'
            gcp_vpc_endpoint_info: '- (GCP only) a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:'
            id: '- the ID of VPC Endpoint in form of account_id/vpc_endpoint_id'
            project_id: '- The Google Cloud project ID of the VPC network where the PSC connection resides.'
            psc_connection_id: '- The unique ID of this PSC connection.'
            psc_endpoint_name: '- The name of the PSC endpoint in the Google Cloud project.'
            region: '- (AWS only) Region of AWS VPC'
            service_attachment_id: '- The service attachment this PSC connection connects to.'
            state: '- (AWS Only) State of VPC Endpoint'
            vpc_endpoint_id: '- Canonical unique identifier of VPC Endpoint in Databricks Account'
            vpc_endpoint_name: '- Name of VPC Endpoint in Databricks Account'
        importStatements: []
    databricks_mws_workspaces:
        subCategory: ""
        name: databricks_mws_workspaces
        title: databricks_mws_workspaces Resource
        examples:
            - name: serverless_workspace
              manifest: |-
                {
                  "account_id": "",
                  "aws_region": "us-east-1",
                  "compute_mode": "SERVERLESS",
                  "workspace_name": "serverless-workspace"
                }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_region": "${var.region}",
                  "credentials_id": "${databricks_mws_credentials.this.credentials_id}",
                  "network_id": "${databricks_mws_networks.this.network_id}",
                  "provider": "${databricks.mws}",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.this.storage_configuration_id}",
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                aws_region: var.region
                credentials_id: databricks_mws_credentials.this.credentials_id
                network_id: databricks_mws_networks.this.network_id
                provider: databricks.mws
                storage_configuration_id: databricks_mws_storage_configurations.this.storage_configuration_id
                workspace_name: var.prefix
              dependencies:
                databricks_mws_credentials.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "${var.prefix}-creds",
                      "provider": "${databricks.mws}",
                      "role_arn": "${var.crossaccount_arn}"
                    }
                databricks_mws_networks.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "network_name": "${var.prefix}-network",
                      "provider": "${databricks.mws}",
                      "security_group_ids": [
                        "${var.security_group}"
                      ],
                      "subnet_ids": "${var.subnets_private}",
                      "vpc_id": "${var.vpc_id}"
                    }
                databricks_mws_storage_configurations.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${var.root_bucket}",
                      "provider": "${databricks.mws}",
                      "storage_configuration_name": "${var.prefix}-storage"
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_region": "us-east-1",
                  "credentials_id": "${databricks_mws_credentials.this.credentials_id}",
                  "custom_tags": {
                    "SoldToCode": "1234"
                  },
                  "provider": "${databricks.mws}",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.this.storage_configuration_id}",
                  "workspace_name": "${local.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.this.credentials_id
                provider: databricks.mws
                storage_configuration_id: databricks_mws_storage_configurations.this.storage_configuration_id
                workspace_name: local.prefix
              dependencies:
                aws_iam_role.cross_account_role: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.this.json}",
                      "name": "${local.prefix}-crossaccount",
                      "tags": "${var.tags}"
                    }
                aws_iam_role_policy.this: |-
                    {
                      "name": "${local.prefix}-policy",
                      "policy": "${data.databricks_aws_crossaccount_policy.this.json}",
                      "role": "${aws_iam_role.cross_account_role.id}"
                    }
                aws_s3_bucket.root_storage_bucket: |-
                    {
                      "acl": "private",
                      "bucket": "${local.prefix}-rootbucket",
                      "force_destroy": true,
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket_policy.root_bucket_policy: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "depends_on": [
                        "${aws_s3_bucket_public_access_block.root_storage_bucket}"
                      ],
                      "policy": "${data.databricks_aws_bucket_policy.this.json}"
                    }
                aws_s3_bucket_public_access_block.root_storage_bucket: |-
                    {
                      "block_public_acls": true,
                      "block_public_policy": true,
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "depends_on": [
                        "${aws_s3_bucket.root_storage_bucket}"
                      ],
                      "ignore_public_acls": true,
                      "restrict_public_buckets": true
                    }
                aws_s3_bucket_server_side_encryption_configuration.root_storage_bucket: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.bucket}",
                      "rule": [
                        {
                          "apply_server_side_encryption_by_default": [
                            {
                              "sse_algorithm": "AES256"
                            }
                          ]
                        }
                      ]
                    }
                aws_s3_bucket_versioning.root_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "${local.prefix}-creds",
                      "provider": "${databricks.mws}",
                      "role_arn": "${aws_iam_role.cross_account_role.arn}"
                    }
                databricks_mws_storage_configurations.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.root_storage_bucket.bucket}",
                      "provider": "${databricks.mws}",
                      "storage_configuration_name": "${local.prefix}-storage"
                    }
                random_string.naming: |-
                    {
                      "length": 6,
                      "special": false,
                      "upper": false
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "cloud_resource_container": [
                    {
                      "gcp": [
                        {
                          "project_id": "${var.google_project}"
                        }
                      ]
                    }
                  ],
                  "location": "${var.subnet_region}",
                  "network_id": "${databricks_mws_networks.this.network_id}",
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                cloud_resource_container.gcp.project_id: var.google_project
                location: var.subnet_region
                network_id: databricks_mws_networks.this.network_id
                workspace_name: var.prefix
              dependencies:
                databricks_mws_networks.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "gcp_network_info": [
                        {
                          "network_project_id": "${var.google_project}",
                          "pod_ip_range_name": "pods",
                          "service_ip_range_name": "svc",
                          "subnet_id": "${var.subnet_id}",
                          "subnet_region": "${var.subnet_region}",
                          "vpc_id": "${var.vpc_id}"
                        }
                      ],
                      "network_name": "${var.prefix}-network"
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "cloud_resource_container": [
                    {
                      "gcp": [
                        {
                          "project_id": "${data.google_client_config.current.project}"
                        }
                      ]
                    }
                  ],
                  "location": "${data.google_client_config.current.region}",
                  "provider": "${databricks.accounts}",
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                cloud_resource_container.gcp.project_id: data.google_client_config.current.project
                location: data.google_client_config.current.region
                provider: databricks.accounts
                workspace_name: var.prefix
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console.'
            aws_region: '- (AWS only) region of VPC.'
            cloud_resource_container: '- (GCP only) A block that specifies GCP workspace configurations, consisting of following blocks:'
            compute_mode: '- (Optional) - The compute mode for the workspace. When unset, a classic workspace is created, and both credentials_id and storage_configuration_id must be specified. When set to SERVERLESS, the resulting workspace is a serverless workspace, and credentials_id and storage_configuration_id must not be set. The only allowed value for this is SERVERLESS. Changing this field requires recreation of the workspace.'
            creation_time: '- (Integer) time when workspace was created'
            credentials_id: '- (AWS only, Optional) credentials_id from credentials. This must not be specified when compute_mode is set to SERVERLESS.'
            custom_tags: '- (Optional / AWS only) - The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any default_tags or custom_tags on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.'
            deployment_name: '- (Optional) part of URL as in https://<prefix>-<deployment-name>.cloud.databricks.com. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.'
            effective_compute_mode: '- (String) The effective compute mode for the workspace. This is either SERVERLESS for serverless workspaces or HYBRID for classic workspaces.'
            gcp: '- A block that consists of the following field:'
            gcp_workspace_sa: '- (String, GCP only) identifier of a service account created for the workspace in form of db-<workspace-id>@prod-gcp-<region>.iam.gserviceaccount.com'
            id: '- (String) Canonical unique identifier for the workspace, of the format <account-id>/<workspace-id>'
            location: '- (GCP only) region of the subnet.'
            managed_services_customer_managed_key_id: '- (Optional) customer_managed_key_id from customer managed keys with use_cases set to MANAGED_SERVICES. This is used to encrypt the workspace''s notebook and secret data in the control plane.'
            network_id: '- (Optional) network_id from networks.'
            pricing_tier: '- (Optional) - The pricing tier of the workspace.'
            private_access_settings_id: '- (Optional) Canonical unique identifier of databricks_mws_private_access_settings in Databricks Account.'
            project_id: '- The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace.'
            storage_configuration_id: '- (AWS only, Optional) storage_configuration_id from storage configuration. This must not be specified when compute_mode is set to SERVERLESS.'
            storage_customer_managed_key_id: '- (Optional) customer_managed_key_id from customer managed keys with use_cases set to STORAGE. This is used to encrypt the DBFS Storage & Cluster Volumes.'
            token {}.comment: '- (Optional) Comment, that will appear in "User Settings / Access Tokens" page on Workspace UI. By default it''s "Terraform PAT".'
            token {}.lifetime_seconds: '- (Optional) Token expiry lifetime. By default its 2592000 (30 days).'
            workspace_id: '- (String) workspace id'
            workspace_name: '- name of the workspace, will appear on UI.'
            workspace_status: '- (String) workspace status'
            workspace_status_message: '- (String) updates on workspace status'
            workspace_url: '- (String) URL of the workspace'
        importStatements: []
    databricks_notebook:
        subCategory: ""
        name: databricks_notebook
        title: databricks_notebook Resource
        examples:
            - name: ddl
              manifest: |-
                {
                  "path": "${data.databricks_current_user.me.home}/AA/BB/CC",
                  "source": "${path.module}/DDLgen.py"
                }
            - name: notebook
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    # created from ${abspath(path.module)}\n    display(spark.range(10))\n    EOT\n  )}",
                  "language": "PYTHON",
                  "path": "/Shared/Demo"
                }
            - name: lesson
              manifest: |-
                {
                  "path": "/Shared/Intro",
                  "source": "${path.module}/IntroNotebooks.dbc"
                }
        argumentDocs:
            content_base64: '- The base64-encoded notebook source code. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a notebook with configuration properties for a data pipeline.'
            id: '-  Path of notebook on workspace'
            language: '-  (required with content_base64) One of SCALA, PYTHON, SQL, R.'
            object_id: '-  Unique identifier for a NOTEBOOK'
            path: '-  (Required) The absolute path of the notebook or directory, beginning with "/", e.g. "/Demo".'
            source: '- Path to notebook in source code format on local filesystem. Conflicts with content_base64.'
            url: '- Routable URL of the notebook'
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_notification_destination:
        subCategory: ""
        name: databricks_notification_destination
        title: databricks_notification_destination Resource
        examples:
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "email": [
                        {
                          "addresses": [
                            "abc@gmail.com"
                          ]
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "slack": [
                        {
                          "url": "https://hooks.slack.com/services/..."
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "pagerduty": [
                        {
                          "integration_key": "xxxxxx"
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "microsoft_teams": [
                        {
                          "url": "https://outlook.office.com/webhook/..."
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "generic_webhook": [
                        {
                          "password": "password",
                          "url": "https://example.com/webhook",
                          "username": "username"
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
        argumentDocs:
            addresses: '- (Required) The list of email addresses to send notifications to.'
            channel_id: '- (Optional) Slack channel ID for notifications.'
            config: '- (Required) The configuration of the Notification Destination. It must contain exactly one of the following blocks:'
            destination_type: '- the type of Notification Destination.'
            display_name: '- (Required) The display name of the Notification Destination.'
            email: '- The email configuration of the Notification Destination. It must contain the following:'
            generic_webhook: '- The Generic Webhook configuration of the Notification Destination. It must contain the following:'
            id: '- The unique ID of the Notification Destination.'
            integration_key: '- (Required) The PagerDuty integration key.'
            microsoft_teams: '- The Microsoft Teams configuration of the Notification Destination. It must contain the following:'
            oauth_token: '- (Optional) OAuth token for Slack authentication.'
            pagerduty: '- The PagerDuty configuration of the Notification Destination. It must contain the following:'
            password: '- (Optional) The password for basic authentication.'
            slack: '- The Slack configuration of the Notification Destination. It must contain the following:'
            url: '- (Required) The Slack webhook URL.'
            username: '- (Optional) The username for basic authentication.'
        importStatements: []
    databricks_obo_token:
        subCategory: ""
        name: databricks_obo_token
        title: databricks_obo_token Resource
        examples:
            - name: this
              manifest: |-
                {
                  "application_id": "${databricks_service_principal.this.application_id}",
                  "comment": "PAT on behalf of ${databricks_service_principal.this.display_name}",
                  "depends_on": [
                    "${databricks_permissions.token_usage}"
                  ],
                  "lifetime_seconds": 3600
                }
              references:
                application_id: databricks_service_principal.this.application_id
              dependencies:
                databricks_permissions.token_usage: |-
                    {
                      "access_control": [
                        {
                          "permission_level": "CAN_USE",
                          "service_principal_name": "${databricks_service_principal.this.application_id}"
                        }
                      ],
                      "authorization": "tokens"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "Automation-only SP"
                    }
            - name: this
              manifest: |-
                {
                  "application_id": "${databricks_service_principal.this.application_id}",
                  "comment": "PAT on behalf of ${databricks_service_principal.this.display_name}",
                  "depends_on": [
                    "${databricks_group_member.this}"
                  ],
                  "lifetime_seconds": 3600
                }
              references:
                application_id: databricks_service_principal.this.application_id
              dependencies:
                databricks_group_member.this: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_service_principal.this.id}"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "Terraform"
                    }
        argumentDocs:
            application_id: '- Application ID of databricks_service_principal to create a PAT token for.'
            comment: '- (String, Optional) Comment that describes the purpose of the token.'
            id: '- Canonical unique identifier for the token.'
            lifetime_seconds: '- (Integer, Optional) The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.'
            token_value: '- Sensitive value of the newly-created token.'
        importStatements: []
    databricks_online_store Resource:
        subCategory: ""
        name: databricks_online_store Resource
        title: databricks_online_store Resource
        argumentDocs:
            capacity: (string, required) - The capacity of the online store. Valid values are "CU_1", "CU_2", "CU_4", "CU_8"
            creation_time: (string) - The timestamp when the online store was created
            creator: (string) - The email of the creator of the online store
            name: (string, required) - The name of the online store. This is the unique identifier for the online store
            read_replica_count: (integer, optional) - The number of read replicas for the online store. Defaults to 0
            state: '(string) - The current state of the online store. Possible values are: AVAILABLE, DELETING, FAILING_OVER, STARTING, STOPPED, UPDATING'
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_online_table:
        subCategory: ""
        name: databricks_online_table
        title: databricks_online_table Resource
        examples:
            - name: this
              manifest: |-
                {
                  "name": "main.default.online_table",
                  "spec": [
                    {
                      "primary_key_columns": [
                        "id"
                      ],
                      "run_triggered": [
                        {}
                      ],
                      "source_table_full_name": "main.default.source_table"
                    }
                  ]
                }
        argumentDocs:
            detailed_state: '- The state of the online table.'
            id: '- The same as the name of the online table.'
            message: '- A text description of the current state of the online table.'
            name: '- (Required) 3-level name of the Online Table to create.'
            perform_full_copy: '- (Optional) Whether to create a full-copy pipeline -- a pipeline that stops after creates a full copy of the source table upon initialization and does not process any change data feeds (CDFs) afterwards. The pipeline can still be manually triggered afterwards, but it always perform a full copy of the source table and there are no incremental updates. This mode is useful for syncing views or tables without CDFs to online tables. Note that the full-copy pipeline only supports "triggered" scheduling policy.'
            pipeline_id: '- ID of the associated Delta Live Table pipeline.'
            primary_key_columns: '- (Required) list of the columns comprising the primary key.'
            run_continuously: '- empty block that specifies that pipeline runs continuously after generating the initial data.  Conflicts with run_triggered.'
            run_triggered: '- empty block that specifies that pipeline stops after generating the initial data and can be triggered later (manually, through a cron job or through data triggers).'
            source_table_full_name: '- (Required) full name of the source table.'
            spec: '- (Required) object containing specification of the online table:'
            status: '- object describing status of the online table:'
            table_serving_url: '- Data serving REST API URL for this table.'
            timeseries_key: '- (Optional) Time series key to deduplicate (tie-break) rows with the same primary key.'
            unity_catalog_provisioning_state: '- The provisioning state of the online table entity in Unity Catalog. This is distinct from the state of the data synchronization pipeline (i.e. the table may be in "ACTIVE" but the pipeline may be in "PROVISIONING" as it runs asynchronously).'
        importStatements: []
    databricks_permission_assignment:
        subCategory: ""
        name: databricks_permission_assignment
        title: databricks_permission_assignment Resource
        examples:
            - name: add_user
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${data.databricks_user.me.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_user.me.id
                provider: databricks.workspace
            - name: add_admin_spn
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${data.databricks_service_principal.sp.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_service_principal.sp.id
                provider: databricks.workspace
            - name: this
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${data.databricks_group.account_level.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_group.account_level.id
                provider: databricks.workspace
        argumentDocs:
            '"ADMIN"': '- Adds principal to the workspace admins group. This gives workspace admin privileges to manage users and groups, workspace configurations, and more.'
            '"USER"': '- Adds principal to the workspace users group. This gives basic workspace access.'
            id: '- ID of the permission assignment - same as principal_id.'
            permissions: '- The list of workspace permissions to assign to the principal:'
            principal_id: '- Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the account-level SCIM API, or using databricks_user, databricks_service_principal or databricks_group data sources with account API (and has to be an account admin). A more sensible approach is to retrieve the list of principal_id as outputs from another Terraform stack.'
        importStatements: []
    databricks_permissions:
        subCategory: ""
        name: databricks_permissions
        title: databricks_permissions Resource
        examples:
            - name: cluster_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_ATTACH_TO"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_RESTART"
                    },
                    {
                      "group_name": "${databricks_group.ds.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "cluster_id": "${databricks_cluster.shared_autoscaling.id}"
                }
              references:
                access_control.group_name: databricks_group.ds.display_name
                cluster_id: databricks_cluster.shared_autoscaling.id
              dependencies:
                databricks_cluster.shared_autoscaling: |-
                    {
                      "autoscale": [
                        {
                          "max_workers": 10,
                          "min_workers": 1
                        }
                      ],
                      "autotermination_minutes": 60,
                      "cluster_name": "Shared Autoscaling",
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: policy_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.ds.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "cluster_policy_id": "${databricks_cluster_policy.something_simple.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                cluster_policy_id: databricks_cluster_policy.something_simple.id
              dependencies:
                databricks_cluster_policy.something_simple: |-
                    {
                      "definition": "${jsonencode({\n    \"spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL\" : {\n      \"type\" : \"forbidden\"\n    },\n    \"spark_conf.spark.secondkey\" : {\n      \"type\" : \"forbidden\"\n    }\n  })}",
                      "name": "Some simple policy"
                    }
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: pool_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_ATTACH_TO"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "instance_pool_id": "${databricks_instance_pool.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                instance_pool_id: databricks_instance_pool.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_instance_pool.this: |-
                    {
                      "idle_instance_autotermination_minutes": 60,
                      "instance_pool_name": "Reserved Instances",
                      "max_capacity": 10,
                      "min_idle_instances": 0,
                      "node_type_id": "${data.databricks_node_type.smallest.id}"
                    }
            - name: job_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "permission_level": "IS_OWNER",
                      "service_principal_name": "${databricks_service_principal.aws_principal.application_id}"
                    }
                  ],
                  "job_id": "${databricks_job.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                access_control.service_principal_name: databricks_service_principal.aws_principal.application_id
                job_id: databricks_job.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_job.this: |-
                    {
                      "max_concurrent_runs": 1,
                      "name": "Featurization",
                      "task": [
                        {
                          "new_cluster": [
                            {
                              "node_type_id": "${data.databricks_node_type.smallest.id}",
                              "num_workers": 300,
                              "spark_version": "${data.databricks_spark_version.latest.id}"
                            }
                          ],
                          "notebook_task": [
                            {
                              "notebook_path": "/Production/MakeFeatures"
                            }
                          ],
                          "task_key": "task1"
                        }
                      ]
                    }
                databricks_service_principal.aws_principal: |-
                    {
                      "display_name": "main"
                    }
            - name: ldp_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "pipeline_id": "${databricks_pipeline.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                pipeline_id: databricks_pipeline.this.id
              dependencies:
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_notebook.ldp_demo: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    import dlt\n    json_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n    @dlt.table(\n       comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n    )\n    def clickstream_raw():\n        return (spark.read.format(\"json\").load(json_path))\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/ldp_demo"
                    }
                databricks_pipeline.this: |-
                    {
                      "configuration": {
                        "key1": "value1",
                        "key2": "value2"
                      },
                      "continuous": false,
                      "filters": [
                        {
                          "exclude": [
                            "com.databricks.exclude"
                          ],
                          "include": [
                            "com.databricks.include"
                          ]
                        }
                      ],
                      "library": [
                        {
                          "notebook": [
                            {
                              "path": "${databricks_notebook.ldp_demo.id}"
                            }
                          ]
                        }
                      ],
                      "name": "LDP Demo Pipeline (${data.databricks_current_user.me.alphanumeric})",
                      "storage": "/test/tf-pipeline"
                    }
            - name: notebook_usage_by_path
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "notebook_path": "${databricks_notebook.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                notebook_path: databricks_notebook.this.path
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\"# Welcome to your Python notebook\")}",
                      "language": "PYTHON",
                      "path": "/Production/ETL/Features"
                    }
            - name: notebook_usage_by_id
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "notebook_id": "${databricks_notebook.this.object_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                notebook_id: databricks_notebook.this.object_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\"# Welcome to your Python notebook\")}",
                      "language": "PYTHON",
                      "path": "/Production/ETL/Features"
                    }
            - name: workspace_file_usage_by_path
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "workspace_file_path": "${databricks_workspace_file.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                workspace_file_path: databricks_workspace_file.this.path
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_workspace_file.this: |-
                    {
                      "content_base64": "${base64encode(\"print('Hello World')\")}",
                      "path": "/Production/ETL/Features.py"
                    }
            - name: workspace_file_usage_by_id
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "workspace_file_id": "${databricks_workspace_file.this.object_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                workspace_file_id: databricks_workspace_file.this.object_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_workspace_file.this: |-
                    {
                      "content_base64": "${base64encode(\"print('Hello World')\")}",
                      "path": "/Production/ETL/Features.py"
                    }
            - name: folder_usage_by_path
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "directory_path": "${databricks_directory.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                directory_path: databricks_directory.this.path
              dependencies:
                databricks_directory.this: |-
                    {
                      "path": "/Production/ETL"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: folder_usage_by_id
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "directory_id": "${databricks_directory.this.object_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                directory_id: databricks_directory.this.object_id
              dependencies:
                databricks_directory.this: |-
                    {
                      "path": "/Production/ETL"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: repo_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "repo_id": "${databricks_repo.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                repo_id: databricks_repo.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_repo.this: |-
                    {
                      "url": "https://github.com/user/demo.git"
                    }
            - name: experiment_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "experiment_id": "${databricks_mlflow_experiment.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                experiment_id: databricks_mlflow_experiment.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_mlflow_experiment.this: |-
                    {
                      "artifact_location": "s3://bucket/my-experiment",
                      "description": "My MLflow experiment description",
                      "name": "${data.databricks_current_user.me.home}/Sample"
                    }
            - name: model_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE_PRODUCTION_VERSIONS"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE_STAGING_VERSIONS"
                    }
                  ],
                  "registered_model_id": "${databricks_mlflow_model.this.registered_model_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                registered_model_id: databricks_mlflow_model.this.registered_model_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_mlflow_model.this: |-
                    {
                      "name": "SomePredictions"
                    }
            - name: ml_serving_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_QUERY"
                    }
                  ],
                  "serving_endpoint_id": "${databricks_model_serving.this.serving_endpoint_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                serving_endpoint_id: databricks_model_serving.this.serving_endpoint_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_model_serving.this: |-
                    {
                      "config": [
                        {
                          "served_models": [
                            {
                              "model_name": "test",
                              "model_version": "1",
                              "name": "prod_model",
                              "scale_to_zero_enabled": true,
                              "workload_size": "Small"
                            }
                          ]
                        }
                      ],
                      "name": "tf-test"
                    }
            - name: vector_search_endpoint_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "vector_search_endpoint_id": "${databricks_vector_search_endpoint.this.endpoint_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                vector_search_endpoint_id: databricks_vector_search_endpoint.this.endpoint_id
              dependencies:
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_vector_search_endpoint.this: |-
                    {
                      "endpoint_type": "STANDARD",
                      "name": "vector-search-test"
                    }
            - name: password_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.guests.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "authorization": "passwords"
                }
              references:
                access_control.group_name: databricks_group.guests.display_name
              dependencies:
                databricks_group.guests: |-
                    {
                      "display_name": "Guest Users"
                    }
            - name: token_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "authorization": "tokens"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: endpoint_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_endpoint_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                sql_endpoint_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "Small",
                      "max_num_clusters": 1,
                      "name": "Endpoint of ${data.databricks_current_user.me.alphanumeric}",
                      "tags": [
                        {
                          "custom_tags": [
                            {
                              "key": "City",
                              "value": "Amsterdam"
                            }
                          ]
                        }
                      ]
                    }
            - name: dashboard_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "dashboard_id": "${databricks_dashboard.dashboard.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                dashboard_id: databricks_dashboard.dashboard.id
              dependencies:
                databricks_dashboard.dashboard: |-
                    {
                      "display_name": "TF New Dashboard"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: sql_dashboard_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_dashboard_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: query_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_query_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: app_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "alert_v2_id": "12345"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: alert_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_alert_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: app_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "app_name": "myapp"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: app_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "database_instance_name": "my_database"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: model_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    }
                  ],
                  "registered_model_id": "${databricks_mlflow_model.model.registered_model_id}"
                }
              references:
                registered_model_id: databricks_mlflow_model.model.registered_model_id
              dependencies:
                databricks_mlflow_model.model: |-
                    {
                      "description": "MLflow registered model",
                      "name": "example_model"
                    }
        argumentDocs:
            CAN_MANAGE: permission for items in the Workspace > Shared Icon Shared folder. You can grant CAN_MANAGE permission to notebooks and folders by moving them to the Shared Icon Shared folder.
            IS_OWNER: permission. Destroying databricks_permissions resource for a job would revert ownership to the creator.
            admins.group_name: '- (Optional) name of the group. We recommend setting permissions on groups.'
            admins.permission_level: '- (Required) permission level according to specific resource. See examples above for the reference.'
            admins.service_principal_name: '- (Optional) Application ID (not service principal name!) of the service_principal.'
            admins.user_name: '- (Optional) name of the user.'
            app_name: '- app name'
            authorization: '- either tokens or passwords.'
            cluster_id: '- cluster id'
            cluster_policy_id: '- cluster policy id'
            directory_id: '- directory id'
            directory_path: '- path of directory'
            experiment_id: '- MLflow experiment id'
            id: '- Canonical unique identifier for the permissions in form of /<object type>/<object id>.'
            instance_pool_id: '- instance pool id'
            job_id: '- job id'
            notebook_id: '- ID of notebook within workspace'
            notebook_path: '- path of notebook'
            object_type: '- type of permissions.'
            pipeline_id: '- pipeline id'
            registered_model_id: '- MLflow registered model id'
            repo_id: '- repo id'
            repo_path: '- path of databricks repo directory(/Repos/<username>/...)'
            serving_endpoint_id: '- Model Serving endpoint id.'
            sql_alert_id: '- SQL alert id'
            sql_dashboard_id: '- SQL dashboard id'
            sql_endpoint_id: '- SQL warehouse id'
            sql_query_id: '- SQL query id'
            vector_search_endpoint_id: '- Vector Search endpoint id.'
        importStatements: []
    databricks_pipeline:
        subCategory: ""
        name: databricks_pipeline
        title: databricks_pipeline Resource
        examples:
            - name: this
              manifest: |-
                {
                  "catalog": "main",
                  "cluster": [
                    {
                      "custom_tags": {
                        "cluster_type": "default"
                      },
                      "label": "default",
                      "num_workers": 2
                    },
                    {
                      "custom_tags": {
                        "cluster_type": "maintenance"
                      },
                      "label": "maintenance",
                      "num_workers": 1
                    }
                  ],
                  "configuration": {
                    "key1": "value1",
                    "key2": "value2"
                  },
                  "continuous": false,
                  "library": [
                    {
                      "notebook": [
                        {
                          "path": "${databricks_notebook.ldp_demo.id}"
                        }
                      ]
                    },
                    {
                      "file": [
                        {
                          "path": "${databricks_repo.ldp_demo.path}/pipeline.sql"
                        }
                      ]
                    },
                    {
                      "glob": [
                        {
                          "include": "${databricks_repo.ldp_demo.path}/subfolder/**"
                        }
                      ]
                    }
                  ],
                  "name": "Pipeline Name",
                  "notification": [
                    {
                      "alerts": [
                        "on-update-failure",
                        "on-update-fatal-failure",
                        "on-update-success",
                        "on-flow-failure"
                      ],
                      "email_recipients": [
                        "user@domain.com",
                        "user1@domain.com"
                      ]
                    }
                  ],
                  "schema": "ldp_demo"
                }
              references:
                library.notebook.path: databricks_notebook.ldp_demo.id
              dependencies:
                databricks_notebook.ldp_demo: '{}'
                databricks_repo.ldp_demo: '{}'
            - name: this
              manifest: |-
                {
                  "catalog": "main",
                  "environment": [
                    {
                      "dependencies": [
                        "foo==0.0.1",
                        "-r /Workspace/Users/user.name/my-pipeline/requirements.txt",
                        "/Volumes/main/default/libs/my_lib.whl"
                      ]
                    }
                  ],
                  "name": "Serverless demo",
                  "schema": "ldp_demo",
                  "serverless": true
                }
        argumentDocs:
            alerts: (Required) non-empty list of alert types. Right now following alert types are supported, consult documentation for actual list
            allow_duplicate_names: '- Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is false.'
            budget_policy_id: '- optional string specifying ID of the budget policy for this Lakeflow Declarative Pipeline.'
            catalog: '- The name of catalog in Unity Catalog. Change of this parameter forces recreation of the pipeline. (Conflicts with storage).'
            channel: '- optional name of the release channel for Spark version used by Lakeflow Declarative Pipeline.  Supported values are: CURRENT (default) and PREVIEW.'
            cluster: blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. Please note that Lakeflow Declarative Pipeline clusters are supporting only subset of attributes as described in   Also, note that autoscale block is extended with the mode parameter that controls the autoscaling algorithm (possible values are ENHANCED for new, enhanced autoscaling algorithm, or LEGACY for old algorithm).
            configuration: '- An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.'
            connection_id: '- Immutable. The Unity Catalog connection this gateway pipeline uses to communicate with the source.'
            continuous: '- A flag indicating whether to run the pipeline continuously. The default value is false.'
            dependencies: '- (Required) a list of pip dependencies, as supported by the version of pip in this environment. Each dependency is a pip requirement file line.  See API docs for more information.'
            deployment: '- Deployment type of this pipeline. Supports following attributes:'
            development: '- A flag indicating whether to run the pipeline in development mode. The default value is false.'
            edition: '- optional name of the product edition. Supported values are: CORE, PRO, ADVANCED (default).  Not required when serverless is set to true.'
            email_recipients: (Required) non-empty list of emails to notify.
            event_log: '- an optional block specifying a table where LDP Event Log will be stored.  Consists of the following fields:'
            exclude: '- Paths to exclude.'
            file: '- specifies path to a file in Databricks Workspace to include as source. Actual path is specified as path attribute inside the block.'
            filters: '- Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:'
            gateway_definition: '- The definition of a gateway pipeline to support CDC. Consists of following attributes:'
            gateway_storage_catalog: '- Required, Immutable. The name of the catalog for the gateway pipeline''s storage location.'
            gateway_storage_name: '- Required. The Unity Catalog-compatible naming for the gateway storage location. This is the destination to use for the data that is extracted by the gateway. Lakeflow Declarative Pipelines system will automatically create the storage location under the catalog and schema.'
            gateway_storage_schema: '- Required, Immutable. The name of the schema for the gateway pipelines''s storage location.'
            glob: '- The unified field to include source code. Each entry should have the include attribute that can specify a notebook path, a file path, or a folder path that ends /** (to include everything from that folder). This field cannot be used together with notebook or file.'
            id: '- Canonical unique identifier of the Lakeflow Declarative Pipeline.'
            include: '- Paths to include.'
            kind: '- The deployment method that manages the pipeline.'
            library: blocks - Specifies pipeline code.
            library.connection_name: '- Immutable. The Unity Catalog connection this ingestion pipeline uses to communicate with the source. Specify either ingestion_gateway_id or connection_name.'
            library.ingestion_gateway_id: '- Immutable. Identifier for the ingestion gateway used by this ingestion pipeline to communicate with the source. Specify either ingestion_gateway_id or connection_name.'
            library.objects: '- Required. Settings specifying tables to replicate and the destination for the replicated tables.'
            library.table_configuration: '- Configuration settings to control the ingestion of tables. These settings are applied to all tables in the pipeline.'
            metadata_file_path: '- The path to the file containing metadata about the deployment.'
            name: '- A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.'
            notebook: '- specifies path to a Databricks Notebook to include as source. Actual path is specified as path attribute inside the block.'
            on-flow-failure: '- a single data flow fails.'
            on-update-failure: '- a pipeline update fails with a retryable error.'
            on-update-fatal-failure: '- a pipeline update fails with a non-retryable (fatal) error.'
            on-update-success: '- a pipeline update completes successfully.'
            photon: '- A flag indicating whether to use Photon engine. The default value is false.'
            root_path: '- An optional string specifying the root path for this pipeline. This is used as the root directory when editing the pipeline in the Databricks user interface and it is added to sys.path when executing Python sources during pipeline execution.'
            schema: '- (Optional, String, Conflicts with target) The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.'
            serverless: '- An optional flag indicating if serverless compute should be used for this Lakeflow Declarative Pipeline.  Requires catalog to be set, as it could be used only with Unity Catalog.'
            storage: '- A location on cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. Change of this parameter forces recreation of the pipeline. (Conflicts with catalog).'
            tags: '- (Optional, map of strings) A map of tags associated with the pipeline. These are forwarded to the cluster as cluster tags, and are therefore subject to the same limitations. A maximum of 25 tags can be added to the pipeline.'
            target: '- (Optional, String, Conflicts with schema) The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.'
            url: '- URL of the Lakeflow Declarative Pipeline on the given workspace.'
        importStatements: []
    databricks_policy_info Resource:
        subCategory: ""
        name: databricks_policy_info Resource
        title: databricks_policy_info Resource
        argumentDocs:
            alias: (string, optional) - The alias of a matched column
            column_mask: |-
                (ColumnMaskOptions, optional) - Options for column mask policies. Valid only if policy_type is POLICY_TYPE_COLUMN_MASK.
                Required on create and optional on update. When specified on update,
                the new options will replace the existing options as a whole
            comment: (string, optional) - Optional description of the policy
            condition: (string, optional) - The condition expression used to match a table column
            constant: (string, optional) - A constant literal
            created_at: (integer) - Time at which the policy was created, in epoch milliseconds. Output only
            created_by: (string) - Username of the user who created the policy. Output only
            except_principals: (list of string, optional) - Optional list of user or group names that should be excluded from the policy
            for_securable_type: |-
                (string, required) - Type of securables that the policy should take effect on.
                Only TABLE is supported at this moment.
                Required on create and optional on update. Possible values are: CATALOG, CLEAN_ROOM, CONNECTION, CREDENTIAL, EXTERNAL_LOCATION, EXTERNAL_METADATA, FUNCTION, METASTORE, PIPELINE, PROVIDER, RECIPIENT, SCHEMA, SHARE, STAGING_TABLE, STORAGE_CREDENTIAL, TABLE, VOLUME
            function_name: |-
                (string, required) - The fully qualified name of the column mask function.
                The function is called on each row of the target table.
                The function's first argument and its return type should match the type of the masked column.
                Required on create and update
            id: (string) - Unique identifier of the policy. This field is output only and is generated by the system
            match_columns: |-
                (list of MatchColumn, optional) - Optional list of condition expressions used to match table columns.
                Only valid when for_securable_type is TABLE.
                When specified, the policy only applies to tables whose columns satisfy all match conditions
            name: |-
                (string, optional) - Name of the policy. Required on create and optional on update.
                To rename the policy, set name to a different value on update
            on_column: |-
                (string, required) - The alias of the column to be masked. The alias must refer to one of matched columns.
                The values of the column is passed to the column mask function as the first argument.
                Required on create and update
            on_securable_fullname: |-
                (string, optional) - Full name of the securable on which the policy is defined.
                Required on create and ignored on update
            on_securable_type: |-
                (string, optional) - Type of the securable on which the policy is defined.
                Only CATALOG, SCHEMA and TABLE are supported at this moment.
                Required on create and ignored on update. Possible values are: CATALOG, CLEAN_ROOM, CONNECTION, CREDENTIAL, EXTERNAL_LOCATION, EXTERNAL_METADATA, FUNCTION, METASTORE, PIPELINE, PROVIDER, RECIPIENT, SCHEMA, SHARE, STAGING_TABLE, STORAGE_CREDENTIAL, TABLE, VOLUME
            policy_type: '(string, required) - Type of the policy. Required on create and ignored on update. Possible values are: POLICY_TYPE_COLUMN_MASK, POLICY_TYPE_ROW_FILTER'
            row_filter: |-
                (RowFilterOptions, optional) - Options for row filter policies. Valid only if policy_type is POLICY_TYPE_ROW_FILTER.
                Required on create and optional on update. When specified on update,
                the new options will replace the existing options as a whole
            to_principals: |-
                (list of string, required) - List of user or group names that the policy applies to.
                Required on create and optional on update
            updated_at: (integer) - Time at which the policy was last modified, in epoch milliseconds. Output only
            updated_by: (string) - Username of the user who last modified the policy. Output only
            using: |-
                (list of FunctionArgument, optional) - Optional list of column aliases or constant literals to be passed as additional arguments to the column mask function.
                The type of each column should match the positional argument of the column mask function
            when_condition: (string, optional) - Optional condition when the policy should take effect
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_provider:
        subCategory: ""
        name: databricks_provider
        title: databricks_provider Resource
        examples:
            - name: dbprovider
              manifest: |-
                {
                  "authentication_type": "TOKEN",
                  "comment": "made by terraform 2",
                  "name": "terraform-test-provider",
                  "recipient_profile_str": "${jsonencode(\n    {\n      \"shareCredentialsVersion\" : 1,\n      \"bearerToken\" : \"token\",\n      \"endpoint\" : \"endpoint\",\n      \"expirationTime\" : \"expiration-time\"\n    }\n  )}"
                }
        argumentDocs:
            authentication_type: '- (Optional) The delta sharing authentication type. Valid values are TOKEN.'
            comment: '- (Optional) Description about the provider.'
            id: '- ID of this provider - same as the name.'
            name: '- Name of provider. Change forces creation of a new resource.'
            recipient_profile_str: '- (Optional) This is the json file that is created from a recipient url.'
        importStatements: []
    databricks_quality_monitor:
        subCategory: ""
        name: databricks_quality_monitor
        title: databricks_quality_monitor Resource
        examples:
            - name: testTimeseriesMonitor
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_quality_monitoring/${databricks_sql_table.myTestTable.name}",
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_sql_table.myTestTable.name}",
                  "time_series": [
                    {
                      "granularities": [
                        "1 hour"
                      ],
                      "timestamp_col": "timestamp"
                    }
                  ]
                }
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_sql_table.myTestTable: |-
                    {
                      "catalog_name": "main",
                      "column": [
                        {
                          "name": "timestamp",
                          "type": "int"
                        }
                      ],
                      "data_source_format": "DELTA",
                      "name": "bar",
                      "schema_name": "${databricks_schema.things.name}",
                      "table_type": "MANAGED"
                    }
            - name: testMonitorInference
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_quality_monitoring/${databricks_table.myTestTable.name}",
                  "inference_log": [
                    {
                      "granularities": [
                        "1 hour"
                      ],
                      "model_id_col": "model_id",
                      "prediction_col": "prediction",
                      "problem_type": "PROBLEM_TYPE_REGRESSION",
                      "timestamp_col": "timestamp"
                    }
                  ],
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_table.myTestTable.name}"
                }
            - name: testMonitorInference
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_quality_monitoring/${databricks_table.myTestTable.name}",
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "snapshot": [
                    {}
                  ],
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_table.myTestTable.name}"
                }
        argumentDocs:
            assets_dir: '- (Required) - The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)'
            baseline_table_name: |-
                - Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
                table.
            custom_metrics: '- Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).'
            dashboard_id: '- The ID of the generated dashboard.'
            data_classification_config: '- The data classification config for the monitor'
            definition: '- create metric definition'
            drift_metrics_table_name: '- The full name of the drift metrics table. Format: catalog_name.schema_name.table_name.'
            granularities: '-  List of granularities to use when aggregating data into time windows based on their timestamp.'
            id: '-  ID of this monitor is the same as the full table name of the format {catalog}.{schema_name}.{table_name}'
            inference_log: '- Configuration for the inference log monitor'
            input_columns: '- Columns on the monitored table to apply the custom metrics to.'
            label_col: '- Column of the model label'
            model_id_col: '- Column of the model id or version'
            monitor_version: '- The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted'
            name: '- Name of the custom metric.'
            notifications: '- The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name email_addresses containing a list of emails to notify:'
            on_failure: '- who to send notifications to on monitor failure.'
            on_new_classification_tag_detected: '- Who to send notifications to when new data classification tags are detected.'
            output_data_type: '- The output type of the custom metric.'
            output_schema_name: '- (Required) - Schema where output metric tables are created'
            prediction_col: '- Column of the model prediction'
            prediction_proba_col: '- Column of the model prediction probabilities'
            problem_type: '- Problem type the model aims to solve. Either PROBLEM_TYPE_CLASSIFICATION or PROBLEM_TYPE_REGRESSION'
            profile_metrics_table_name: '- The full name of the profile metrics table. Format: catalog_name.schema_name.table_name.'
            quartz_cron_expression: '- string expression that determines when to run the monitor. See Quartz documentation for examples.'
            schedule: '- The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:'
            skip_builtin_dashboard: '- Whether to skip creating a default dashboard summarizing data quality metrics.  (Can''t be updated after creation).'
            slicing_exprs: '- List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.'
            snapshot: '- Configuration for monitoring snapshot tables.'
            status: '- Status of the Monitor'
            table_name: '- (Required) - The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}'
            time_series: '- Configuration for monitoring timeseries tables.'
            timestamp_col: '- Column of the timestamp of predictions'
            timezone_id: '- string with timezone id (e.g., PST) in which to evaluate the Quartz expression.'
            type: '- The type of the custom metric.'
            warehouse_id: '- Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.  (Can''t be updated after creation)'
        importStatements: []
    databricks_quality_monitor_v2:
        subCategory: ""
        name: databricks_quality_monitor_v2
        title: databricks_quality_monitor_v2 Resource
        examples:
            - name: this
              manifest: |-
                {
                  "object_id": "${databricks_schema.this.schema_id}",
                  "object_type": "schema"
                }
              references:
                object_id: databricks_schema.this.schema_id
              dependencies:
                databricks_schema.this: |-
                    {
                      "catalog_name": "my_catalog",
                      "name": "my_schema"
                    }
        argumentDocs:
            anomaly_detection_config: (AnomalyDetectionConfig)
            last_run_id: (string) - Run id of the last run of the workflow
            latest_run_status: '(string) - The status of the last run of the workflow. Possible values are: ANOMALY_DETECTION_RUN_STATUS_CANCELED, ANOMALY_DETECTION_RUN_STATUS_FAILED, ANOMALY_DETECTION_RUN_STATUS_JOB_DELETED, ANOMALY_DETECTION_RUN_STATUS_PENDING, ANOMALY_DETECTION_RUN_STATUS_RUNNING, ANOMALY_DETECTION_RUN_STATUS_SUCCESS, ANOMALY_DETECTION_RUN_STATUS_UNKNOWN, ANOMALY_DETECTION_RUN_STATUS_WORKSPACE_MISMATCH_ERROR'
            object_id: (string, required) - The uuid of the request object. For example, schema id
            object_type: '(string, required) - The type of the monitored object. Can be one of the following: schema'
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_query:
        subCategory: ""
        name: databricks_query
        title: databricks_query Resource
        examples:
            - name: this
              manifest: |-
                {
                  "display_name": "My Query Name",
                  "parent_path": "${databricks_directory.shared_dir.path}",
                  "query_text": "SELECT 42 as value",
                  "warehouse_id": "${databricks_sql_endpoint.example.id}"
                }
              references:
                parent_path: databricks_directory.shared_dir.path
                warehouse_id: databricks_sql_endpoint.example.id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
            - name: query
              manifest: |-
                {
                  "display_name": "My Query",
                  "parameter": [
                    {
                      "name": "p1",
                      "text_value": [
                        {
                          "value": "default"
                        }
                      ],
                      "title": "Title for p1"
                    }
                  ],
                  "parent_path": "${databricks_directory.shared_dir.path}",
                  "query_text": "select 42 as value",
                  "warehouse_id": "${databricks_sql_endpoint.example.id}"
                }
              references:
                parent_path: databricks_directory.shared_dir.path
                warehouse_id: databricks_sql_endpoint.example.id
        argumentDocs:
            apply_auto_limit: '- (Optional, Boolean) Whether to apply a 1000 row limit to the query result.'
            catalog: '- (Optional, String) Name of the catalog where this query will be executed.'
            create_time: '- The timestamp string indicating when the query was created.'
            databricks_permissions: .
            databricks_sql_query: ', for example, by executing the terraform state show databricks_sql_query.query command.'
            date_range_value: '- (Block) Date-range query parameter value. Consists of following attributes (Can only specify one of dynamic_date_range_value or date_range_value):'
            date_value: '- (Block) Date query parameter value. Consists of following attributes (Can only specify one of dynamic_date_value or date_value):'
            description: '- (Optional, String) General description that conveys additional information about this query such as usage notes.'
            display_name: '- (Required, String) Name of the query.'
            dynamic_date_range_value: '- (String) Dynamic date-time range value based on current date-time.  Possible values are TODAY, YESTERDAY, THIS_WEEK, THIS_MONTH, THIS_YEAR, LAST_WEEK, LAST_MONTH, LAST_YEAR, LAST_HOUR, LAST_8_HOURS, LAST_24_HOURS, LAST_7_DAYS, LAST_14_DAYS, LAST_30_DAYS, LAST_60_DAYS, LAST_90_DAYS, LAST_12_MONTHS.'
            dynamic_date_value: '- (String) Dynamic date-time value based on current date-time.  Possible values are NOW, YESTERDAY.'
            end: (Required, String) - end of the date range.
            enum_options: '- (String) List of valid query parameter values, newline delimited.'
            enum_value: '- (Block) Dropdown parameter value. Consists of following attributes:'
            id: '- unique ID of the created Query.'
            last_modifier_user_name: '- Username of the user who last saved changes to this query.'
            lifecycle_state: '- The workspace state of the query. Used for tracking trashed status. (Possible values are ACTIVE or TRASHED).'
            multi_values_options: '- (Optional, Block) If specified, allows multiple values to be selected for this parameter. Consists of following attributes:'
            name: '- (Required, String) Literal parameter marker that appears between double curly braces in the query text.'
            numeric_value: '-  (Block) Numeric parameter value. Consists of following attributes:'
            owner_user_name: '- (Optional, String) Query owner''s username.'
            parameter: '- (Optional, Block) Query parameter definition.  Consists of following attributes (one of *_value is required):'
            parent: (if exists) is renamed to parent_path attribute, and should be converted from folders/object_id to the actual path.
            parent_path: '- (Optional, String) The path to a workspace folder containing the query. The default is the user''s home folder.  If changed, the query will be recreated.'
            precision: '- (Optional, String) Date-time precision to format the value into when the query is run.  Possible values are DAY_PRECISION, MINUTE_PRECISION, SECOND_PRECISION.  Defaults to DAY_PRECISION (YYYY-MM-DD).'
            prefix: '- (Optional, String) Character that prefixes each selected parameter value.'
            query_backed_value: '- (Block) Query-based dropdown parameter value. Consists of following attributes:'
            query_id: '- (Required, String) ID of the query that provides the parameter values.'
            query_text: '- (Required, String) Text of SQL query.'
            run_as_mode: '- (Optional, String) Sets the "Run as" role for the object.  Should be one of OWNER, VIEWER.'
            schema: '- (Optional, String) Name of the schema where this query will be executed.'
            separator: '- (Optional, String) Character that separates each selected parameter value. Defaults to a comma.'
            start: (Required, String) - begin of the date range.
            start_day_of_week: '- (Optional, Int) Specify what day that starts the week.'
            suffix: '- (Optional, String) Character that suffixes each selected parameter value.'
            tags: '- (Optional, List of strings) Tags that will be added to the query.'
            terraform import databricks_query.query <query-id>: command.
            terraform import.databricks_permissions: .
            terraform import.import: 'and removed blocks like this:'
            terraform import.terraform apply: command to apply changes.
            terraform import.terraform plan: command to check possible changes, such as value type change, etc.
            terraform plan: command to check possible changes, such as value type change, etc.
            terraform state rm databricks_sql_query.query: command.
            text_value: '- (Block) Text parameter value. Consists of following attributes:'
            title: '- (Optional, String) Text displayed in the user-facing parameter widget in the UI.'
            update_time: '- The timestamp string indicating when the query was updated.'
            value: '- (Required, String) - actual text value.'
            values: '- (Array of strings) List of selected query parameter values.'
            warehouse_id: '- (Required, String) ID of a SQL warehouse which will be used to execute this query.'
        importStatements: []
    databricks_recipient:
        subCategory: ""
        name: databricks_recipient
        title: databricks_recipient Resource
        examples:
            - name: db2open
              manifest: |-
                {
                  "authentication_type": "TOKEN",
                  "comment": "made by terraform",
                  "ip_access_list": [
                    {
                      "allowed_ip_addresses": []
                    }
                  ],
                  "name": "${data.databricks_current_user.current.alphanumeric}-recipient",
                  "sharing_code": "${random_password.db2opensharecode.result}"
                }
              references:
                sharing_code: random_password.db2opensharecode.result
              dependencies:
                random_password.db2opensharecode: |-
                    {
                      "length": 16,
                      "special": true
                    }
            - name: db2db
              manifest: |-
                {
                  "authentication_type": "DATABRICKS",
                  "comment": "made by terraform",
                  "data_recipient_global_metastore_id": "${databricks_metastore.recipient_metastore.global_metastore_id}",
                  "name": "${data.databricks_current_user.current.alphanumeric}-recipient"
                }
              references:
                data_recipient_global_metastore_id: databricks_metastore.recipient_metastore.global_metastore_id
              dependencies:
                databricks_metastore.recipient_metastore: |-
                    {
                      "delta_sharing_recipient_token_lifetime_in_seconds": "60000000",
                      "delta_sharing_scope": "INTERNAL",
                      "force_destroy": true,
                      "name": "recipient",
                      "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                    }
        argumentDocs:
            activation_url: '- Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.'
            authentication_type: '- (Optional) The delta sharing authentication type. Valid values are TOKEN and DATABRICKS.'
            cloud: '- Cloud vendor of the recipient''s Unity Catalog Metstore. This field is only present when the authentication_type is DATABRICKS.'
            comment: '- (Optional) Description about the recipient.'
            created_at: '- Time at which this recipient Token was created, in epoch milliseconds.'
            created_by: '- Username of recipient token creator.'
            data_recipient_global_metastore_id: '- Required when authentication_type is DATABRICKS.'
            expiration_time: '- Expiration timestamp of the token in epoch milliseconds.'
            id: '- the ID of the recipient - the same as the name.'
            ip_access_list: '- (Optional) Recipient IP access list.'
            ip_access_list.allowed_ip_addresses: '- Allowed IP Addresses in CIDR notation. Limit of 100.'
            metastore_id: '- Unique identifier of recipient''s Unity Catalog metastore. This field is only present when the authentication_type is DATABRICKS.'
            name: '- Name of recipient. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the recipient owner.'
            properties: (Required) a map of string key-value pairs with recipient's properties.  Properties with name starting with databricks. are reserved.
            properties_kvpairs: '- (Optional) Recipient properties - object consisting of following fields:'
            region: '- Cloud region of the recipient''s Unity Catalog Metstore. This field is only present when the authentication_type is DATABRICKS.'
            sharing_code: '- (Optional) The one-time sharing code provided by the data recipient.'
            tokens: '- List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:'
            updated_at: '- Time at which this recipient Token was updated, in epoch milliseconds.'
            updated_by: '- Username of recipient Token updater.'
        importStatements: []
    databricks_recipient_federation_policy Resource:
        subCategory: ""
        name: databricks_recipient_federation_policy Resource
        title: databricks_recipient_federation_policy Resource
        argumentDocs:
            audiences: |-
                (list of string, optional) - The allowed token audiences, as specified in the 'aud' claim of federated tokens.
                The audience identifier is intended to represent the recipient of the token.
                Can be any non-empty string value. As long as the audience in the token matches at least one audience in the policy,
            azp: ', this must be the client ID of the OAuth app registered in Entra ID'
            comment: (string, optional) - Description of the policy. This is a user-provided description
            create_time: (string) - System-generated timestamp indicating when the policy was created
            groups: ', this must be the Object ID of the group in Entra ID.'
            id: (string) - Unique, immutable system-generated identifier for the federation policy
            issuer: (string, required) - The required token issuer, as specified in the 'iss' claim of federated tokens
            name: |-
                (string, optional) - Name of the federation policy. A recipient can have multiple policies with different names.
                The name must contain only lowercase alphanumeric characters, numbers, and hyphens
            oid: ', this must be the Object ID of the user in Entra ID.'
            oidc_policy: (OidcFederationPolicy, optional) - Specifies the policy to use for validating OIDC claims in the federated tokens
            sub: ': Subject identifier for other use cases'
            subject: |-
                (string, required) - The required token subject, as specified in the subject claim of federated tokens.
                The subject claim identifies the identity of the user or machine accessing the resource.
                Examples for Entra ID (AAD):
            subject_claim: |-
                (string, required) - The claim that contains the subject of the token.
                Depending on the identity provider and the use case (U2M or M2M), this can vary:
            update_time: (string) - System-generated timestamp indicating when the policy was last updated
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_registered_model:
        subCategory: ""
        name: databricks_registered_model
        title: databricks_registered_model Resource
        examples:
            - name: this
              manifest: |-
                {
                  "catalog_name": "main",
                  "name": "my_model",
                  "schema_name": "default"
                }
        argumentDocs:
            ALL_PRIVILEGES: ', APPLY_TAG, and EXECUTE privileges.'
            catalog_name: '- (Required) The name of the catalog where the schema and the registered model reside. Change of this parameter forces recreation of the resource.'
            comment: '- (Optional) The comment attached to the registered model.'
            id: '- Equal to the full name of the model (catalog_name.schema_name.name) and used to identify the model uniquely across the metastore.'
            name: '- (Required) The name of the registered model.  Change of this parameter forces recreation of the resource.'
            owner: '- (Optional) Name of the registered model owner.'
            schema_name: '- (Required) The name of the schema where the registered model resides. Change of this parameter forces recreation of the resource.'
            storage_location: '- (Optional) The storage location under which model version data files are stored. Change of this parameter forces recreation of the resource.'
        importStatements: []
    databricks_repo:
        subCategory: ""
        name: databricks_repo
        title: databricks_repo Resource
        examples:
            - name: nutter_in_home
              manifest: |-
                {
                  "url": "https://github.com/user/demo.git"
                }
        argumentDocs:
            branch: '- (Optional) name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with tag.  If branch is removed, and tag isn''t specified, then the repository will stay at the previously checked out state.'
            commit_hash: '- Hash of the HEAD commit at time of the last executed operation. It won''t change if you manually perform pull operation via UI or API'
            git_provider: '- (Optional, if it''s possible to detect Git provider by host name) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult Repos API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition, awsCodeCommit.'
            id: '-  Git folder identifier'
            path: '- (Optional) path to put the checked out Git folder. If not specified, , then the Git folder will be created in the default location.  If the value changes, Git folder is re-created.'
            sparse_checkout.patterns: '- array of paths (directories) that will be used for sparse checkout.  List of patterns could be updated in-place.'
            tag: '- (Optional) name of the tag for initial checkout.  Conflicts with branch.'
            url: '-  (Required) The URL of the Git Repository to clone from. If the value changes, Git folder is re-created.'
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_restrict_workspace_admins_setting:
        subCategory: ""
        name: databricks_restrict_workspace_admins_setting
        title: databricks_restrict_workspace_admins_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "restrict_workspace_admins": [
                    {
                      "status": "RESTRICT_TOKENS_AND_JOB_RUN_AS"
                    }
                  ]
                }
        argumentDocs:
            restrict_workspace_admins: '- (Required) The configuration details.'
            status: '- (Required) The restrict workspace admins status for the workspace.'
        importStatements: []
    databricks_schema:
        subCategory: ""
        name: databricks_schema
        title: databricks_schema Resource
        examples:
            - name: things
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.id}",
                  "comment": "this database is managed by terraform",
                  "name": "things",
                  "properties": {
                    "kind": "various"
                  }
                }
              references:
                catalog_name: databricks_catalog.sandbox.id
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
        argumentDocs:
            catalog_name: '- Name of parent catalog. Change forces creation of a new resource.'
            comment: '- (Optional) User-supplied free-form text.'
            enable_predictive_optimization: '- (Optional) Whether predictive optimization should be enabled for this object and objects under it. Can be ENABLE, DISABLE or INHERIT'
            force_destroy: '- (Optional) Delete schema regardless of its contents.'
            id: '- ID of this schema in form of <catalog_name>.<name>.'
            name: '- Name of Schema relative to parent catalog. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the schema owner.'
            properties: '- (Optional) Extensible Schema properties.'
            schema_id: '- The unique identifier of the schema.'
            storage_root: '- (Optional) Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.'
        importStatements: []
    databricks_secret:
        subCategory: ""
        name: databricks_secret
        title: databricks_secret Resource
        examples:
            - name: publishing_api
              manifest: |-
                {
                  "key": "publishing_api",
                  "scope": "${databricks_secret_scope.app.id}",
                  "string_value": "${data.azurerm_key_vault_secret.example.value}"
                }
              references:
                scope: databricks_secret_scope.app.id
                string_value: data.azurerm_key_vault_secret.example.value
              dependencies:
                databricks_cluster.this: |-
                    {
                      "spark_conf": {
                        "fs.azure.account.oauth2.client.secret": "${databricks_secret.publishing_api.config_reference}"
                      }
                    }
                databricks_secret_scope.app: |-
                    {
                      "name": "application-secret-scope"
                    }
        argumentDocs:
            config_reference: '- (String) value to use as a secret reference in Spark configuration and environment variables: {{secrets/scope/key}}.'
            id: '- Canonical unique identifier for the secret.'
            key: '- (Required) (String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.'
            last_updated_timestamp: '- (Integer) time secret was updated'
            scope: '- (Required) (String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.'
            string_value: '- (Required) (String) super secret sensitive value.'
        importStatements: []
    databricks_secret_acl:
        subCategory: ""
        name: databricks_secret_acl
        title: databricks_secret_acl Resource
        examples:
            - name: my_secret_acl
              manifest: |-
                {
                  "permission": "READ",
                  "principal": "${databricks_group.ds.display_name}",
                  "scope": "${databricks_secret_scope.app.name}"
                }
              references:
                principal: databricks_group.ds.display_name
                scope: databricks_secret_scope.app.name
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "data-scientists"
                    }
                databricks_secret.publishing_api: |-
                    {
                      "key": "publishing_api",
                      "scope": "${databricks_secret_scope.app.name}",
                      "string_value": "${data.azurerm_key_vault_secret.example.value}"
                    }
                databricks_secret_scope.app: |-
                    {
                      "name": "app-secret-scope"
                    }
        argumentDocs:
            application_id: attribute of databricks_service_principal.
            display_name: attribute of databricks_group.  Use users to allow access for all workspace users.
            permission: '- (Required) READ, WRITE or MANAGE.'
            principal: '- (Required) principal''s identifier. It can be:'
            scope: '- (Required) name of the scope'
            user_name: attribute of databricks_user.
        importStatements: []
    databricks_secret_scope:
        subCategory: ""
        name: databricks_secret_scope
        title: databricks_secret_scope Resource
        examples:
            - name: this
              manifest: |-
                {
                  "name": "terraform-demo-scope"
                }
            - name: kv
              manifest: |-
                {
                  "keyvault_metadata": [
                    {
                      "dns_name": "${azurerm_key_vault.this.vault_uri}",
                      "resource_id": "${azurerm_key_vault.this.id}"
                    }
                  ],
                  "name": "keyvault-managed"
                }
              references:
                keyvault_metadata.dns_name: azurerm_key_vault.this.vault_uri
                keyvault_metadata.resource_id: azurerm_key_vault.this.id
              dependencies:
                azurerm_key_vault.this: |-
                    {
                      "location": "${azurerm_resource_group.example.location}",
                      "name": "${var.prefix}-kv",
                      "purge_protection_enabled": false,
                      "resource_group_name": "${azurerm_resource_group.example.name}",
                      "sku_name": "standard",
                      "soft_delete_enabled": false,
                      "tags": "${var.tags}",
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
                azurerm_key_vault_access_policy.this: |-
                    {
                      "key_vault_id": "${azurerm_key_vault.this.id}",
                      "object_id": "${data.azurerm_client_config.current.object_id}",
                      "secret_permissions": [
                        "Delete",
                        "Get",
                        "List",
                        "Set"
                      ],
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
        argumentDocs:
            backend_type: '- Either DATABRICKS or AZURE_KEYVAULT'
            id: '- The id for the secret scope object.'
            initial_manage_principal: '- (Optional) The principal with the only possible value users that is initially granted MANAGE permission to the created scope.  If it''s omitted, then the databricks_secret_acl with MANAGE permission applied to the scope is assigned to the API request issuer''s user identity (see documentation). This part of the state cannot be imported.'
            name: '- (Required) Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.'
        importStatements: []
    databricks_service_principal:
        subCategory: ""
        name: databricks_service_principal
        title: databricks_service_principal Resource
        examples:
            - name: sp
              manifest: |-
                {
                  "display_name": "Admin SP"
                }
            - name: sp
              manifest: |-
                {
                  "display_name": "Admin SP"
                }
              dependencies:
                databricks_group_member.i-am-admin: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_service_principal.sp.id}"
                    }
            - name: sp
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "application_id": "00000000-0000-0000-0000-000000000000",
                  "display_name": "Example service principal"
                }
            - name: sp
              manifest: |-
                {
                  "display_name": "Automation-only SP",
                  "provider": "${databricks.account}"
                }
              references:
                provider: databricks.account
            - name: sp
              manifest: |-
                {
                  "application_id": "00000000-0000-0000-0000-000000000000",
                  "provider": "${databricks.account}"
                }
              references:
                provider: databricks.account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. servicePrincipals/00000000-0000-0000-0000-000000000000.'
            active: '- (Optional) Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.'
            allow_cluster_create: '- (Optional) Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.'
            allow_instance_pool_create: '- (Optional) Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            application_id: This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.
            databricks_sql_access: '- (Optional) This is a field to allow the service principal to have access to Databricks SQL feature through databricks_sql_endpoint.'
            disable_as_user_deletion: '- (Optional) Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to true when the provider is configured at the account-level and false when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.'
            display_name: '- (Required for Databricks-managed service principals) This is an alias for the service principal and can be the full name of the service principal.'
            external_id: '- (Optional) ID of the service principal in an external identity provider.'
            force: '- (Optional) Ignore cannot create service principal: Service principal with application ID X already exists errors and implicitly import the specified service principal into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            force_delete_home_dir: '- (Optional) This flag determines whether the service principal''s home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            force_delete_repos: '- (Optional) This flag determines whether the service principal''s repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            home: '- Home folder of the service principal, e.g. /Users/00000000-0000-0000-0000-000000000000.'
            id: '- Canonical unique identifier for the service principal (SCIM ID).'
            repos: '- Personal Repos location of the service principal, e.g. /Repos/00000000-0000-0000-0000-000000000000.'
            workspace_access: '- (Optional) This is a field to allow the service principal to have access to a Databricks Workspace.'
            workspace_consume: '- (Optional) This is a field to allow the service principal to have access to a Databricks Workspace as consumer, with limited access to workspace UI.  Couldn''t be used with workspace_access or databricks_sql_access.'
        importStatements: []
    databricks_service_principal_federation_policy:
        subCategory: ""
        name: databricks_service_principal_federation_policy
        title: databricks_service_principal_federation_policy Resource
        examples:
            - name: this
              manifest: |-
                {
                  "oidc_policy": {
                    "issuer": "https://myidp.example.com",
                    "subject": "subject-in-token-from-myidp",
                    "subject_claim": "sub"
                  },
                  "policy_id": "my-policy",
                  "service_principal_id": 1234
                }
        argumentDocs:
            audiences: |-
                (list of string, optional) - The allowed token audiences, as specified in the 'aud' claim of federated tokens.
                The audience identifier is intended to represent the recipient of the token.
                Can be any non-empty string value. As long as the audience in the token matches
                at least one audience in the policy, the token is considered a match. If audiences
                is unspecified, defaults to your Databricks account id
            create_time: (string) - Creation time of the federation policy
            description: (string, optional) - Description of the federation policy
            issuer: (string, optional) - The required token issuer, as specified in the 'iss' claim of federated tokens
            jwks_json: |-
                (string, optional) - The public keys used to validate the signature of federated tokens, in JWKS format.
                Most use cases should not need to specify this field. If jwks_uri and jwks_json
                are both unspecified (recommended), Databricks automatically fetches the public
                keys from your issuer’s well known endpoint. Databricks strongly recommends
                relying on your issuer’s well known endpoint for discovering public keys
            jwks_uri: |-
                (string, optional) - URL of the public keys used to validate the signature of federated tokens, in
                JWKS format. Most use cases should not need to specify this field. If jwks_uri
                and jwks_json are both unspecified (recommended), Databricks automatically
                fetches the public keys from your issuer’s well known endpoint. Databricks
                strongly recommends relying on your issuer’s well known endpoint for discovering
                public keys
            name: |-
                (string) - Resource name for the federation policy. Example values include
                accounts/<account-id>/federationPolicies/my-federation-policy for Account Federation Policies, and
                accounts/<account-id>/servicePrincipals/<service-principal-id>/federationPolicies/my-federation-policy
                for Service Principal Federation Policies. Typically an output parameter, which does not need to be
                specified in create or update requests. If specified in a request, must match the value in the
                request URL
            oidc_policy: (OidcFederationPolicy, optional)
            policy_id: (string) - The ID of the federation policy
            service_principal_id: (integer) - The service principal ID that this federation policy applies to. Only set for service principal federation policies
            subject: |-
                (string, optional) - The required token subject, as specified in the subject claim of federated tokens.
                Must be specified for service principal federation policies. Must not be specified
                for account federation policies
            subject_claim: |-
                (string, optional) - The claim that contains the subject of the token. If unspecified, the default value
                is 'sub'
            uid: (string) - Unique, immutable id of the federation policy
            update_time: (string) - Last update time of the federation policy
        importStatements: []
    databricks_service_principal_role:
        subCategory: ""
        name: databricks_service_principal_role
        title: databricks_service_principal_role Resource
        examples:
            - name: my_service_principal_instance_profile
              manifest: |-
                {
                  "role": "${databricks_instance_profile.instance_profile.id}",
                  "service_principal_id": "${databricks_service_principal.this.id}"
                }
              references:
                role: databricks_instance_profile.instance_profile.id
                service_principal_id: databricks_service_principal.this.id
              dependencies:
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "My Service Principal"
                    }
            - name: tf_admin_account
              manifest: |-
                {
                  "role": "account_admin",
                  "service_principal_id": "${databricks_service_principal.tf_admin.id}"
                }
              references:
                service_principal_id: databricks_service_principal.tf_admin.id
              dependencies:
                databricks_service_principal.tf_admin: |-
                    {
                      "display_name": "Terraform Admin"
                    }
        argumentDocs:
            id: '- The id in the format <service_principal_id>|<role>.'
            role: '-  (Required) This is the role name, role id, or instance profile resource.'
            service_principal_id: '- (Required) This is the id of the service principal resource.'
        importStatements: []
    databricks_service_principal_secret:
        subCategory: ""
        name: databricks_service_principal_secret
        title: databricks_service_principal_secret Resource
        examples:
            - name: terraform_sp
              manifest: |-
                {
                  "service_principal_id": "${databricks_service_principal.this.id}"
                }
              references:
                service_principal_id: databricks_service_principal.this.id
            - name: terraform_sp
              manifest: |-
                {
                  "service_principal_id": "${databricks_service_principal.this.id}",
                  "time_rotating": "Terraform (created: ${time_rotating.this.rfc3339})"
                }
              references:
                service_principal_id: databricks_service_principal.this.id
              dependencies:
                time_rotating.this: |-
                    {
                      "rotation_days": 30
                    }
        argumentDocs:
            create_time: '- UTC time when the secret was created.'
            expire_time: '- UTC time when the secret will expire. If the field is not present, the secret does not expire.'
            id: '- ID of the secret'
            lifetime: (Optional, string) - The lifetime of the secret in seconds formatted as NNNNs. If this parameter is not provided, the secret will have a default lifetime of 730 days (63072000s).  Expiration of secret will lead to generation of new secret.
            secret: '- Sensitive Generated secret for the service principal.'
            secret_hash: '- Secret Hash.'
            service_principal_id: (Required, string) - SCIM ID of the databricks_service_principal (not application ID).
            status: '- Status of the secret (i.e., ACTIVE - see REST API docs for full list).'
            time_rotating: '- (Optional, string) - Changing this argument forces recreation of the secret.'
            update_time: '- UTC time when the secret was updated.'
        importStatements: []
    databricks_share:
        subCategory: ""
        name: databricks_share
        title: databricks_share Resource
        examples:
            - name: some
              manifest: |-
                {
                  "dynamic": {
                    "object": [
                      {
                        "content": [
                          {
                            "data_object_type": "TABLE",
                            "name": "${object.value}"
                          }
                        ],
                        "for_each": "${data.databricks_tables.things.ids}"
                      }
                    ]
                  },
                  "name": "my_share"
                }
              references:
                dynamic.content.name: object.value
                dynamic.for_each: data.databricks_tables.things.ids
            - name: schema_share
              manifest: |-
                {
                  "name": "schema_share",
                  "object": [
                    {
                      "data_object_type": "SCHEMA",
                      "history_data_sharing_status": "ENABLED",
                      "name": "catalog_name.schema_name"
                    }
                  ]
                }
            - name: some
              manifest: |-
                {
                  "name": "my_share",
                  "object": [
                    {
                      "data_object_type": "TABLE",
                      "history_data_sharing_status": "ENABLED",
                      "name": "my_catalog.my_schema.my_table",
                      "partition": [
                        {
                          "value": [
                            {
                              "name": "year",
                              "op": "EQUAL",
                              "value": "2009"
                            },
                            {
                              "name": "month",
                              "op": "EQUAL",
                              "value": "12"
                            }
                          ]
                        },
                        {
                          "value": [
                            {
                              "name": "year",
                              "op": "EQUAL",
                              "value": "2010"
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            cdf_enabled: '- (Optional) Whether to enable Change Data Feed (cdf) on the shared object. When this field is set, field history_data_sharing_status can not be set.'
            comment: '- (Optional) User-supplied free-form text.'
            created_at: '- Time when the share was created.'
            created_by: '- The principal that created the share.'
            data_object_type: '- (Required) Type of the data object, currently TABLE, VIEW, SCHEMA, VOLUME, and MODEL are supported.'
            history_data_sharing_status: '- (Optional) Whether to enable history sharing, one of: ENABLED, DISABLED. When a table has history sharing enabled, recipients can query table data by version, starting from the current table version. If not specified, clients can only query starting from the version of the object at the time it was added to the share. NOTE: The start_version should be less than or equal the current version of the object. When this field is set, field cdf_enabled can not be set.'
            id: '- the ID of the share, the same as name.'
            name: '- (Required) Name of share. Change forces creation of a new resource.'
            op: '- The operator to apply for the value, one of: EQUAL, LIKE'
            owner: '- (Optional) User name/group name/sp application_id of the share owner.'
            recipient_property_key: '- (Optional) The key of a Delta Sharing recipient''s property. For example databricks-account-id. When this field is set, field value can not be set.'
            shared_as: '- (Optional) A user-provided new name for the data object within the share. If this new name is not provided, the object''s original name will be used as the shared_as name. The shared_as name must be unique within a Share. Change forces creation of a new resource.'
            start_version: '- (Optional) The start version associated with the object for cdf. This allows data providers to control the lowest object version that is accessible by clients.'
            status: '- Status of the object, one of: ACTIVE, PERMISSION_DENIED.'
            value: '- (Optional) The value of the partition column. When this value is not set, it means null value. When this field is set, field recipient_property_key can not be set.'
        importStatements: []
    databricks_sql_alert:
        subCategory: ""
        name: databricks_sql_alert
        title: databricks_sql_alert Resource
        examples:
            - name: alert
              manifest: |-
                {
                  "name": "My Alert",
                  "options": [
                    {
                      "column": "p1",
                      "muted": false,
                      "op": "==",
                      "value": "2"
                    }
                  ],
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "query_id": "${databricks_sql_query.this.id}",
                  "rearm": 1
                }
              references:
                query_id: databricks_sql_query.this.id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
                databricks_sql_query.this: |-
                    {
                      "data_source_id": "${databricks_sql_endpoint.example.data_source_id}",
                      "name": "My Query Name",
                      "parent": "folders/${databricks_directory.shared_dir.object_id}",
                      "query": "SELECT 1 AS p1, 2 as p2"
                    }
        argumentDocs:
            column: '- (Required, String) Name of column in the query result to compare in alert evaluation.'
            custom_body: '- (Optional, String) Custom body of alert notification, if it exists. See Alerts API reference for custom templating instructions.'
            custom_subject: '- (Optional, String) Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See Alerts API reference for custom templating instructions.'
            empty_result_state: '- (Optional, String) State that alert evaluates to when query result is empty.  Currently supported values are unknown, triggered, ok - check API documentation for full list of supported values.'
            id: '- unique ID of the SQL Alert.'
            muted: '- (Optional, bool) Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered.'
            name: '- (Required, String) Name of the alert.'
            op: '- (Required, String Enum) Operator used to compare in alert evaluation. (Enum: >, >=, <, <=, ==, !=)'
            options: '- (Required) Alert configuration options.'
            parent: '- (Optional, String) The identifier of the workspace folder containing the alert. The default is ther user''s home folder. The folder identifier is formatted as folder/<folder_id>.'
            query_id: '- (Required, String) ID of the query evaluated by the alert.'
            rearm: '- (Optional, Integer) Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.'
            value: '- (Required, String) Value used to compare in alert evaluation.'
        importStatements: []
    databricks_sql_dashboard:
        subCategory: ""
        name: databricks_sql_dashboard
        title: databricks_sql_dashboard Resource
        examples:
            - name: d1
              manifest: |-
                {
                  "name": "My Dashboard Name",
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "tags": [
                    "some-tag",
                    "another-tag"
                  ]
                }
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Dashboards"
                    }
        argumentDocs:
            id: '- the unique ID of the SQL Dashboard.'
        importStatements: []
    databricks_sql_endpoint:
        subCategory: ""
        name: databricks_sql_endpoint
        title: databricks_sql_endpoint Resource
        examples:
            - name: this
              manifest: |-
                {
                  "cluster_size": "Small",
                  "max_num_clusters": 1,
                  "name": "Endpoint of ${data.databricks_current_user.me.alphanumeric}",
                  "tags": [
                    {
                      "custom_tags": [
                        {
                          "key": "City",
                          "value": "Amsterdam"
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            auto_stop_mins: '- Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.'
            channel: 'block, consisting of following fields:'
            channel.name: '- Name of the Databricks SQL release channel. Possible values are: CHANNEL_NAME_PREVIEW and CHANNEL_NAME_CURRENT. Default is CHANNEL_NAME_CURRENT.'
            cluster_size: '- (Required) The size of the clusters allocated to the endpoint: "2X-Small", "X-Small", "Small", "Medium", "Large", "X-Large", "2X-Large", "3X-Large", "4X-Large".'
            creator_name: '- The username of the user who created the endpoint.'
            data_source_id: '- (Deprecated) ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.'
            databricks_sql_access: on databricks_group or databricks_user.
            enable_photon: '- Whether to enable Photon. This field is optional and is enabled by default.'
            enable_serverless_compute: '- Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.'
            "false": for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023 (between November 1, 2022 and May 19, 2023 for Azure), the default remains the previous behavior which is default to true if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated terms of use, workspace admins are prompted in the Databricks SQL UI. A workspace must meet the requirements.
            health: '- Health status of the endpoint.'
            id: '- the unique ID of the SQL warehouse.'
            jdbc_url: '- JDBC connection string.'
            max_num_clusters: '- Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to 1.'
            min_num_clusters: '- Minimum number of clusters available when a SQL warehouse is running. The default is 1.'
            name: '- (Required) Name of the SQL warehouse. Must be unique.'
            num_active_sessions: '- The current number of clusters used by the endpoint.'
            num_clusters: '- The current number of clusters used by the endpoint.'
            odbc_params: '- ODBC connection params: odbc_params.hostname, odbc_params.path, odbc_params.protocol, and odbc_params.port.'
            spot_instance_policy: '- The spot policy to use for allocating instances to clusters: COST_OPTIMIZED or RELIABILITY_OPTIMIZED. This field is optional. Default is COST_OPTIMIZED.'
            state: '- The current state of the endpoint.'
            tags: '- Databricks tags all endpoint resources with these tags.'
            warehouse_type: '- SQL warehouse type. See for AWS or Azure. Set to PRO or CLASSIC. If the field enable_serverless_compute has the value true either explicitly or through the default logic (see that field above for details), the default is PRO, which is required for serverless SQL warehouses. Otherwise, the default is CLASSIC.'
        importStatements: []
    databricks_sql_global_config:
        subCategory: ""
        name: databricks_sql_global_config
        title: databricks_sql_global_config Resource
        examples:
            - name: this
              manifest: |-
                {
                  "data_access_config": {
                    "spark.sql.session.timeZone": "UTC"
                  },
                  "instance_profile_arn": "arn:....",
                  "security_policy": "DATA_ACCESS_CONTROL"
                }
            - name: this
              manifest: |-
                {
                  "data_access_config": {
                    "spark.hadoop.fs.azure.account.auth.type": "OAuth",
                    "spark.hadoop.fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
                    "spark.hadoop.fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/${var.tenant_id}/oauth2/token",
                    "spark.hadoop.fs.azure.account.oauth2.client.id": "${var.application_id}",
                    "spark.hadoop.fs.azure.account.oauth2.client.secret": "{{secrets/${local.secret_scope}/${local.secret_key}}}"
                  },
                  "security_policy": "DATA_ACCESS_CONTROL",
                  "sql_config_params": {
                    "ANSI_MODE": "true"
                  }
                }
        argumentDocs:
            data_access_config: (Optional, Map) - Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the documentation for a full list.  Apply will fail if you're specifying not permitted configuration.
            google_service_account: (Optional, String) - used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.
            instance_profile_arn: (Optional, String) - databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.
            security_policy: '(Optional, String) - The policy for controlling access to datasets. Default value: DATA_ACCESS_CONTROL, consult documentation for list of possible values'
            sql_config_params: (Optional, Map) - SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.
        importStatements: []
    databricks_sql_permissions:
        subCategory: ""
        name: databricks_sql_permissions
        title: databricks_sql_permissions Resource
        examples:
            - name: foo_table
              manifest: |-
                {
                  "privilege_assignments": [
                    {
                      "principal": "serge@example.com",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    },
                    {
                      "principal": "special group",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "foo"
                }
            - name: foo_table
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.cluster_name.id}"
                }
              references:
                cluster_id: databricks_cluster.cluster_name.id
        argumentDocs:
            anonymous function/: '- anonymous function. / suffix is mandatory.'
            anonymous_function: '- (Boolean) If this access control for using an anonymous function. Defaults to false.'
            any file/: '- direct access to any file. / suffix is mandatory.'
            any_file: '- (Boolean) If this access control for reading/writing any file. Defaults to false.'
            catalog: '- (Boolean) If this access control for the entire catalog. Defaults to false.'
            catalog/: '- entire catalog. / suffix is mandatory.'
            cluster_id: '- (Optional) Id of an existing databricks_cluster, where the appropriate GRANT/REVOKE commands are executed. This cluster must have the appropriate data security mode (USER_ISOLATION or LEGACY_TABLE_ACL specified). If no cluster_id is specified, a TACL-enabled cluster with the name terraform-table-acl is automatically created.'
            database: '- Name of the database. Has a default value of default.'
            database/bar: '- bar database.'
            privilege_assignments.CREATE: '- gives the ability to create an object (for example, a table in a database).'
            privilege_assignments.CREATE_NAMED_FUNCTION: '- gives the ability to create a named UDF in an existing catalog or database.'
            privilege_assignments.MODIFY: '- gives the ability to add, delete, and modify data to or from an object.'
            privilege_assignments.MODIFY_CLASSPATH: '- gives the ability to add files to the Spark classpath.'
            privilege_assignments.READ_METADATA: '- gives the ability to view an object and its metadata.'
            privilege_assignments.SELECT: '- gives read access to an object.'
            privilege_assignments.USAGE: '- do not give any abilities, but is an additional requirement to perform any action on a database object.'
            privilege_assignments.principal: '- display_name for a databricks_group or databricks_user, application_id for a databricks_service_principal.'
            privilege_assignments.privileges: '- set of available privilege names in upper case.'
            table: '- Name of the table. Can be combined with the database.'
            table/default.foo: '- table foo in a default database. The database is always mandatory.'
            view: '- Name of the view. Can be combined with the database.'
            view/bar.foo: '- view foo in bar database.'
        importStatements: []
    databricks_sql_query:
        subCategory: ""
        name: databricks_sql_query
        title: databricks_sql_query Resource
        examples:
            - name: q1
              manifest: |-
                {
                  "data_source_id": "${databricks_sql_endpoint.example.data_source_id}",
                  "name": "My Query Name",
                  "parameter": [
                    {
                      "name": "p1",
                      "text": [
                        {
                          "value": "default"
                        }
                      ],
                      "title": "Title for p1"
                    },
                    {
                      "enum": [
                        {
                          "multiple": [
                            {
                              "prefix": "\"",
                              "separator": ",",
                              "suffix": "\""
                            }
                          ],
                          "options": [
                            "default",
                            "foo",
                            "bar"
                          ],
                          "value": "default"
                        }
                      ],
                      "name": "p2",
                      "title": "Title for p2"
                    },
                    {
                      "date": [
                        {
                          "value": "2022-01-01"
                        }
                      ],
                      "name": "p3",
                      "title": "Title for p3"
                    }
                  ],
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "query": "                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n",
                  "run_as_role": "viewer",
                  "tags": [
                    "t1",
                    "t2"
                  ]
                }
              references:
                data_source_id: databricks_sql_endpoint.example.data_source_id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
        argumentDocs:
            data_source_id: '- Data source ID of a SQL warehouse'
            description: '- General description that conveys additional information about this query such as usage notes.'
            id: '- the unique ID of the SQL Query.'
            name: '- The title of this query that appears in list views, widget headings, and on the query page.'
            parent: '- The identifier of the workspace folder containing the object.'
            query: '- The text of the query to be run.'
            run_as_role: '- Run as role. Possible values are viewer, owner.'
            text.value: '- The default value for this parameter.'
            title: '- The text displayed in a parameter picking widget.'
        importStatements: []
    databricks_sql_table:
        subCategory: ""
        name: databricks_sql_table
        title: databricks_sql_table Resource
        examples:
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "column": [
                    {
                      "name": "id",
                      "type": "int"
                    },
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "name": "quickstart_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "MANAGED"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: thing_view
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "cluster_id": "0423-201305-xsrt82qn",
                  "comment": "this view is managed by terraform",
                  "name": "quickstart_table_view",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "VIEW",
                  "view_definition": "${format(\"SELECT name FROM %s WHERE id == 1\", databricks_sql_table.thing.id)}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "column": [
                    {
                      "name": "id",
                      "type": "int"
                    },
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "name": "quickstart_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "MANAGED",
                  "warehouse_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
                warehouse_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "2X-Small",
                      "max_num_clusters": 1,
                      "name": "endpoint"
                    }
            - name: thing_view
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "comment": "this view is managed by terraform",
                  "name": "quickstart_table_view",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "VIEW",
                  "view_definition": "${format(\"SELECT name FROM %s WHERE id == 1\", databricks_sql_table.thing.id)}",
                  "warehouse_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
                warehouse_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "2X-Small",
                      "max_num_clusters": 1,
                      "name": "endpoint"
                    }
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "column": [
                    {
                      "identity": "default",
                      "name": "id",
                      "type": "bigint"
                    },
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "name": "identity_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "MANAGED"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "cluster_keys": [
                    "AUTO"
                  ],
                  "column": [
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "name": "auto_cluster_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "MANAGED"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
            - name: this
              manifest: |-
                {
                  "catalog_name": "catalog",
                  "column": [
                    {
                      "comment": "comment",
                      "name": "col1",
                      "nullable": true,
                      "type": "STRING",
                      "type_json": "{\"type\":\"STRING\"}"
                    }
                  ],
                  "comment": "comment",
                  "data_source_format": "DELTA",
                  "name": "table",
                  "properties": {
                    "key": "value"
                  },
                  "schema_name": "schema",
                  "table_type": "MANAGED"
                }
              dependencies:
                databricks_table.this: '{}'
        argumentDocs:
            catalog_name: '- Name of parent catalog. Change forces the creation of a new resource.'
            cluster_id: '- (Optional) All table CRUD operations must be executed on a running cluster or SQL warehouse. If a cluster_id is specified, it will be used to execute SQL commands to manage this table. If empty, a cluster will be created automatically with the name terraform-sql-table. Conflicts with warehouse_id.'
            cluster_keys: '- (Optional) a subset of columns to liquid cluster the table by. For automatic clustering, set cluster_keys to ["AUTO"]. To turn off clustering, set it to ["NONE"]. Conflicts with partitions.'
            comment: '- (Optional) User-supplied free-form text. Changing the comment is not currently supported on the VIEW table type.'
            data_source_format: '- (Optional) External tables are supported in multiple data source formats. The string constants identifying these formats are DELTA, CSV, JSON, AVRO, PARQUET, ORC, and TEXT. Change forces the creation of a new resource. Not supported for MANAGED tables or VIEW.'
            id: '- ID of this table in the form of <catalog_name>.<schema_name>.<name>.'
            identity: '- (Optional) Whether the field is an identity column. Can be default, always, or unset. It is unset by default.'
            name: '- Name of table relative to parent catalog and schema. Change forces the creation of a new resource.'
            nullable: '- (Optional) Whether field is nullable (Default: true)'
            options: '- (Optional) Map of user defined table options. Change forces creation of a new resource.'
            owner: '- (Optional) User name/group name/sp application_id of the table owner.'
            partitions: '- (Optional) a subset of columns to partition the table by. Change forces the creation of a new resource. Conflicts with cluster_keys.'
            properties: '- (Optional) A map of table properties.'
            schema_name: '- Name of parent Schema relative to parent Catalog. Change forces the creation of a new resource.'
            storage_credential_name: '- (Optional) For EXTERNAL Tables only: the name of storage credential to use. Change forces the creation of a new resource.'
            storage_location: '- (Optional) URL of storage location for Table data (required for EXTERNAL Tables). Not supported for VIEW or MANAGED table_type.'
            table_type: '- Distinguishes a view vs. managed/external Table. MANAGED, EXTERNAL, METRIC_VIEW or VIEW. Change forces the creation of a new resource.'
            type: '- Column type spec (with metadata) as SQL text. Not supported for VIEW table_type.'
            view_definition: '- (Optional) SQL text defining the view (for table_type == "VIEW"). Not supported for MANAGED or EXTERNAL table_type.'
            warehouse_id: '- (Optional) All table CRUD operations must be executed on a running cluster or SQL warehouse. If a warehouse_id is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with cluster_id.'
        importStatements: []
    databricks_sql_visualization:
        subCategory: ""
        name: databricks_sql_visualization
        title: databricks_sql_visualization Resource
        examples:
            - name: q1v1
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Table",
                  "options": "${jsonencode(\n    {\n      \"itemsPerPage\" : 25,\n      \"columns\" : [\n        {\n          \"name\" : \"p1\",\n          \"type\" : \"string\"\n          \"title\" : \"Parameter 1\",\n          \"displayAs\" : \"string\",\n        },\n        {\n          \"name\" : \"p2\",\n          \"type\" : \"string\"\n          \"title\" : \"Parameter 2\",\n          \"displayAs\" : \"link\",\n          \"highlightLinks\" : true,\n        }\n      ]\n    }\n  )}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "table"
                }
              references:
                query_id: databricks_sql_query.q1.id
            - name: q1v1
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Table",
                  "options": "${file(\"${path.module}/visualizations/q1v1.json\")}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "table"
                }
              references:
                query_id: databricks_sql_query.q1.id
            - name: q1v2
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Chart",
                  "options": "${file(\"${path.module}/visualizations/q1v2.json\")}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "chart"
                }
              references:
                query_id: databricks_sql_query.q1.id
        argumentDocs: {}
        importStatements: []
    databricks_sql_widget:
        subCategory: ""
        name: databricks_sql_widget
        title: databricks_sql_widget Resource
        examples:
            - name: d1w1
              manifest: |-
                {
                  "dashboard_id": "${databricks_sql_dashboard.d1.id}",
                  "position": [
                    {
                      "pos_x": 0,
                      "pos_y": 0,
                      "size_x": 3,
                      "size_y": 4
                    }
                  ],
                  "text": "Hello! I'm a **text widget**!"
                }
              references:
                dashboard_id: databricks_sql_dashboard.d1.id
            - name: d1w2
              manifest: |-
                {
                  "dashboard_id": "${databricks_sql_dashboard.d1.id}",
                  "position": [
                    {
                      "pos_x": 3,
                      "pos_y": 0,
                      "size_x": 3,
                      "size_y": 4
                    }
                  ],
                  "visualization_id": "${databricks_sql_visualization.q1v1.id}"
                }
              references:
                dashboard_id: databricks_sql_dashboard.d1.id
                visualization_id: databricks_sql_visualization.q1v1.id
        argumentDocs: {}
        importStatements: []
    databricks_storage_credential:
        subCategory: ""
        name: databricks_storage_credential
        title: databricks_storage_credential Resource
        examples:
            - name: external
              manifest: |-
                {
                  "aws_iam_role": [
                    {
                      "role_arn": "${aws_iam_role.external_data_access.arn}"
                    }
                  ],
                  "comment": "Managed by TF",
                  "name": "${aws_iam_role.external_data_access.name}"
                }
              references:
                aws_iam_role.role_arn: aws_iam_role.external_data_access.arn
                name: aws_iam_role.external_data_access.name
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external.id}"
                    }
            - name: external_mi
              manifest: |-
                {
                  "azure_managed_identity": [
                    {
                      "access_connector_id": "${azurerm_databricks_access_connector.example.id}"
                    }
                  ],
                  "comment": "Managed identity credential managed by TF",
                  "name": "mi_credential"
                }
              references:
                azure_managed_identity.access_connector_id: azurerm_databricks_access_connector.example.id
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external_mi.id}"
                    }
            - name: external
              manifest: |-
                {
                  "databricks_gcp_service_account": [
                    {}
                  ],
                  "name": "the-creds"
                }
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external.id}"
                    }
        argumentDocs:
            aws_iam_role.role_arn: '- The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF'
            azure_managed_identity.access_connector_id: '- The Resource ID of the Azure Databricks Access Connector resource, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name.'
            azure_managed_identity.managed_identity_id: '- (Optional) The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name.'
            azure_service_principal.application_id: '- The application ID of the application registration within the referenced AAD tenant'
            azure_service_principal.client_secret: '- The client secret generated for the above app ID in AAD. This field is redacted on output'
            azure_service_principal.directory_id: '- The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application'
            cloudflare_api_token.access_key_id: '- R2 API token access key ID'
            cloudflare_api_token.account_id: '- R2 account ID'
            cloudflare_api_token.secret_access_key: '- R2 API token secret access key'
            databricks_gcp_service_account.email: (output only) - The email of the GCP service account created, to be granted access to relevant buckets.
            databricks_storage_credential: represents authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal/managed identity for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.
            force_destroy: '- (Optional) Delete storage credential regardless of its dependencies.'
            force_update: '- (Optional) Update storage credential regardless of its dependents.'
            id: '- ID of this storage credential - same as the name.'
            isolation_mode: '- (Optional) Whether the storage credential is accessible from all workspaces or a specific set of workspaces. Can be ISOLATION_MODE_ISOLATED or ISOLATION_MODE_OPEN. Setting the credential to ISOLATION_MODE_ISOLATED will automatically allow access from the current workspace.'
            metastore_id: '- (Required for account-level) Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.'
            name: '- Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the storage credential owner.'
            read_only: '- (Optional) Indicates whether the storage credential is only usable for read operations.'
            skip_validation: '- (Optional) Suppress validation errors if any & force save the storage credential.'
            storage_credential_id: '- Unique ID of storage credential.'
        importStatements: []
    databricks_system_schema:
        subCategory: ""
        name: databricks_system_schema
        title: databricks_system_schema Resource
        examples:
            - name: this
              manifest: |-
                {
                  "schema": "access"
                }
        argumentDocs:
            full_name: '- the full name of the system schema, in form of system.<schema>.'
            id: '- the ID of system schema in form of metastore_id|schema_name.'
            schema: '- (Required) name of the system schema.'
            state: '- The current state of enablement for the system schema.'
        importStatements: []
    databricks_tag_policy Resource:
        subCategory: ""
        name: databricks_tag_policy Resource
        title: databricks_tag_policy Resource
        argumentDocs:
            description: (string, optional)
            id: (string)
            name: (string, required)
            tag_key: (string, required)
            values: (list of Value, optional)
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
    databricks_token:
        subCategory: ""
        name: databricks_token
        title: databricks_token Resource
        examples:
            - name: pat
              manifest: |-
                {
                  "comment": "Terraform Provisioning",
                  "lifetime_seconds": 8640000,
                  "provider": "${databricks.created_workspace}"
                }
              references:
                provider: databricks.created_workspace
            - name: pat
              manifest: |-
                {
                  "comment": "Terraform (created: ${time_rotating.this.rfc3339})",
                  "lifetime_seconds": "${60 * 24 * 60 * 60}"
                }
              dependencies:
                time_rotating.this: |-
                    {
                      "rotation_days": 30
                    }
        argumentDocs:
            comment: '- (Optional) (String) Comment that will appear on the user’s settings page for this token.'
            id: '- Canonical unique identifier for the token.'
            lifetime_seconds: '- (Optional) (Integer) The lifetime of the token, in seconds. If no lifetime is specified, then expire time will be set to maximum allowed by the workspace configuration or platform.'
            token_value: '- Sensitive value of the newly-created token.'
        importStatements: []
    databricks_user:
        subCategory: ""
        name: databricks_user
        title: databricks_user Resource
        examples:
            - name: me
              manifest: |-
                {
                  "user_name": "me@example.com"
                }
            - name: me
              manifest: |-
                {
                  "user_name": "me@example.com"
                }
              dependencies:
                databricks_group_member.i-am-admin: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_user.me.id}"
                    }
            - name: me
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "display_name": "Example user",
                  "user_name": "me@example.com"
                }
            - name: account_user
              manifest: |-
                {
                  "display_name": "Example user",
                  "provider": "${databricks.mws}",
                  "user_name": "me@example.com"
                }
              references:
                provider: databricks.mws
            - name: account_user
              manifest: |-
                {
                  "display_name": "Example user",
                  "provider": "${databricks.azure_account}",
                  "user_name": "me@example.com"
                }
              references:
                provider: databricks.azure_account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. users/mr.foo@example.com.'
            active: '- (Optional) Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.'
            allow_cluster_create: '-  (Optional) Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the user to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            disable_as_user_deletion: '- (Optional) Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to true when the provider is configured at the account-level and false when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.'
            display_name: '- (Optional) This is an alias for the username that can be the full name of the user.'
            external_id: '- (Optional) ID of the user in an external identity provider.'
            force: '- (Optional) Ignore cannot create user: User with username X already exists errors and implicitly import the specific user into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            force_delete_home_dir: '- (Optional) This flag determines whether the user''s home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.'
            force_delete_repos: '- (Optional) This flag determines whether the user''s repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            home: '- Home folder of the user, e.g. /Users/mr.foo@example.com.'
            id: '- Canonical unique identifier for the user (SCIM ID).'
            repos: '- Personal Repos location of the user, e.g. /Repos/mr.foo@example.com.'
            user_name: '- (Required) This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.'
            workspace_access: '- (Optional) This is a field to allow the user to have access to a Databricks Workspace.'
            workspace_consume: '- (Optional) This is a field to allow the user to have access to a Databricks Workspace as consumer, with limited access to workspace UI.  Couldn''t be used with workspace_access or databricks_sql_access.'
        importStatements: []
    databricks_user_instance_profile:
        subCategory: ""
        name: databricks_user_instance_profile
        title: databricks_user_instance_profile Resource
        examples:
            - name: my_user_instance_profile
              manifest: |-
                {
                  "instance_profile_id": "${databricks_instance_profile.instance_profile.id}",
                  "user_id": "${databricks_user.my_user.id}"
                }
              references:
                instance_profile_id: databricks_instance_profile.instance_profile.id
                user_id: databricks_user.my_user.id
              dependencies:
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
        argumentDocs:
            id: '- The id in the format <user_id>|<instance_profile_id>.'
            instance_profile_id: '-  (Required) This is the id of the instance profile resource.'
            user_id: '- (Required) This is the id of the user resource.'
        importStatements: []
    databricks_user_role:
        subCategory: ""
        name: databricks_user_role
        title: databricks_user_role Resource
        examples:
            - name: my_user_role
              manifest: |-
                {
                  "role": "${databricks_instance_profile.instance_profile.id}",
                  "user_id": "${databricks_user.my_user.id}"
                }
              references:
                role: databricks_instance_profile.instance_profile.id
                user_id: databricks_user.my_user.id
              dependencies:
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
            - name: my_user_account_admin
              manifest: |-
                {
                  "role": "account_admin",
                  "user_id": "${databricks_user.my_user.id}"
                }
              references:
                user_id: databricks_user.my_user.id
              dependencies:
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
        argumentDocs:
            id: '- The id in the format <user_id>|<role>.'
            role: '-  (Required) Either a role name or the ARN/ID of the instance profile resource.'
            user_id: '- (Required) This is the id of the user resource.'
        importStatements: []
    databricks_vector_search_endpoint:
        subCategory: ""
        name: databricks_vector_search_endpoint
        title: databricks_vector_search_endpoint Resource
        examples:
            - name: this
              manifest: |-
                {
                  "endpoint_type": "STANDARD",
                  "name": "vector-search-test"
                }
        argumentDocs:
            budget_policy_id: '- (Optional) The Budget Policy ID set for this resource.'
            creation_timestamp: '- Timestamp of endpoint creation (milliseconds).'
            creator: '- Creator of the endpoint.'
            effective_budget_policy_id: '- The effective budget policy ID.'
            endpoint_id: '- Unique internal identifier of the endpoint (UUID).'
            endpoint_status: '- Object describing the current status of the endpoint consisting of the following fields:'
            endpoint_type: '(Required) Type of Mosaic AI Vector Search Endpoint.  Currently only accepting single value: STANDARD (See documentation for the list of currently supported values). (Change leads to recreation of the resource).'
            id: '- The same as the name of the endpoint.'
            last_updated_timestamp: '- Timestamp of the last update to the endpoint (milliseconds).'
            last_updated_user: '- User who last updated the endpoint.'
            message: '- Additional status message.'
            name: '- (Required) Name of the Mosaic AI Vector Search Endpoint to create. (Change leads to recreation of the resource).'
            num_indexes: '- Number of indexes on the endpoint.'
            state: '- Current state of the endpoint. Currently following values are supported: PROVISIONING, ONLINE, and OFFLINE.'
        importStatements: []
    databricks_vector_search_index:
        subCategory: ""
        name: databricks_vector_search_index
        title: databricks_vector_search_index Resource
        examples:
            - name: sync
              manifest: |-
                {
                  "delta_sync_index_spec": [
                    {
                      "embedding_source_columns": [
                        {
                          "embedding_model_endpoint_name": "${databricks_model_serving.this.name}",
                          "name": "text"
                        }
                      ],
                      "pipeline_type": "TRIGGERED",
                      "source_table": "main.default.source_table"
                    }
                  ],
                  "endpoint_name": "${databricks_vector_search_endpoint.this.name}",
                  "index_type": "DELTA_SYNC",
                  "name": "main.default.vector_search_index",
                  "primary_key": "id"
                }
              references:
                delta_sync_index_spec.embedding_source_columns.embedding_model_endpoint_name: databricks_model_serving.this.name
                endpoint_name: databricks_vector_search_endpoint.this.name
        argumentDocs:
            CONTINUOUS: ': If the pipeline uses continuous execution, the pipeline processes new data as it arrives in the source table to keep the vector index fresh.'
            DELTA_SYNC: ': An index that automatically syncs with a source Delta Table, automatically and incrementally updating the index as the underlying data in the Delta Table changes.'
            DIRECT_ACCESS: ': An index that supports the direct read and write of vectors and metadata through our REST and SDK APIs. With this model, the user manages index updates.'
            TRIGGERED: ': If the pipeline uses the triggered execution mode, the system stops processing after successfully refreshing the source table in the pipeline once, ensuring the table is updated based on the data available when the update started.'
            creator: '- Creator of the endpoint.'
            delta_sync_index_spec: '- (object) Specification for Delta Sync Index. Required if index_type is DELTA_SYNC. This field is a block and is documented below.'
            delta_sync_index_spec.columns_to_sync: '- (optional) list of columns to sync. If not specified, all columns are syncronized.'
            delta_sync_index_spec.embedding_source_columns: '- (required if embedding_vector_columns isn''t provided) array of objects representing columns that contain the embedding source.  Each entry consists of:'
            delta_sync_index_spec.embedding_vector_columns: '- (required if embedding_source_columns isn''t provided)  array of objects representing columns that contain the embedding vectors. Each entry consists of:'
            delta_sync_index_spec.embedding_writeback_table: '- (optional) Automatically sync the vector index contents and computed embeddings to the specified Delta table. The only supported table name is the index name with the suffix _writeback_table.'
            delta_sync_index_spec.pipeline_type: '- Pipeline execution mode. Possible values are:'
            delta_sync_index_spec.source_table: (required) The name of the source table.
            direct_access_index_spec: '- (object) Specification for Direct Vector Access Index. Required if index_type is DIRECT_ACCESS. This field is a block and is documented below.'
            direct_access_index_spec.embedding_source_columns: '- (required if embedding_vector_columns isn''t provided) array of objects representing columns that contain the embedding source.  Each entry consists of:'
            direct_access_index_spec.embedding_vector_columns: '- (required if embedding_source_columns isn''t provided)  array of objects representing columns that contain the embedding vectors. Each entry consists of:'
            direct_access_index_spec.schema_json: '- The schema of the index in JSON format.  Check the API documentation for a list of supported data types.'
            embedding_dimension: '- Dimension of the embedding vector.'
            embedding_model_endpoint_name: '- The name of the embedding model endpoint, used by default for both ingestion and querying.'
            endpoint_name: '- (required) The name of the Mosaic AI Vector Search Endpoint that will be used for indexing the data.'
            id: '- The same as the name of the index.'
            index_type: '- (required) Mosaic AI Vector Search index type. Currently supported values are:'
            index_url: '- Index API Url to be used to perform operations on the index'
            indexed_row_count: '- Number of rows indexed'
            message: '- Message associated with the index status'
            model_endpoint_name_for_query: '- (Optional) The name of the embedding model endpoint which, if specified, is used for querying (not ingestion).'
            name: '- (required) Three-level name of the Mosaic AI Vector Search Index to create (catalog.schema.index_name).'
            pipeline_id: '- ID of the associated Delta Live Table pipeline.'
            primary_key: '- (required) The column name that will be used as a primary key.'
            ready: '- Whether the index is ready for search'
            status: '- Object describing the current status of the index consisting of the following fields:'
        importStatements: []
    databricks_volume:
        subCategory: ""
        name: databricks_volume
        title: databricks_volume Resource
        examples:
            - name: this
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "comment": "this volume is managed by terraform",
                  "name": "quickstart_volume",
                  "schema_name": "${databricks_schema.things.name}",
                  "storage_location": "${databricks_external_location.some.url}",
                  "volume_type": "EXTERNAL"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                schema_name: databricks_schema.things.name
                storage_location: databricks_external_location.some.url
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_external_location.some: |-
                    {
                      "credential_name": "${databricks_storage_credential.external.name}",
                      "name": "external_location",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "name": "creds"
                    }
        argumentDocs:
            <catalogName>: ': The name of the catalog containing the Volume.'
            <schemaName>: ': The name of the schema containing the Volume.'
            <volumeName>: ': The name of the Volume. It identifies the volume object.'
            catalog_name: '- Name of parent Catalog. Change forces creation of a new resource.'
            comment: '- (Optional) Free-form text.'
            id: '- ID of this Unity Catalog Volume in form of <catalog>.<schema>.<name>.'
            name: '- Name of the Volume'
            owner: '- (Optional) Name of the volume owner.'
            schema_name: '- Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.'
            storage_location: '- (Optional) Path inside an External Location. Only used for EXTERNAL Volumes. Change forces creation of a new resource.'
            volume_path: '- base file path for this Unity Catalog Volume in form of /Volumes/<catalog>/<schema>/<name>.'
            volume_type: '- Volume type. EXTERNAL or MANAGED. Change forces creation of a new resource.'
        importStatements: []
    databricks_workspace_binding:
        subCategory: ""
        name: databricks_workspace_binding
        title: databricks_workspace_binding Resource
        examples:
            - name: sandbox
              manifest: |-
                {
                  "securable_name": "${databricks_catalog.sandbox.name}",
                  "workspace_id": "${databricks_mws_workspaces.other.workspace_id}"
                }
              references:
                securable_name: databricks_catalog.sandbox.name
                workspace_id: databricks_mws_workspaces.other.workspace_id
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "isolation_mode": "ISOLATED",
                      "name": "sandbox"
                    }
        argumentDocs:
            binding_type: '- (Optional) Binding mode. Default to BINDING_TYPE_READ_WRITE. Possible values are BINDING_TYPE_READ_ONLY, BINDING_TYPE_READ_WRITE.'
            securable_name: '- Name of securable. Change forces creation of a new resource.'
            securable_type: '- Type of securable. Can be catalog, external_location, storage_credential or credential. Default to catalog. Change forces creation of a new resource.'
            workspace_id: '- ID of the workspace. Change forces creation of a new resource.'
        importStatements: []
    databricks_workspace_conf:
        subCategory: ""
        name: databricks_workspace_conf
        title: databricks_workspace_conf Resource
        examples:
            - name: this
              manifest: |-
                {
                  "custom_config": {
                    "enableIpAccessLists": true
                  }
                }
        argumentDocs:
            custom_config: '- (Required) Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with enable or enforce will be reset to false value, regardless of initial default one.'
            enableDeprecatedClusterNamedInitScripts: '- (boolean) Enable or disable legacy cluster-named init scripts for this workspace.'
            enableDeprecatedGlobalInitScripts: '- (boolean) Enable or disable legacy global init scripts for this workspace.'
            enableIpAccessLists: '- enables the use of databricks_ip_access_list resources'
            enableTokensConfig: '- (boolean) Enable or disable personal access tokens for this workspace.'
            maxTokenLifetimeDays: '- (string) Maximum token lifetime of new tokens in days, as an integer. This value can range from 1 day to 730 days (2 years). If not specified, the maximum lifetime of new tokens is 730 days. WARNING: This limit only applies to new tokens, so there may be tokens with lifetimes longer than this value, including unlimited lifetime. Such tokens may have been created before the current maximum token lifetime was set.'
        importStatements: []
    databricks_workspace_file:
        subCategory: ""
        name: databricks_workspace_file
        title: databricks_workspace_file Resource
        examples:
            - name: module
              manifest: |-
                {
                  "path": "${data.databricks_current_user.me.home}/AA/BB/CC",
                  "source": "${path.module}/module.py"
                }
            - name: init_script
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"Hello World\"\n    EOT\n  )}",
                  "path": "/Shared/init-script.sh"
                }
        argumentDocs:
            content_base64: '- The base64-encoded file content. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a workspace file with configuration properties for a data pipeline.'
            id: '-  Path of workspace file'
            object_id: '-  Unique identifier for a workspace file'
            path: '-  (Required) The absolute path of the workspace file, beginning with "/", e.g. "/Demo".'
            source: '- Path to file on local filesystem. Conflicts with content_base64.'
            url: '- Routable URL of the workspace file'
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_workspace_network_option:
        subCategory: ""
        name: databricks_workspace_network_option
        title: databricks_workspace_network_option Resource
        examples:
            - name: example_workspace_network_option
              manifest: |-
                {
                  "network_policy_id": "default-policy",
                  "workspace_id": "9999999999999999"
                }
        argumentDocs:
            network_policy_id: |-
                (string, optional) - The network policy ID to apply to the workspace. This controls the network access rules
                for all serverless compute resources in the workspace. Each workspace can only be
                linked to one policy at a time. If no policy is explicitly assigned,
                the workspace will use 'default-policy'
            workspace_id: (integer, optional) - The workspace ID
        importStatements: []
    databricks_workspace_setting_v2 Resource:
        subCategory: ""
        name: databricks_workspace_setting_v2 Resource
        title: databricks_workspace_setting_v2 Resource
        argumentDocs:
            access_policy_type: '(string, required) - . Possible values are: ALLOW_ALL_DOMAINS, ALLOW_APPROVED_DOMAINS, DENY_ALL_DOMAINS'
            aibi_dashboard_embedding_access_policy: (AibiDashboardEmbeddingAccessPolicy, optional)
            aibi_dashboard_embedding_approved_domains: (AibiDashboardEmbeddingApprovedDomains, optional)
            approved_domains: (list of string, optional)
            automatic_cluster_update_workspace: '(ClusterAutoRestartMessage, optional) - todo: Mark these Public after onboarded to DSL'
            boolean_val: (BooleanMessage, optional)
            can_toggle: (boolean, optional)
            day_of_week: '(string, optional) - . Possible values are: FRIDAY, MONDAY, SATURDAY, SUNDAY, THURSDAY, TUESDAY, WEDNESDAY'
            default_data_security_mode: (DefaultDataSecurityModeMessage, optional)
            effective_aibi_dashboard_embedding_access_policy: (AibiDashboardEmbeddingAccessPolicy, optional)
            effective_aibi_dashboard_embedding_approved_domains: (AibiDashboardEmbeddingApprovedDomains, optional)
            effective_automatic_cluster_update_workspace: (ClusterAutoRestartMessage, optional)
            effective_boolean_val: (BooleanMessage)
            effective_default_data_security_mode: (DefaultDataSecurityModeMessage, optional)
            effective_integer_val: (IntegerMessage)
            effective_personal_compute: (PersonalComputeMessage, optional)
            effective_restrict_workspace_admins: (RestrictWorkspaceAdminsMessage, optional)
            effective_string_val: (StringMessage)
            enabled: (boolean, optional)
            enablement_details: (ClusterAutoRestartMessageEnablementDetails, optional)
            forced_for_compliance_mode: (boolean, optional) - The feature is force enabled if compliance mode is active
            frequency: '(string, optional) - . Possible values are: EVERY_WEEK, FIRST_AND_THIRD_OF_MONTH, FIRST_OF_MONTH, FOURTH_OF_MONTH, SECOND_AND_FOURTH_OF_MONTH, SECOND_OF_MONTH, THIRD_OF_MONTH'
            hours: (integer, optional)
            integer_val: (IntegerMessage, optional)
            maintenance_window: (ClusterAutoRestartMessageMaintenanceWindow, optional)
            minutes: (integer, optional)
            name: (string, optional) - Name of the setting
            personal_compute: (PersonalComputeMessage, optional)
            restart_even_if_no_updates_available: (boolean, optional)
            restrict_workspace_admins: (RestrictWorkspaceAdminsMessage, optional)
            status: '(string, required) - . Possible values are: NOT_SET, SINGLE_USER, USER_ISOLATION'
            string_val: (StringMessage, optional)
            unavailable_for_disabled_entitlement: (boolean, optional) - The feature is unavailable if the corresponding entitlement disabled (see getShieldEntitlementEnable)
            unavailable_for_non_enterprise_tier: (boolean, optional) - The feature is unavailable if the customer doesn't have enterprise tier
            value: (boolean, optional)
            week_day_based_schedule: (ClusterAutoRestartMessageMaintenanceWindowWeekDayBasedSchedule, optional)
            window_start_time: (ClusterAutoRestartMessageMaintenanceWindowWindowStartTime, optional)
            workspace_id: (string, optional) - Workspace ID of the resource
        importStatements: []
