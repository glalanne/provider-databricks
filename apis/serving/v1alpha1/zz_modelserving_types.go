// SPDX-FileCopyrightText: 2024 The Crossplane Authors <https://crossplane.io>
//
// SPDX-License-Identifier: Apache-2.0

// Code generated by upjet. DO NOT EDIT.

package v1alpha1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	v1 "github.com/crossplane/crossplane-runtime/apis/common/v1"
)

type AIGatewayInitParameters struct {

	// Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:
	Guardrails []GuardrailsInitParameters `json:"guardrails,omitempty" tf:"guardrails,omitempty"`

	// Block describing the configuration of usage tracking. Consists of the following attributes:
	InferenceTableConfig []InferenceTableConfigInitParameters `json:"inferenceTableConfig,omitempty" tf:"inference_table_config,omitempty"`

	// Block describing rate limits for AI gateway. For details see the description of rate_limits block above.
	RateLimits []RateLimitsInitParameters `json:"rateLimits,omitempty" tf:"rate_limits,omitempty"`

	// Block with configuration for payload logging using inference tables. For details see the description of auto_capture_config block above.
	UsageTrackingConfig []UsageTrackingConfigInitParameters `json:"usageTrackingConfig,omitempty" tf:"usage_tracking_config,omitempty"`
}

type AIGatewayObservation struct {

	// Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:
	Guardrails []GuardrailsObservation `json:"guardrails,omitempty" tf:"guardrails,omitempty"`

	// Block describing the configuration of usage tracking. Consists of the following attributes:
	InferenceTableConfig []InferenceTableConfigObservation `json:"inferenceTableConfig,omitempty" tf:"inference_table_config,omitempty"`

	// Block describing rate limits for AI gateway. For details see the description of rate_limits block above.
	RateLimits []RateLimitsObservation `json:"rateLimits,omitempty" tf:"rate_limits,omitempty"`

	// Block with configuration for payload logging using inference tables. For details see the description of auto_capture_config block above.
	UsageTrackingConfig []UsageTrackingConfigObservation `json:"usageTrackingConfig,omitempty" tf:"usage_tracking_config,omitempty"`
}

type AIGatewayParameters struct {

	// Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:
	// +kubebuilder:validation:Optional
	Guardrails []GuardrailsParameters `json:"guardrails,omitempty" tf:"guardrails,omitempty"`

	// Block describing the configuration of usage tracking. Consists of the following attributes:
	// +kubebuilder:validation:Optional
	InferenceTableConfig []InferenceTableConfigParameters `json:"inferenceTableConfig,omitempty" tf:"inference_table_config,omitempty"`

	// Block describing rate limits for AI gateway. For details see the description of rate_limits block above.
	// +kubebuilder:validation:Optional
	RateLimits []RateLimitsParameters `json:"rateLimits,omitempty" tf:"rate_limits,omitempty"`

	// Block with configuration for payload logging using inference tables. For details see the description of auto_capture_config block above.
	// +kubebuilder:validation:Optional
	UsageTrackingConfig []UsageTrackingConfigParameters `json:"usageTrackingConfig,omitempty" tf:"usage_tracking_config,omitempty"`
}

type Ai21LabsConfigInitParameters struct {

	// The Databricks secret key reference for an AI21Labs API key.
	Ai21LabsAPIKey *string `json:"ai21labsApiKey,omitempty" tf:"ai21labs_api_key,omitempty"`

	// An AI21 Labs API key provided as a plaintext string.
	Ai21LabsAPIKeyPlaintext *string `json:"ai21labsApiKeyPlaintext,omitempty" tf:"ai21labs_api_key_plaintext,omitempty"`
}

type Ai21LabsConfigObservation struct {

	// The Databricks secret key reference for an AI21Labs API key.
	Ai21LabsAPIKey *string `json:"ai21labsApiKey,omitempty" tf:"ai21labs_api_key,omitempty"`

	// An AI21 Labs API key provided as a plaintext string.
	Ai21LabsAPIKeyPlaintext *string `json:"ai21labsApiKeyPlaintext,omitempty" tf:"ai21labs_api_key_plaintext,omitempty"`
}

type Ai21LabsConfigParameters struct {

	// The Databricks secret key reference for an AI21Labs API key.
	// +kubebuilder:validation:Optional
	Ai21LabsAPIKey *string `json:"ai21labsApiKey,omitempty" tf:"ai21labs_api_key,omitempty"`

	// An AI21 Labs API key provided as a plaintext string.
	// +kubebuilder:validation:Optional
	Ai21LabsAPIKeyPlaintext *string `json:"ai21labsApiKeyPlaintext,omitempty" tf:"ai21labs_api_key_plaintext,omitempty"`
}

type AmazonBedrockConfigInitParameters struct {

	// The Databricks secret key reference for an AWS Access Key ID with permissions to interact with Bedrock services.
	AwsAccessKeyID *string `json:"awsAccessKeyId,omitempty" tf:"aws_access_key_id,omitempty"`

	// An AWS access key ID with permissions to interact with Bedrock services provided as a plaintext string.
	AwsAccessKeyIDPlaintext *string `json:"awsAccessKeyIdPlaintext,omitempty" tf:"aws_access_key_id_plaintext,omitempty"`

	// The AWS region to use. Bedrock has to be enabled there.
	AwsRegion *string `json:"awsRegion,omitempty" tf:"aws_region,omitempty"`

	// The Databricks secret key reference for an AWS Secret Access Key paired with the access key ID, with permissions to interact with Bedrock services.
	AwsSecretAccessKey *string `json:"awsSecretAccessKey,omitempty" tf:"aws_secret_access_key,omitempty"`

	// An AWS secret access key paired with the access key ID, with permissions to interact with Bedrock services provided as a plaintext string.
	AwsSecretAccessKeyPlaintext *string `json:"awsSecretAccessKeyPlaintext,omitempty" tf:"aws_secret_access_key_plaintext,omitempty"`

	// The underlying provider in Amazon Bedrock. Supported values (case insensitive) include: Anthropic, Cohere, AI21Labs, Amazon.
	BedrockProvider *string `json:"bedrockProvider,omitempty" tf:"bedrock_provider,omitempty"`
}

type AmazonBedrockConfigObservation struct {

	// The Databricks secret key reference for an AWS Access Key ID with permissions to interact with Bedrock services.
	AwsAccessKeyID *string `json:"awsAccessKeyId,omitempty" tf:"aws_access_key_id,omitempty"`

	// An AWS access key ID with permissions to interact with Bedrock services provided as a plaintext string.
	AwsAccessKeyIDPlaintext *string `json:"awsAccessKeyIdPlaintext,omitempty" tf:"aws_access_key_id_plaintext,omitempty"`

	// The AWS region to use. Bedrock has to be enabled there.
	AwsRegion *string `json:"awsRegion,omitempty" tf:"aws_region,omitempty"`

	// The Databricks secret key reference for an AWS Secret Access Key paired with the access key ID, with permissions to interact with Bedrock services.
	AwsSecretAccessKey *string `json:"awsSecretAccessKey,omitempty" tf:"aws_secret_access_key,omitempty"`

	// An AWS secret access key paired with the access key ID, with permissions to interact with Bedrock services provided as a plaintext string.
	AwsSecretAccessKeyPlaintext *string `json:"awsSecretAccessKeyPlaintext,omitempty" tf:"aws_secret_access_key_plaintext,omitempty"`

	// The underlying provider in Amazon Bedrock. Supported values (case insensitive) include: Anthropic, Cohere, AI21Labs, Amazon.
	BedrockProvider *string `json:"bedrockProvider,omitempty" tf:"bedrock_provider,omitempty"`
}

type AmazonBedrockConfigParameters struct {

	// The Databricks secret key reference for an AWS Access Key ID with permissions to interact with Bedrock services.
	// +kubebuilder:validation:Optional
	AwsAccessKeyID *string `json:"awsAccessKeyId,omitempty" tf:"aws_access_key_id,omitempty"`

	// An AWS access key ID with permissions to interact with Bedrock services provided as a plaintext string.
	// +kubebuilder:validation:Optional
	AwsAccessKeyIDPlaintext *string `json:"awsAccessKeyIdPlaintext,omitempty" tf:"aws_access_key_id_plaintext,omitempty"`

	// The AWS region to use. Bedrock has to be enabled there.
	// +kubebuilder:validation:Optional
	AwsRegion *string `json:"awsRegion" tf:"aws_region,omitempty"`

	// The Databricks secret key reference for an AWS Secret Access Key paired with the access key ID, with permissions to interact with Bedrock services.
	// +kubebuilder:validation:Optional
	AwsSecretAccessKey *string `json:"awsSecretAccessKey,omitempty" tf:"aws_secret_access_key,omitempty"`

	// An AWS secret access key paired with the access key ID, with permissions to interact with Bedrock services provided as a plaintext string.
	// +kubebuilder:validation:Optional
	AwsSecretAccessKeyPlaintext *string `json:"awsSecretAccessKeyPlaintext,omitempty" tf:"aws_secret_access_key_plaintext,omitempty"`

	// The underlying provider in Amazon Bedrock. Supported values (case insensitive) include: Anthropic, Cohere, AI21Labs, Amazon.
	// +kubebuilder:validation:Optional
	BedrockProvider *string `json:"bedrockProvider" tf:"bedrock_provider,omitempty"`
}

type AnthropicConfigInitParameters struct {

	// The Databricks secret key reference for an Anthropic API key.
	AnthropicAPIKey *string `json:"anthropicApiKey,omitempty" tf:"anthropic_api_key,omitempty"`

	// The Anthropic API key provided as a plaintext string.
	AnthropicAPIKeyPlaintext *string `json:"anthropicApiKeyPlaintext,omitempty" tf:"anthropic_api_key_plaintext,omitempty"`
}

type AnthropicConfigObservation struct {

	// The Databricks secret key reference for an Anthropic API key.
	AnthropicAPIKey *string `json:"anthropicApiKey,omitempty" tf:"anthropic_api_key,omitempty"`

	// The Anthropic API key provided as a plaintext string.
	AnthropicAPIKeyPlaintext *string `json:"anthropicApiKeyPlaintext,omitempty" tf:"anthropic_api_key_plaintext,omitempty"`
}

type AnthropicConfigParameters struct {

	// The Databricks secret key reference for an Anthropic API key.
	// +kubebuilder:validation:Optional
	AnthropicAPIKey *string `json:"anthropicApiKey,omitempty" tf:"anthropic_api_key,omitempty"`

	// The Anthropic API key provided as a plaintext string.
	// +kubebuilder:validation:Optional
	AnthropicAPIKeyPlaintext *string `json:"anthropicApiKeyPlaintext,omitempty" tf:"anthropic_api_key_plaintext,omitempty"`
}

type AutoCaptureConfigInitParameters struct {

	// The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.
	CatalogName *string `json:"catalogName,omitempty" tf:"catalog_name,omitempty"`

	// boolean flag specifying if usage tracking is enabled.
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.
	SchemaName *string `json:"schemaName,omitempty" tf:"schema_name,omitempty"`

	// The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.
	TableNamePrefix *string `json:"tableNamePrefix,omitempty" tf:"table_name_prefix,omitempty"`
}

type AutoCaptureConfigObservation struct {

	// The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.
	CatalogName *string `json:"catalogName,omitempty" tf:"catalog_name,omitempty"`

	// boolean flag specifying if usage tracking is enabled.
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.
	SchemaName *string `json:"schemaName,omitempty" tf:"schema_name,omitempty"`

	// The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.
	TableNamePrefix *string `json:"tableNamePrefix,omitempty" tf:"table_name_prefix,omitempty"`
}

type AutoCaptureConfigParameters struct {

	// The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.
	// +kubebuilder:validation:Optional
	CatalogName *string `json:"catalogName,omitempty" tf:"catalog_name,omitempty"`

	// boolean flag specifying if usage tracking is enabled.
	// +kubebuilder:validation:Optional
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.
	// +kubebuilder:validation:Optional
	SchemaName *string `json:"schemaName,omitempty" tf:"schema_name,omitempty"`

	// The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.
	// +kubebuilder:validation:Optional
	TableNamePrefix *string `json:"tableNamePrefix,omitempty" tf:"table_name_prefix,omitempty"`
}

type CohereConfigInitParameters struct {
	CohereAPIBase *string `json:"cohereApiBase,omitempty" tf:"cohere_api_base,omitempty"`

	// The Databricks secret key reference for a Cohere API key.
	CohereAPIKey *string `json:"cohereApiKey,omitempty" tf:"cohere_api_key,omitempty"`

	// The Cohere API key provided as a plaintext string.
	CohereAPIKeyPlaintext *string `json:"cohereApiKeyPlaintext,omitempty" tf:"cohere_api_key_plaintext,omitempty"`
}

type CohereConfigObservation struct {
	CohereAPIBase *string `json:"cohereApiBase,omitempty" tf:"cohere_api_base,omitempty"`

	// The Databricks secret key reference for a Cohere API key.
	CohereAPIKey *string `json:"cohereApiKey,omitempty" tf:"cohere_api_key,omitempty"`

	// The Cohere API key provided as a plaintext string.
	CohereAPIKeyPlaintext *string `json:"cohereApiKeyPlaintext,omitempty" tf:"cohere_api_key_plaintext,omitempty"`
}

type CohereConfigParameters struct {

	// +kubebuilder:validation:Optional
	CohereAPIBase *string `json:"cohereApiBase,omitempty" tf:"cohere_api_base,omitempty"`

	// The Databricks secret key reference for a Cohere API key.
	// +kubebuilder:validation:Optional
	CohereAPIKey *string `json:"cohereApiKey,omitempty" tf:"cohere_api_key,omitempty"`

	// The Cohere API key provided as a plaintext string.
	// +kubebuilder:validation:Optional
	CohereAPIKeyPlaintext *string `json:"cohereApiKeyPlaintext,omitempty" tf:"cohere_api_key_plaintext,omitempty"`
}

type ConfigInitParameters struct {

	// Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.
	AutoCaptureConfig []AutoCaptureConfigInitParameters `json:"autoCaptureConfig,omitempty" tf:"auto_capture_config,omitempty"`

	// A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities.
	ServedEntities []ServedEntitiesInitParameters `json:"servedEntities,omitempty" tf:"served_entities,omitempty"`

	// (Deprecated, use served_entities instead) Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.
	ServedModels []ServedModelsInitParameters `json:"servedModels,omitempty" tf:"served_models,omitempty"`

	// A single block represents the traffic split configuration amongst the served models.
	TrafficConfig []TrafficConfigInitParameters `json:"trafficConfig,omitempty" tf:"traffic_config,omitempty"`
}

type ConfigObservation struct {

	// Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.
	AutoCaptureConfig []AutoCaptureConfigObservation `json:"autoCaptureConfig,omitempty" tf:"auto_capture_config,omitempty"`

	// A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities.
	ServedEntities []ServedEntitiesObservation `json:"servedEntities,omitempty" tf:"served_entities,omitempty"`

	// (Deprecated, use served_entities instead) Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.
	ServedModels []ServedModelsObservation `json:"servedModels,omitempty" tf:"served_models,omitempty"`

	// A single block represents the traffic split configuration amongst the served models.
	TrafficConfig []TrafficConfigObservation `json:"trafficConfig,omitempty" tf:"traffic_config,omitempty"`
}

type ConfigParameters struct {

	// Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.
	// +kubebuilder:validation:Optional
	AutoCaptureConfig []AutoCaptureConfigParameters `json:"autoCaptureConfig,omitempty" tf:"auto_capture_config,omitempty"`

	// A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities.
	// +kubebuilder:validation:Optional
	ServedEntities []ServedEntitiesParameters `json:"servedEntities,omitempty" tf:"served_entities,omitempty"`

	// (Deprecated, use served_entities instead) Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.
	// +kubebuilder:validation:Optional
	ServedModels []ServedModelsParameters `json:"servedModels,omitempty" tf:"served_models,omitempty"`

	// A single block represents the traffic split configuration amongst the served models.
	// +kubebuilder:validation:Optional
	TrafficConfig []TrafficConfigParameters `json:"trafficConfig,omitempty" tf:"traffic_config,omitempty"`
}

type DatabricksModelServingConfigInitParameters struct {

	// The Databricks secret key reference for a Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model.
	DatabricksAPIToken *string `json:"databricksApiToken,omitempty" tf:"databricks_api_token,omitempty"`

	// The Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model provided as a plaintext string.
	DatabricksAPITokenPlaintext *string `json:"databricksApiTokenPlaintext,omitempty" tf:"databricks_api_token_plaintext,omitempty"`

	// The URL of the Databricks workspace containing the model serving endpoint pointed to by this external model.
	DatabricksWorkspaceURL *string `json:"databricksWorkspaceUrl,omitempty" tf:"databricks_workspace_url,omitempty"`
}

type DatabricksModelServingConfigObservation struct {

	// The Databricks secret key reference for a Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model.
	DatabricksAPIToken *string `json:"databricksApiToken,omitempty" tf:"databricks_api_token,omitempty"`

	// The Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model provided as a plaintext string.
	DatabricksAPITokenPlaintext *string `json:"databricksApiTokenPlaintext,omitempty" tf:"databricks_api_token_plaintext,omitempty"`

	// The URL of the Databricks workspace containing the model serving endpoint pointed to by this external model.
	DatabricksWorkspaceURL *string `json:"databricksWorkspaceUrl,omitempty" tf:"databricks_workspace_url,omitempty"`
}

type DatabricksModelServingConfigParameters struct {

	// The Databricks secret key reference for a Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model.
	// +kubebuilder:validation:Optional
	DatabricksAPIToken *string `json:"databricksApiToken,omitempty" tf:"databricks_api_token,omitempty"`

	// The Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model provided as a plaintext string.
	// +kubebuilder:validation:Optional
	DatabricksAPITokenPlaintext *string `json:"databricksApiTokenPlaintext,omitempty" tf:"databricks_api_token_plaintext,omitempty"`

	// The URL of the Databricks workspace containing the model serving endpoint pointed to by this external model.
	// +kubebuilder:validation:Optional
	DatabricksWorkspaceURL *string `json:"databricksWorkspaceUrl" tf:"databricks_workspace_url,omitempty"`
}

type ExternalModelInitParameters struct {

	// AI21Labs Config
	Ai21LabsConfig []Ai21LabsConfigInitParameters `json:"ai21labsConfig,omitempty" tf:"ai21labs_config,omitempty"`

	// Amazon Bedrock Config
	AmazonBedrockConfig []AmazonBedrockConfigInitParameters `json:"amazonBedrockConfig,omitempty" tf:"amazon_bedrock_config,omitempty"`

	// Anthropic Config
	AnthropicConfig []AnthropicConfigInitParameters `json:"anthropicConfig,omitempty" tf:"anthropic_config,omitempty"`

	// Cohere Config
	CohereConfig []CohereConfigInitParameters `json:"cohereConfig,omitempty" tf:"cohere_config,omitempty"`

	// Databricks Model Serving Config
	DatabricksModelServingConfig []DatabricksModelServingConfigInitParameters `json:"databricksModelServingConfig,omitempty" tf:"databricks_model_serving_config,omitempty"`

	// Google Cloud Vertex AI Config.
	GoogleCloudVertexAIConfig []GoogleCloudVertexAIConfigInitParameters `json:"googleCloudVertexAiConfig,omitempty" tf:"google_cloud_vertex_ai_config,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// OpenAI Config
	OpenaiConfig []OpenaiConfigInitParameters `json:"openaiConfig,omitempty" tf:"openai_config,omitempty"`

	// PaLM Config
	PalmConfig []PalmConfigInitParameters `json:"palmConfig,omitempty" tf:"palm_config,omitempty"`

	// The name of the provider for the external model. Currently, the supported providers are ai21labs, anthropic, amazon-bedrock, cohere, databricks-model-serving, google-cloud-vertex-ai, openai, and palm.
	Provider *string `json:"provider,omitempty" tf:"provider,omitempty"`

	// The task type of the external model.
	Task *string `json:"task,omitempty" tf:"task,omitempty"`
}

type ExternalModelObservation struct {

	// AI21Labs Config
	Ai21LabsConfig []Ai21LabsConfigObservation `json:"ai21labsConfig,omitempty" tf:"ai21labs_config,omitempty"`

	// Amazon Bedrock Config
	AmazonBedrockConfig []AmazonBedrockConfigObservation `json:"amazonBedrockConfig,omitempty" tf:"amazon_bedrock_config,omitempty"`

	// Anthropic Config
	AnthropicConfig []AnthropicConfigObservation `json:"anthropicConfig,omitempty" tf:"anthropic_config,omitempty"`

	// Cohere Config
	CohereConfig []CohereConfigObservation `json:"cohereConfig,omitempty" tf:"cohere_config,omitempty"`

	// Databricks Model Serving Config
	DatabricksModelServingConfig []DatabricksModelServingConfigObservation `json:"databricksModelServingConfig,omitempty" tf:"databricks_model_serving_config,omitempty"`

	// Google Cloud Vertex AI Config.
	GoogleCloudVertexAIConfig []GoogleCloudVertexAIConfigObservation `json:"googleCloudVertexAiConfig,omitempty" tf:"google_cloud_vertex_ai_config,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// OpenAI Config
	OpenaiConfig []OpenaiConfigObservation `json:"openaiConfig,omitempty" tf:"openai_config,omitempty"`

	// PaLM Config
	PalmConfig []PalmConfigObservation `json:"palmConfig,omitempty" tf:"palm_config,omitempty"`

	// The name of the provider for the external model. Currently, the supported providers are ai21labs, anthropic, amazon-bedrock, cohere, databricks-model-serving, google-cloud-vertex-ai, openai, and palm.
	Provider *string `json:"provider,omitempty" tf:"provider,omitempty"`

	// The task type of the external model.
	Task *string `json:"task,omitempty" tf:"task,omitempty"`
}

type ExternalModelParameters struct {

	// AI21Labs Config
	// +kubebuilder:validation:Optional
	Ai21LabsConfig []Ai21LabsConfigParameters `json:"ai21labsConfig,omitempty" tf:"ai21labs_config,omitempty"`

	// Amazon Bedrock Config
	// +kubebuilder:validation:Optional
	AmazonBedrockConfig []AmazonBedrockConfigParameters `json:"amazonBedrockConfig,omitempty" tf:"amazon_bedrock_config,omitempty"`

	// Anthropic Config
	// +kubebuilder:validation:Optional
	AnthropicConfig []AnthropicConfigParameters `json:"anthropicConfig,omitempty" tf:"anthropic_config,omitempty"`

	// Cohere Config
	// +kubebuilder:validation:Optional
	CohereConfig []CohereConfigParameters `json:"cohereConfig,omitempty" tf:"cohere_config,omitempty"`

	// Databricks Model Serving Config
	// +kubebuilder:validation:Optional
	DatabricksModelServingConfig []DatabricksModelServingConfigParameters `json:"databricksModelServingConfig,omitempty" tf:"databricks_model_serving_config,omitempty"`

	// Google Cloud Vertex AI Config.
	// +kubebuilder:validation:Optional
	GoogleCloudVertexAIConfig []GoogleCloudVertexAIConfigParameters `json:"googleCloudVertexAiConfig,omitempty" tf:"google_cloud_vertex_ai_config,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	// +kubebuilder:validation:Optional
	Name *string `json:"name" tf:"name,omitempty"`

	// OpenAI Config
	// +kubebuilder:validation:Optional
	OpenaiConfig []OpenaiConfigParameters `json:"openaiConfig,omitempty" tf:"openai_config,omitempty"`

	// PaLM Config
	// +kubebuilder:validation:Optional
	PalmConfig []PalmConfigParameters `json:"palmConfig,omitempty" tf:"palm_config,omitempty"`

	// The name of the provider for the external model. Currently, the supported providers are ai21labs, anthropic, amazon-bedrock, cohere, databricks-model-serving, google-cloud-vertex-ai, openai, and palm.
	// +kubebuilder:validation:Optional
	Provider *string `json:"provider" tf:"provider,omitempty"`

	// The task type of the external model.
	// +kubebuilder:validation:Optional
	Task *string `json:"task" tf:"task,omitempty"`
}

type GoogleCloudVertexAIConfigInitParameters struct {

	// The Databricks secret key reference for a private key for the service account that has access to the Google Cloud Vertex AI Service.
	PrivateKey *string `json:"privateKey,omitempty" tf:"private_key,omitempty"`

	// The private key for the service account that has access to the Google Cloud Vertex AI Service is provided as a plaintext secret.
	PrivateKeyPlaintext *string `json:"privateKeyPlaintext,omitempty" tf:"private_key_plaintext,omitempty"`

	// This is the Google Cloud project id that the service account is associated with.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// This is the region for the Google Cloud Vertex AI Service.
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type GoogleCloudVertexAIConfigObservation struct {

	// The Databricks secret key reference for a private key for the service account that has access to the Google Cloud Vertex AI Service.
	PrivateKey *string `json:"privateKey,omitempty" tf:"private_key,omitempty"`

	// The private key for the service account that has access to the Google Cloud Vertex AI Service is provided as a plaintext secret.
	PrivateKeyPlaintext *string `json:"privateKeyPlaintext,omitempty" tf:"private_key_plaintext,omitempty"`

	// This is the Google Cloud project id that the service account is associated with.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// This is the region for the Google Cloud Vertex AI Service.
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type GoogleCloudVertexAIConfigParameters struct {

	// The Databricks secret key reference for a private key for the service account that has access to the Google Cloud Vertex AI Service.
	// +kubebuilder:validation:Optional
	PrivateKey *string `json:"privateKey,omitempty" tf:"private_key,omitempty"`

	// The private key for the service account that has access to the Google Cloud Vertex AI Service is provided as a plaintext secret.
	// +kubebuilder:validation:Optional
	PrivateKeyPlaintext *string `json:"privateKeyPlaintext,omitempty" tf:"private_key_plaintext,omitempty"`

	// This is the Google Cloud project id that the service account is associated with.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// This is the region for the Google Cloud Vertex AI Service.
	// +kubebuilder:validation:Optional
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type GuardrailsInitParameters struct {

	// A block with configuration for input guardrail filters:
	Input []InputInitParameters `json:"input,omitempty" tf:"input,omitempty"`

	// A block with configuration for output guardrail filters.  Has the same structure as input block.
	Output []OutputInitParameters `json:"output,omitempty" tf:"output,omitempty"`
}

type GuardrailsObservation struct {

	// A block with configuration for input guardrail filters:
	Input []InputObservation `json:"input,omitempty" tf:"input,omitempty"`

	// A block with configuration for output guardrail filters.  Has the same structure as input block.
	Output []OutputObservation `json:"output,omitempty" tf:"output,omitempty"`
}

type GuardrailsParameters struct {

	// A block with configuration for input guardrail filters:
	// +kubebuilder:validation:Optional
	Input []InputParameters `json:"input,omitempty" tf:"input,omitempty"`

	// A block with configuration for output guardrail filters.  Has the same structure as input block.
	// +kubebuilder:validation:Optional
	Output []OutputParameters `json:"output,omitempty" tf:"output,omitempty"`
}

type InferenceTableConfigInitParameters struct {

	// The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.
	CatalogName *string `json:"catalogName,omitempty" tf:"catalog_name,omitempty"`

	// boolean flag specifying if usage tracking is enabled.
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.
	SchemaName *string `json:"schemaName,omitempty" tf:"schema_name,omitempty"`

	// The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.
	TableNamePrefix *string `json:"tableNamePrefix,omitempty" tf:"table_name_prefix,omitempty"`
}

type InferenceTableConfigObservation struct {

	// The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.
	CatalogName *string `json:"catalogName,omitempty" tf:"catalog_name,omitempty"`

	// boolean flag specifying if usage tracking is enabled.
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.
	SchemaName *string `json:"schemaName,omitempty" tf:"schema_name,omitempty"`

	// The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.
	TableNamePrefix *string `json:"tableNamePrefix,omitempty" tf:"table_name_prefix,omitempty"`
}

type InferenceTableConfigParameters struct {

	// The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.
	// +kubebuilder:validation:Optional
	CatalogName *string `json:"catalogName,omitempty" tf:"catalog_name,omitempty"`

	// boolean flag specifying if usage tracking is enabled.
	// +kubebuilder:validation:Optional
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.
	// +kubebuilder:validation:Optional
	SchemaName *string `json:"schemaName,omitempty" tf:"schema_name,omitempty"`

	// The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.
	// +kubebuilder:validation:Optional
	TableNamePrefix *string `json:"tableNamePrefix,omitempty" tf:"table_name_prefix,omitempty"`
}

type InputInitParameters struct {

	// List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.
	InvalidKeywords []*string `json:"invalidKeywords,omitempty" tf:"invalid_keywords,omitempty"`

	// Block with configuration for guardrail PII filter:
	Pii []PiiInitParameters `json:"pii,omitempty" tf:"pii,omitempty"`

	// the boolean flag that indicates whether the safety filter is enabled.
	Safety *bool `json:"safety,omitempty" tf:"safety,omitempty"`

	// The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.
	ValidTopics []*string `json:"validTopics,omitempty" tf:"valid_topics,omitempty"`
}

type InputObservation struct {

	// List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.
	InvalidKeywords []*string `json:"invalidKeywords,omitempty" tf:"invalid_keywords,omitempty"`

	// Block with configuration for guardrail PII filter:
	Pii []PiiObservation `json:"pii,omitempty" tf:"pii,omitempty"`

	// the boolean flag that indicates whether the safety filter is enabled.
	Safety *bool `json:"safety,omitempty" tf:"safety,omitempty"`

	// The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.
	ValidTopics []*string `json:"validTopics,omitempty" tf:"valid_topics,omitempty"`
}

type InputParameters struct {

	// List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.
	// +kubebuilder:validation:Optional
	InvalidKeywords []*string `json:"invalidKeywords,omitempty" tf:"invalid_keywords,omitempty"`

	// Block with configuration for guardrail PII filter:
	// +kubebuilder:validation:Optional
	Pii []PiiParameters `json:"pii,omitempty" tf:"pii,omitempty"`

	// the boolean flag that indicates whether the safety filter is enabled.
	// +kubebuilder:validation:Optional
	Safety *bool `json:"safety,omitempty" tf:"safety,omitempty"`

	// The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.
	// +kubebuilder:validation:Optional
	ValidTopics []*string `json:"validTopics,omitempty" tf:"valid_topics,omitempty"`
}

type ModelServingInitParameters struct {

	// A block with AI Gateway configuration for the serving endpoint. Note: only external model endpoints are supported as of now.
	AIGateway []AIGatewayInitParameters `json:"aiGateway,omitempty" tf:"ai_gateway,omitempty"`

	// The model serving endpoint configuration.
	Config []ConfigInitParameters `json:"config,omitempty" tf:"config,omitempty"`

	// The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// A list of rate limit blocks to be applied to the serving endpoint. Note: only external and foundation model endpoints are supported as of now.
	RateLimits []ModelServingRateLimitsInitParameters `json:"rateLimits,omitempty" tf:"rate_limits,omitempty"`

	// A boolean enabling route optimization for the endpoint. Note: only available for custom models.
	RouteOptimized *bool `json:"routeOptimized,omitempty" tf:"route_optimized,omitempty"`

	// Tags to be attached to the serving endpoint and automatically propagated to billing logs.
	Tags []TagsInitParameters `json:"tags,omitempty" tf:"tags,omitempty"`
}

type ModelServingObservation struct {

	// A block with AI Gateway configuration for the serving endpoint. Note: only external model endpoints are supported as of now.
	AIGateway []AIGatewayObservation `json:"aiGateway,omitempty" tf:"ai_gateway,omitempty"`

	// The model serving endpoint configuration.
	Config []ConfigObservation `json:"config,omitempty" tf:"config,omitempty"`

	// Equal to the name argument and used to identify the serving endpoint.
	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// A list of rate limit blocks to be applied to the serving endpoint. Note: only external and foundation model endpoints are supported as of now.
	RateLimits []ModelServingRateLimitsObservation `json:"rateLimits,omitempty" tf:"rate_limits,omitempty"`

	// A boolean enabling route optimization for the endpoint. Note: only available for custom models.
	RouteOptimized *bool `json:"routeOptimized,omitempty" tf:"route_optimized,omitempty"`

	// Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.
	ServingEndpointID *string `json:"servingEndpointId,omitempty" tf:"serving_endpoint_id,omitempty"`

	// Tags to be attached to the serving endpoint and automatically propagated to billing logs.
	Tags []TagsObservation `json:"tags,omitempty" tf:"tags,omitempty"`
}

type ModelServingParameters struct {

	// A block with AI Gateway configuration for the serving endpoint. Note: only external model endpoints are supported as of now.
	// +kubebuilder:validation:Optional
	AIGateway []AIGatewayParameters `json:"aiGateway,omitempty" tf:"ai_gateway,omitempty"`

	// The model serving endpoint configuration.
	// +kubebuilder:validation:Optional
	Config []ConfigParameters `json:"config,omitempty" tf:"config,omitempty"`

	// The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.
	// +kubebuilder:validation:Optional
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// A list of rate limit blocks to be applied to the serving endpoint. Note: only external and foundation model endpoints are supported as of now.
	// +kubebuilder:validation:Optional
	RateLimits []ModelServingRateLimitsParameters `json:"rateLimits,omitempty" tf:"rate_limits,omitempty"`

	// A boolean enabling route optimization for the endpoint. Note: only available for custom models.
	// +kubebuilder:validation:Optional
	RouteOptimized *bool `json:"routeOptimized,omitempty" tf:"route_optimized,omitempty"`

	// Tags to be attached to the serving endpoint and automatically propagated to billing logs.
	// +kubebuilder:validation:Optional
	Tags []TagsParameters `json:"tags,omitempty" tf:"tags,omitempty"`
}

type ModelServingRateLimitsInitParameters struct {

	// Used to specify how many calls are allowed for a key within the renewal_period.
	Calls *float64 `json:"calls,omitempty" tf:"calls,omitempty"`

	// Key field for a serving endpoint rate limit. Currently, only user and endpoint are supported, with endpoint being the default if not specified.
	Key *string `json:"key,omitempty" tf:"key,omitempty"`

	// Renewal period field for a serving endpoint rate limit. Currently, only minute is supported.
	RenewalPeriod *string `json:"renewalPeriod,omitempty" tf:"renewal_period,omitempty"`
}

type ModelServingRateLimitsObservation struct {

	// Used to specify how many calls are allowed for a key within the renewal_period.
	Calls *float64 `json:"calls,omitempty" tf:"calls,omitempty"`

	// Key field for a serving endpoint rate limit. Currently, only user and endpoint are supported, with endpoint being the default if not specified.
	Key *string `json:"key,omitempty" tf:"key,omitempty"`

	// Renewal period field for a serving endpoint rate limit. Currently, only minute is supported.
	RenewalPeriod *string `json:"renewalPeriod,omitempty" tf:"renewal_period,omitempty"`
}

type ModelServingRateLimitsParameters struct {

	// Used to specify how many calls are allowed for a key within the renewal_period.
	// +kubebuilder:validation:Optional
	Calls *float64 `json:"calls" tf:"calls,omitempty"`

	// Key field for a serving endpoint rate limit. Currently, only user and endpoint are supported, with endpoint being the default if not specified.
	// +kubebuilder:validation:Optional
	Key *string `json:"key,omitempty" tf:"key,omitempty"`

	// Renewal period field for a serving endpoint rate limit. Currently, only minute is supported.
	// +kubebuilder:validation:Optional
	RenewalPeriod *string `json:"renewalPeriod" tf:"renewal_period,omitempty"`
}

type OpenaiConfigInitParameters struct {

	// This field is only required for Azure AD OpenAI and is the Microsoft Entra Client ID.
	MicrosoftEntraClientID *string `json:"microsoftEntraClientId,omitempty" tf:"microsoft_entra_client_id,omitempty"`

	// The Databricks secret key reference for a client secret used for Microsoft Entra ID authentication.
	MicrosoftEntraClientSecret *string `json:"microsoftEntraClientSecret,omitempty" tf:"microsoft_entra_client_secret,omitempty"`

	// The client secret used for Microsoft Entra ID authentication provided as a plaintext string.
	MicrosoftEntraClientSecretPlaintext *string `json:"microsoftEntraClientSecretPlaintext,omitempty" tf:"microsoft_entra_client_secret_plaintext,omitempty"`

	// This field is only required for Azure AD OpenAI and is the Microsoft Entra Tenant ID.
	MicrosoftEntraTenantID *string `json:"microsoftEntraTenantId,omitempty" tf:"microsoft_entra_tenant_id,omitempty"`

	// This is the base URL for the OpenAI API (default: "https://api.openai.com/v1"). For Azure OpenAI, this field is required and is the base URL for the Azure OpenAI API service provided by Azure.
	OpenaiAPIBase *string `json:"openaiApiBase,omitempty" tf:"openai_api_base,omitempty"`

	// The Databricks secret key reference for an OpenAI or Azure OpenAI API key.
	OpenaiAPIKey *string `json:"openaiApiKey,omitempty" tf:"openai_api_key,omitempty"`

	// The OpenAI API key using the OpenAI or Azure service provided as a plaintext string.
	OpenaiAPIKeyPlaintext *string `json:"openaiApiKeyPlaintext,omitempty" tf:"openai_api_key_plaintext,omitempty"`

	// This is an optional field to specify the type of OpenAI API to use. For Azure OpenAI, this field is required, and this parameter represents the preferred security access validation protocol. For access token validation, use azure. For authentication using Azure Active Directory (Azure AD) use, azuread.
	OpenaiAPIType *string `json:"openaiApiType,omitempty" tf:"openai_api_type,omitempty"`

	// This is an optional field to specify the OpenAI API version. For Azure OpenAI, this field is required and is the version of the Azure OpenAI service to utilize, specified by a date.
	OpenaiAPIVersion *string `json:"openaiApiVersion,omitempty" tf:"openai_api_version,omitempty"`

	// This field is only required for Azure OpenAI and is the name of the deployment resource for the Azure OpenAI service.
	OpenaiDeploymentName *string `json:"openaiDeploymentName,omitempty" tf:"openai_deployment_name,omitempty"`

	// This is an optional field to specify the organization in OpenAI or Azure OpenAI.
	OpenaiOrganization *string `json:"openaiOrganization,omitempty" tf:"openai_organization,omitempty"`
}

type OpenaiConfigObservation struct {

	// This field is only required for Azure AD OpenAI and is the Microsoft Entra Client ID.
	MicrosoftEntraClientID *string `json:"microsoftEntraClientId,omitempty" tf:"microsoft_entra_client_id,omitempty"`

	// The Databricks secret key reference for a client secret used for Microsoft Entra ID authentication.
	MicrosoftEntraClientSecret *string `json:"microsoftEntraClientSecret,omitempty" tf:"microsoft_entra_client_secret,omitempty"`

	// The client secret used for Microsoft Entra ID authentication provided as a plaintext string.
	MicrosoftEntraClientSecretPlaintext *string `json:"microsoftEntraClientSecretPlaintext,omitempty" tf:"microsoft_entra_client_secret_plaintext,omitempty"`

	// This field is only required for Azure AD OpenAI and is the Microsoft Entra Tenant ID.
	MicrosoftEntraTenantID *string `json:"microsoftEntraTenantId,omitempty" tf:"microsoft_entra_tenant_id,omitempty"`

	// This is the base URL for the OpenAI API (default: "https://api.openai.com/v1"). For Azure OpenAI, this field is required and is the base URL for the Azure OpenAI API service provided by Azure.
	OpenaiAPIBase *string `json:"openaiApiBase,omitempty" tf:"openai_api_base,omitempty"`

	// The Databricks secret key reference for an OpenAI or Azure OpenAI API key.
	OpenaiAPIKey *string `json:"openaiApiKey,omitempty" tf:"openai_api_key,omitempty"`

	// The OpenAI API key using the OpenAI or Azure service provided as a plaintext string.
	OpenaiAPIKeyPlaintext *string `json:"openaiApiKeyPlaintext,omitempty" tf:"openai_api_key_plaintext,omitempty"`

	// This is an optional field to specify the type of OpenAI API to use. For Azure OpenAI, this field is required, and this parameter represents the preferred security access validation protocol. For access token validation, use azure. For authentication using Azure Active Directory (Azure AD) use, azuread.
	OpenaiAPIType *string `json:"openaiApiType,omitempty" tf:"openai_api_type,omitempty"`

	// This is an optional field to specify the OpenAI API version. For Azure OpenAI, this field is required and is the version of the Azure OpenAI service to utilize, specified by a date.
	OpenaiAPIVersion *string `json:"openaiApiVersion,omitempty" tf:"openai_api_version,omitempty"`

	// This field is only required for Azure OpenAI and is the name of the deployment resource for the Azure OpenAI service.
	OpenaiDeploymentName *string `json:"openaiDeploymentName,omitempty" tf:"openai_deployment_name,omitempty"`

	// This is an optional field to specify the organization in OpenAI or Azure OpenAI.
	OpenaiOrganization *string `json:"openaiOrganization,omitempty" tf:"openai_organization,omitempty"`
}

type OpenaiConfigParameters struct {

	// This field is only required for Azure AD OpenAI and is the Microsoft Entra Client ID.
	// +kubebuilder:validation:Optional
	MicrosoftEntraClientID *string `json:"microsoftEntraClientId,omitempty" tf:"microsoft_entra_client_id,omitempty"`

	// The Databricks secret key reference for a client secret used for Microsoft Entra ID authentication.
	// +kubebuilder:validation:Optional
	MicrosoftEntraClientSecret *string `json:"microsoftEntraClientSecret,omitempty" tf:"microsoft_entra_client_secret,omitempty"`

	// The client secret used for Microsoft Entra ID authentication provided as a plaintext string.
	// +kubebuilder:validation:Optional
	MicrosoftEntraClientSecretPlaintext *string `json:"microsoftEntraClientSecretPlaintext,omitempty" tf:"microsoft_entra_client_secret_plaintext,omitempty"`

	// This field is only required for Azure AD OpenAI and is the Microsoft Entra Tenant ID.
	// +kubebuilder:validation:Optional
	MicrosoftEntraTenantID *string `json:"microsoftEntraTenantId,omitempty" tf:"microsoft_entra_tenant_id,omitempty"`

	// This is the base URL for the OpenAI API (default: "https://api.openai.com/v1"). For Azure OpenAI, this field is required and is the base URL for the Azure OpenAI API service provided by Azure.
	// +kubebuilder:validation:Optional
	OpenaiAPIBase *string `json:"openaiApiBase,omitempty" tf:"openai_api_base,omitempty"`

	// The Databricks secret key reference for an OpenAI or Azure OpenAI API key.
	// +kubebuilder:validation:Optional
	OpenaiAPIKey *string `json:"openaiApiKey,omitempty" tf:"openai_api_key,omitempty"`

	// The OpenAI API key using the OpenAI or Azure service provided as a plaintext string.
	// +kubebuilder:validation:Optional
	OpenaiAPIKeyPlaintext *string `json:"openaiApiKeyPlaintext,omitempty" tf:"openai_api_key_plaintext,omitempty"`

	// This is an optional field to specify the type of OpenAI API to use. For Azure OpenAI, this field is required, and this parameter represents the preferred security access validation protocol. For access token validation, use azure. For authentication using Azure Active Directory (Azure AD) use, azuread.
	// +kubebuilder:validation:Optional
	OpenaiAPIType *string `json:"openaiApiType,omitempty" tf:"openai_api_type,omitempty"`

	// This is an optional field to specify the OpenAI API version. For Azure OpenAI, this field is required and is the version of the Azure OpenAI service to utilize, specified by a date.
	// +kubebuilder:validation:Optional
	OpenaiAPIVersion *string `json:"openaiApiVersion,omitempty" tf:"openai_api_version,omitempty"`

	// This field is only required for Azure OpenAI and is the name of the deployment resource for the Azure OpenAI service.
	// +kubebuilder:validation:Optional
	OpenaiDeploymentName *string `json:"openaiDeploymentName,omitempty" tf:"openai_deployment_name,omitempty"`

	// This is an optional field to specify the organization in OpenAI or Azure OpenAI.
	// +kubebuilder:validation:Optional
	OpenaiOrganization *string `json:"openaiOrganization,omitempty" tf:"openai_organization,omitempty"`
}

type OutputInitParameters struct {

	// List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.
	InvalidKeywords []*string `json:"invalidKeywords,omitempty" tf:"invalid_keywords,omitempty"`

	// Block with configuration for guardrail PII filter:
	Pii []OutputPiiInitParameters `json:"pii,omitempty" tf:"pii,omitempty"`

	// the boolean flag that indicates whether the safety filter is enabled.
	Safety *bool `json:"safety,omitempty" tf:"safety,omitempty"`

	// The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.
	ValidTopics []*string `json:"validTopics,omitempty" tf:"valid_topics,omitempty"`
}

type OutputObservation struct {

	// List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.
	InvalidKeywords []*string `json:"invalidKeywords,omitempty" tf:"invalid_keywords,omitempty"`

	// Block with configuration for guardrail PII filter:
	Pii []OutputPiiObservation `json:"pii,omitempty" tf:"pii,omitempty"`

	// the boolean flag that indicates whether the safety filter is enabled.
	Safety *bool `json:"safety,omitempty" tf:"safety,omitempty"`

	// The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.
	ValidTopics []*string `json:"validTopics,omitempty" tf:"valid_topics,omitempty"`
}

type OutputParameters struct {

	// List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.
	// +kubebuilder:validation:Optional
	InvalidKeywords []*string `json:"invalidKeywords,omitempty" tf:"invalid_keywords,omitempty"`

	// Block with configuration for guardrail PII filter:
	// +kubebuilder:validation:Optional
	Pii []OutputPiiParameters `json:"pii,omitempty" tf:"pii,omitempty"`

	// the boolean flag that indicates whether the safety filter is enabled.
	// +kubebuilder:validation:Optional
	Safety *bool `json:"safety,omitempty" tf:"safety,omitempty"`

	// The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.
	// +kubebuilder:validation:Optional
	ValidTopics []*string `json:"validTopics,omitempty" tf:"valid_topics,omitempty"`
}

type OutputPiiInitParameters struct {

	// a string that describes the behavior for PII filter. Currently only BLOCK value is supported.
	Behavior *string `json:"behavior,omitempty" tf:"behavior,omitempty"`
}

type OutputPiiObservation struct {

	// a string that describes the behavior for PII filter. Currently only BLOCK value is supported.
	Behavior *string `json:"behavior,omitempty" tf:"behavior,omitempty"`
}

type OutputPiiParameters struct {

	// a string that describes the behavior for PII filter. Currently only BLOCK value is supported.
	// +kubebuilder:validation:Optional
	Behavior *string `json:"behavior" tf:"behavior,omitempty"`
}

type PalmConfigInitParameters struct {

	// The Databricks secret key reference for a PaLM API key.
	PalmAPIKey *string `json:"palmApiKey,omitempty" tf:"palm_api_key,omitempty"`

	// The PaLM API key provided as a plaintext string.
	PalmAPIKeyPlaintext *string `json:"palmApiKeyPlaintext,omitempty" tf:"palm_api_key_plaintext,omitempty"`
}

type PalmConfigObservation struct {

	// The Databricks secret key reference for a PaLM API key.
	PalmAPIKey *string `json:"palmApiKey,omitempty" tf:"palm_api_key,omitempty"`

	// The PaLM API key provided as a plaintext string.
	PalmAPIKeyPlaintext *string `json:"palmApiKeyPlaintext,omitempty" tf:"palm_api_key_plaintext,omitempty"`
}

type PalmConfigParameters struct {

	// The Databricks secret key reference for a PaLM API key.
	// +kubebuilder:validation:Optional
	PalmAPIKey *string `json:"palmApiKey,omitempty" tf:"palm_api_key,omitempty"`

	// The PaLM API key provided as a plaintext string.
	// +kubebuilder:validation:Optional
	PalmAPIKeyPlaintext *string `json:"palmApiKeyPlaintext,omitempty" tf:"palm_api_key_plaintext,omitempty"`
}

type PiiInitParameters struct {

	// a string that describes the behavior for PII filter. Currently only BLOCK value is supported.
	Behavior *string `json:"behavior,omitempty" tf:"behavior,omitempty"`
}

type PiiObservation struct {

	// a string that describes the behavior for PII filter. Currently only BLOCK value is supported.
	Behavior *string `json:"behavior,omitempty" tf:"behavior,omitempty"`
}

type PiiParameters struct {

	// a string that describes the behavior for PII filter. Currently only BLOCK value is supported.
	// +kubebuilder:validation:Optional
	Behavior *string `json:"behavior" tf:"behavior,omitempty"`
}

type RateLimitsInitParameters struct {

	// Used to specify how many calls are allowed for a key within the renewal_period.
	Calls *float64 `json:"calls,omitempty" tf:"calls,omitempty"`

	// The key field for a tag.
	Key *string `json:"key,omitempty" tf:"key,omitempty"`

	// Renewal period field for a serving endpoint rate limit. Currently, only minute is supported.
	RenewalPeriod *string `json:"renewalPeriod,omitempty" tf:"renewal_period,omitempty"`
}

type RateLimitsObservation struct {

	// Used to specify how many calls are allowed for a key within the renewal_period.
	Calls *float64 `json:"calls,omitempty" tf:"calls,omitempty"`

	// The key field for a tag.
	Key *string `json:"key,omitempty" tf:"key,omitempty"`

	// Renewal period field for a serving endpoint rate limit. Currently, only minute is supported.
	RenewalPeriod *string `json:"renewalPeriod,omitempty" tf:"renewal_period,omitempty"`
}

type RateLimitsParameters struct {

	// Used to specify how many calls are allowed for a key within the renewal_period.
	// +kubebuilder:validation:Optional
	Calls *float64 `json:"calls" tf:"calls,omitempty"`

	// The key field for a tag.
	// +kubebuilder:validation:Optional
	Key *string `json:"key,omitempty" tf:"key,omitempty"`

	// Renewal period field for a serving endpoint rate limit. Currently, only minute is supported.
	// +kubebuilder:validation:Optional
	RenewalPeriod *string `json:"renewalPeriod" tf:"renewal_period,omitempty"`
}

type RoutesInitParameters struct {

	// The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.
	ServedModelName *string `json:"servedModelName,omitempty" tf:"served_model_name,omitempty"`

	// The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.
	TrafficPercentage *float64 `json:"trafficPercentage,omitempty" tf:"traffic_percentage,omitempty"`
}

type RoutesObservation struct {

	// The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.
	ServedModelName *string `json:"servedModelName,omitempty" tf:"served_model_name,omitempty"`

	// The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.
	TrafficPercentage *float64 `json:"trafficPercentage,omitempty" tf:"traffic_percentage,omitempty"`
}

type RoutesParameters struct {

	// The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.
	// +kubebuilder:validation:Optional
	ServedModelName *string `json:"servedModelName" tf:"served_model_name,omitempty"`

	// The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.
	// +kubebuilder:validation:Optional
	TrafficPercentage *float64 `json:"trafficPercentage" tf:"traffic_percentage,omitempty"`
}

type ServedEntitiesInitParameters struct {

	// The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type FEATURE_SPEC in the UC. If it is a UC object, the full name of the object should be given in the form of catalog_name.schema_name.model_name.
	EntityName *string `json:"entityName,omitempty" tf:"entity_name,omitempty"`

	// The version of the model in Databricks Model Registry to be served or empty if the entity is a FEATURE_SPEC.
	EntityVersion *string `json:"entityVersion,omitempty" tf:"entity_version,omitempty"`

	// a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.
	// +mapType=granular
	EnvironmentVars map[string]*string `json:"environmentVars,omitempty" tf:"environment_vars,omitempty"`

	// The external model to be served. NOTE: Only one of external_model and (entity_name, entity_version, workload_size, workload_type, and scale_to_zero_enabled) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an external_model is present, the served entities list can only have one served_entity object. An existing endpoint with external_model can not be updated to an endpoint without external_model. If the endpoint is created without external_model, users cannot update it to add external_model later.
	ExternalModel []ExternalModelInitParameters `json:"externalModel,omitempty" tf:"external_model,omitempty"`

	// ARN of the instance profile that the served model will use to access AWS resources.
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The maximum tokens per second that the endpoint can scale up to.
	MaxProvisionedThroughput *float64 `json:"maxProvisionedThroughput,omitempty" tf:"max_provisioned_throughput,omitempty"`

	// The minimum tokens per second that the endpoint can scale down to.
	MinProvisionedThroughput *float64 `json:"minProvisionedThroughput,omitempty" tf:"min_provisioned_throughput,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.
	ScaleToZeroEnabled *bool `json:"scaleToZeroEnabled,omitempty" tf:"scale_to_zero_enabled,omitempty"`

	// The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency).
	WorkloadSize *string `json:"workloadSize,omitempty" tf:"workload_size,omitempty"`

	// The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU.
	WorkloadType *string `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type ServedEntitiesObservation struct {

	// The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type FEATURE_SPEC in the UC. If it is a UC object, the full name of the object should be given in the form of catalog_name.schema_name.model_name.
	EntityName *string `json:"entityName,omitempty" tf:"entity_name,omitempty"`

	// The version of the model in Databricks Model Registry to be served or empty if the entity is a FEATURE_SPEC.
	EntityVersion *string `json:"entityVersion,omitempty" tf:"entity_version,omitempty"`

	// a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.
	// +mapType=granular
	EnvironmentVars map[string]*string `json:"environmentVars,omitempty" tf:"environment_vars,omitempty"`

	// The external model to be served. NOTE: Only one of external_model and (entity_name, entity_version, workload_size, workload_type, and scale_to_zero_enabled) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an external_model is present, the served entities list can only have one served_entity object. An existing endpoint with external_model can not be updated to an endpoint without external_model. If the endpoint is created without external_model, users cannot update it to add external_model later.
	ExternalModel []ExternalModelObservation `json:"externalModel,omitempty" tf:"external_model,omitempty"`

	// ARN of the instance profile that the served model will use to access AWS resources.
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The maximum tokens per second that the endpoint can scale up to.
	MaxProvisionedThroughput *float64 `json:"maxProvisionedThroughput,omitempty" tf:"max_provisioned_throughput,omitempty"`

	// The minimum tokens per second that the endpoint can scale down to.
	MinProvisionedThroughput *float64 `json:"minProvisionedThroughput,omitempty" tf:"min_provisioned_throughput,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.
	ScaleToZeroEnabled *bool `json:"scaleToZeroEnabled,omitempty" tf:"scale_to_zero_enabled,omitempty"`

	// The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency).
	WorkloadSize *string `json:"workloadSize,omitempty" tf:"workload_size,omitempty"`

	// The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU.
	WorkloadType *string `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type ServedEntitiesParameters struct {

	// The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type FEATURE_SPEC in the UC. If it is a UC object, the full name of the object should be given in the form of catalog_name.schema_name.model_name.
	// +kubebuilder:validation:Optional
	EntityName *string `json:"entityName,omitempty" tf:"entity_name,omitempty"`

	// The version of the model in Databricks Model Registry to be served or empty if the entity is a FEATURE_SPEC.
	// +kubebuilder:validation:Optional
	EntityVersion *string `json:"entityVersion,omitempty" tf:"entity_version,omitempty"`

	// a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	EnvironmentVars map[string]*string `json:"environmentVars,omitempty" tf:"environment_vars,omitempty"`

	// The external model to be served. NOTE: Only one of external_model and (entity_name, entity_version, workload_size, workload_type, and scale_to_zero_enabled) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an external_model is present, the served entities list can only have one served_entity object. An existing endpoint with external_model can not be updated to an endpoint without external_model. If the endpoint is created without external_model, users cannot update it to add external_model later.
	// +kubebuilder:validation:Optional
	ExternalModel []ExternalModelParameters `json:"externalModel,omitempty" tf:"external_model,omitempty"`

	// ARN of the instance profile that the served model will use to access AWS resources.
	// +kubebuilder:validation:Optional
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The maximum tokens per second that the endpoint can scale up to.
	// +kubebuilder:validation:Optional
	MaxProvisionedThroughput *float64 `json:"maxProvisionedThroughput,omitempty" tf:"max_provisioned_throughput,omitempty"`

	// The minimum tokens per second that the endpoint can scale down to.
	// +kubebuilder:validation:Optional
	MinProvisionedThroughput *float64 `json:"minProvisionedThroughput,omitempty" tf:"min_provisioned_throughput,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	// +kubebuilder:validation:Optional
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.
	// +kubebuilder:validation:Optional
	ScaleToZeroEnabled *bool `json:"scaleToZeroEnabled,omitempty" tf:"scale_to_zero_enabled,omitempty"`

	// The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency).
	// +kubebuilder:validation:Optional
	WorkloadSize *string `json:"workloadSize,omitempty" tf:"workload_size,omitempty"`

	// The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU.
	// +kubebuilder:validation:Optional
	WorkloadType *string `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type ServedModelsInitParameters struct {

	// a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.
	// +mapType=granular
	EnvironmentVars map[string]*string `json:"environmentVars,omitempty" tf:"environment_vars,omitempty"`

	// ARN of the instance profile that the served model will use to access AWS resources.
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The maximum tokens per second that the endpoint can scale up to.
	MaxProvisionedThroughput *float64 `json:"maxProvisionedThroughput,omitempty" tf:"max_provisioned_throughput,omitempty"`

	// The minimum tokens per second that the endpoint can scale down to.
	MinProvisionedThroughput *float64 `json:"minProvisionedThroughput,omitempty" tf:"min_provisioned_throughput,omitempty"`

	// The name of the model in Databricks Model Registry to be served.
	ModelName *string `json:"modelName,omitempty" tf:"model_name,omitempty"`

	// The version of the model in Databricks Model Registry to be served.
	ModelVersion *string `json:"modelVersion,omitempty" tf:"model_version,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.
	ScaleToZeroEnabled *bool `json:"scaleToZeroEnabled,omitempty" tf:"scale_to_zero_enabled,omitempty"`

	// The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency).
	WorkloadSize *string `json:"workloadSize,omitempty" tf:"workload_size,omitempty"`

	// The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU.
	WorkloadType *string `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type ServedModelsObservation struct {

	// a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.
	// +mapType=granular
	EnvironmentVars map[string]*string `json:"environmentVars,omitempty" tf:"environment_vars,omitempty"`

	// ARN of the instance profile that the served model will use to access AWS resources.
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The maximum tokens per second that the endpoint can scale up to.
	MaxProvisionedThroughput *float64 `json:"maxProvisionedThroughput,omitempty" tf:"max_provisioned_throughput,omitempty"`

	// The minimum tokens per second that the endpoint can scale down to.
	MinProvisionedThroughput *float64 `json:"minProvisionedThroughput,omitempty" tf:"min_provisioned_throughput,omitempty"`

	// The name of the model in Databricks Model Registry to be served.
	ModelName *string `json:"modelName,omitempty" tf:"model_name,omitempty"`

	// The version of the model in Databricks Model Registry to be served.
	ModelVersion *string `json:"modelVersion,omitempty" tf:"model_version,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.
	ScaleToZeroEnabled *bool `json:"scaleToZeroEnabled,omitempty" tf:"scale_to_zero_enabled,omitempty"`

	// The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency).
	WorkloadSize *string `json:"workloadSize,omitempty" tf:"workload_size,omitempty"`

	// The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU.
	WorkloadType *string `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type ServedModelsParameters struct {

	// a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	EnvironmentVars map[string]*string `json:"environmentVars,omitempty" tf:"environment_vars,omitempty"`

	// ARN of the instance profile that the served model will use to access AWS resources.
	// +kubebuilder:validation:Optional
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The maximum tokens per second that the endpoint can scale up to.
	// +kubebuilder:validation:Optional
	MaxProvisionedThroughput *float64 `json:"maxProvisionedThroughput,omitempty" tf:"max_provisioned_throughput,omitempty"`

	// The minimum tokens per second that the endpoint can scale down to.
	// +kubebuilder:validation:Optional
	MinProvisionedThroughput *float64 `json:"minProvisionedThroughput,omitempty" tf:"min_provisioned_throughput,omitempty"`

	// The name of the model in Databricks Model Registry to be served.
	// +kubebuilder:validation:Optional
	ModelName *string `json:"modelName" tf:"model_name,omitempty"`

	// The version of the model in Databricks Model Registry to be served.
	// +kubebuilder:validation:Optional
	ModelVersion *string `json:"modelVersion" tf:"model_version,omitempty"`

	// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.
	// +kubebuilder:validation:Optional
	Name *string `json:"name,omitempty" tf:"name,omitempty"`

	// Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.
	// +kubebuilder:validation:Optional
	ScaleToZeroEnabled *bool `json:"scaleToZeroEnabled,omitempty" tf:"scale_to_zero_enabled,omitempty"`

	// The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency).
	// +kubebuilder:validation:Optional
	WorkloadSize *string `json:"workloadSize,omitempty" tf:"workload_size,omitempty"`

	// The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU.
	// +kubebuilder:validation:Optional
	WorkloadType *string `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type TagsInitParameters struct {

	// The key field for a tag.
	Key *string `json:"key,omitempty" tf:"key,omitempty"`

	// The value field for a tag.
	Value *string `json:"value,omitempty" tf:"value,omitempty"`
}

type TagsObservation struct {

	// The key field for a tag.
	Key *string `json:"key,omitempty" tf:"key,omitempty"`

	// The value field for a tag.
	Value *string `json:"value,omitempty" tf:"value,omitempty"`
}

type TagsParameters struct {

	// The key field for a tag.
	// +kubebuilder:validation:Optional
	Key *string `json:"key" tf:"key,omitempty"`

	// The value field for a tag.
	// +kubebuilder:validation:Optional
	Value *string `json:"value,omitempty" tf:"value,omitempty"`
}

type TrafficConfigInitParameters struct {

	// Each block represents a route that defines traffic to each served entity. Each served_entity block needs to have a corresponding routes block.
	Routes []RoutesInitParameters `json:"routes,omitempty" tf:"routes,omitempty"`
}

type TrafficConfigObservation struct {

	// Each block represents a route that defines traffic to each served entity. Each served_entity block needs to have a corresponding routes block.
	Routes []RoutesObservation `json:"routes,omitempty" tf:"routes,omitempty"`
}

type TrafficConfigParameters struct {

	// Each block represents a route that defines traffic to each served entity. Each served_entity block needs to have a corresponding routes block.
	// +kubebuilder:validation:Optional
	Routes []RoutesParameters `json:"routes,omitempty" tf:"routes,omitempty"`
}

type UsageTrackingConfigInitParameters struct {

	// boolean flag specifying if usage tracking is enabled.
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`
}

type UsageTrackingConfigObservation struct {

	// boolean flag specifying if usage tracking is enabled.
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`
}

type UsageTrackingConfigParameters struct {

	// boolean flag specifying if usage tracking is enabled.
	// +kubebuilder:validation:Optional
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`
}

// ModelServingSpec defines the desired state of ModelServing
type ModelServingSpec struct {
	v1.ResourceSpec `json:",inline"`
	ForProvider     ModelServingParameters `json:"forProvider"`
	// THIS IS A BETA FIELD. It will be honored
	// unless the Management Policies feature flag is disabled.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	InitProvider ModelServingInitParameters `json:"initProvider,omitempty"`
}

// ModelServingStatus defines the observed state of ModelServing.
type ModelServingStatus struct {
	v1.ResourceStatus `json:",inline"`
	AtProvider        ModelServingObservation `json:"atProvider,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:storageversion

// ModelServing is the Schema for the ModelServings API.
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,databricks}
type ModelServing struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.config) || (has(self.initProvider) && has(self.initProvider.config))",message="spec.forProvider.config is a required parameter"
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.name) || (has(self.initProvider) && has(self.initProvider.name))",message="spec.forProvider.name is a required parameter"
	Spec   ModelServingSpec   `json:"spec"`
	Status ModelServingStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// ModelServingList contains a list of ModelServings
type ModelServingList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []ModelServing `json:"items"`
}

// Repository type metadata.
var (
	ModelServing_Kind             = "ModelServing"
	ModelServing_GroupKind        = schema.GroupKind{Group: CRDGroup, Kind: ModelServing_Kind}.String()
	ModelServing_KindAPIVersion   = ModelServing_Kind + "." + CRDGroupVersion.String()
	ModelServing_GroupVersionKind = CRDGroupVersion.WithKind(ModelServing_Kind)
)

func init() {
	SchemeBuilder.Register(&ModelServing{}, &ModelServingList{})
}
